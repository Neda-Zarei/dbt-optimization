[0m20:23:26.757727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a9811c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba30490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba302b0>]}


============================== 20:23:26.763559 | 21849da2-fa47-4a88-b001-b2e68de14cce ==============================
[0m20:23:26.763559 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:23:26.764066 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'fail_fast': 'False', 'no_print': 'None', 'introspect': 'True', 'log_path': '/Users/nedazarei/Documents/turintech/dbtproject/logs', 'use_colors': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'debug': 'False', 'version_check': 'True', 'warn_error': 'None', 'log_cache_events': 'False', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'log_format': 'default', 'use_experimental_parser': 'False', 'profiles_dir': '/Users/nedazarei/Documents/turintech/dbtproject', 'target_path': 'None', 'empty': 'None', 'invocation_command': 'dbt deps', 'cache_selected_only': 'False'}
[0m20:23:26.922730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '21849da2-fa47-4a88-b001-b2e68de14cce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b9882b0>]}
[0m20:23:26.943452 [debug] [MainThread]: Set downloads directory='/var/folders/9d/rc17811j07v56khq2t9t7h5m0000gn/T/dbt-downloads-wkw3g488'
[0m20:23:26.943863 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m20:23:27.365625 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m20:23:27.368731 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m20:23:27.772770 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m20:23:27.782202 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m20:23:28.787488 [info ] [MainThread]: Installed from version 1.3.3
[0m20:23:28.787894 [info ] [MainThread]: Up to date!
[0m20:23:28.788227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '21849da2-fa47-4a88-b001-b2e68de14cce', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bb5b340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba09b20>]}
[0m20:23:28.790870 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 2.1120458, "process_in_blocks": "0", "process_kernel_time": 0.261893, "process_mem_max_rss": "110166016", "process_out_blocks": "0", "process_user_time": 1.488211}
[0m20:23:28.791261 [debug] [MainThread]: Command `dbt deps` succeeded at 20:23:28.791177 after 2.11 seconds
[0m20:23:28.791564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a9811c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bb551c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10acd1ca0>]}
[0m20:23:28.791874 [debug] [MainThread]: Flushing usage events
[0m20:23:29.685044 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:23:38.034666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ad5100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ab834f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ab83310>]}


============================== 20:23:38.039859 | 662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3 ==============================
[0m20:23:38.039859 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:23:38.040385 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'partial_parse': 'True', 'write_json': 'True', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'static_parser': 'True', 'profiles_dir': '/Users/nedazarei/Documents/turintech/dbtproject', 'use_experimental_parser': 'False', 'log_path': '/Users/nedazarei/Documents/turintech/dbtproject/logs', 'cache_selected_only': 'False', 'introspect': 'True', 'invocation_command': 'dbt run --full-refresh', 'version_check': 'True', 'printer_width': '80', 'debug': 'False', 'no_print': 'None', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'target_path': 'None', 'fail_fast': 'False', 'use_colors': 'True'}
[0m20:23:38.589211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108389820>]}
[0m20:23:38.661724 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ddc1f0>]}
[0m20:23:38.662928 [info ] [MainThread]: Registered adapter: snowflake=1.10.2
[0m20:23:38.795936 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m20:23:38.796884 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m20:23:38.797243 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cb7f3d0>]}
[0m20:23:40.760563 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `accepted_values` defined on 'stg_trades' in
package 'bain_capital_portfolio_analytics' (models/pipeline_b/schema.yml).
Arguments to generic tests should be nested under the `arguments` property.
[0m20:23:40.761200 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cf6ebb0>]}
[0m20:23:41.103054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d5fa130>]}
[0m20:23:41.256207 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/manifest.json
[0m20:23:41.258817 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/semantic_manifest.json
[0m20:23:41.282050 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d63fac0>]}
[0m20:23:41.282557 [info ] [MainThread]: Found 32 models, 90 data tests, 5 seeds, 12 sources, 632 macros
[0m20:23:41.282873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d712070>]}
[0m20:23:41.286280 [info ] [MainThread]: 
[0m20:23:41.286590 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m20:23:41.286853 [info ] [MainThread]: 
[0m20:23:41.287316 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m20:23:41.293886 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO'
[0m20:23:41.294388 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO'
[0m20:23:41.294804 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO'
[0m20:23:41.397188 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO"
[0m20:23:41.397698 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO"
[0m20:23:41.398121 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO"
[0m20:23:41.398434 [debug] [ThreadPool]: On list_DBT_DEMO: show terse schemas in database DBT_DEMO
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO"} */
[0m20:23:41.398729 [debug] [ThreadPool]: On list_DBT_DEMO: show terse schemas in database DBT_DEMO
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO"} */
[0m20:23:41.399024 [debug] [ThreadPool]: On list_DBT_DEMO: show terse schemas in database DBT_DEMO
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO"} */
[0m20:23:41.399295 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:23:41.399575 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:23:41.399826 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:23:44.956656 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 3.556 seconds
[0m20:23:45.187243 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 3.788 seconds
[0m20:23:45.246439 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 3.847 seconds
[0m20:23:45.258108 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DBT_DEMO, now list_DBT_DEMO_DEV)
[0m20:23:45.258775 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DBT_DEMO, now list_DBT_DEMO_DEV_pipeline_b)
[0m20:23:45.266198 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DBT_DEMO, now list_DBT_DEMO_DEV_pipeline_a)
[0m20:23:45.272215 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV"
[0m20:23:45.274635 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV_pipeline_b"
[0m20:23:45.275094 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO_DEV_pipeline_c'
[0m20:23:45.278748 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV_pipeline_a"
[0m20:23:45.279142 [debug] [ThreadPool]: On list_DBT_DEMO_DEV: show objects in DBT_DEMO.DEV
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV"} */;
[0m20:23:45.279463 [debug] [ThreadPool]: On list_DBT_DEMO_DEV_pipeline_b: show objects in DBT_DEMO.DEV_pipeline_b
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV_pipeline_b"} */;
[0m20:23:45.281781 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV_pipeline_c"
[0m20:23:45.282081 [debug] [ThreadPool]: On list_DBT_DEMO_DEV_pipeline_a: show objects in DBT_DEMO.DEV_pipeline_a
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV_pipeline_a"} */;
[0m20:23:45.282831 [debug] [ThreadPool]: On list_DBT_DEMO_DEV_pipeline_c: show objects in DBT_DEMO.DEV_pipeline_c
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV_pipeline_c"} */;
[0m20:23:45.283308 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:23:45.516831 [debug] [ThreadPool]: SQL status: SUCCESS 17 in 0.234 seconds
[0m20:23:45.527641 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.244 seconds
[0m20:23:45.530503 [debug] [ThreadPool]: SQL status: SUCCESS 9 in 0.248 seconds
[0m20:23:48.068144 [debug] [ThreadPool]: SQL status: SUCCESS 19 in 2.784 seconds
[0m20:23:48.075788 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d7be280>]}
[0m20:23:48.079753 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.stg_benchmark_returns
[0m20:23:48.080150 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.stg_benchmarks
[0m20:23:48.080499 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.stg_brokers
[0m20:23:48.081533 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.stg_cashflows
[0m20:23:48.080988 [info ] [Thread-1  ]: 1 of 32 START sql view model DEV_pipeline_c.stg_benchmark_returns .............. [RUN]
[0m20:23:48.082045 [info ] [Thread-2  ]: 2 of 32 START sql view model DEV_pipeline_c.stg_benchmarks ..................... [RUN]
[0m20:23:48.082462 [info ] [Thread-3  ]: 3 of 32 START sql view model DEV_pipeline_b.stg_brokers ........................ [RUN]
[0m20:23:48.082866 [info ] [Thread-4  ]: 4 of 32 START sql view model DEV_pipeline_a.stg_cashflows ...................... [RUN]
[0m20:23:48.083271 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV, now model.bain_capital_portfolio_analytics.stg_benchmark_returns)
[0m20:23:48.083608 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV_pipeline_b, now model.bain_capital_portfolio_analytics.stg_benchmarks)
[0m20:23:48.083925 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV_pipeline_a, now model.bain_capital_portfolio_analytics.stg_brokers)
[0m20:23:48.084231 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV_pipeline_c, now model.bain_capital_portfolio_analytics.stg_cashflows)
[0m20:23:48.084548 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_benchmark_returns
[0m20:23:48.084839 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_benchmarks
[0m20:23:48.085119 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_brokers
[0m20:23:48.085409 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_cashflows
[0m20:23:48.094431 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_benchmark_returns"
[0m20:23:48.098132 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_benchmarks"
[0m20:23:48.100874 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_brokers"
[0m20:23:48.103959 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_cashflows"
[0m20:23:48.105113 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.stg_benchmark_returns
[0m20:23:48.117754 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.stg_benchmarks
[0m20:23:48.118169 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.stg_brokers
[0m20:23:48.129432 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.stg_cashflows
[0m20:23:48.137663 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_benchmark_returns"
[0m20:23:48.140646 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_benchmarks"
[0m20:23:48.143386 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_brokers"
[0m20:23:48.146028 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_cashflows"
[0m20:23:48.148043 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_benchmarks"
[0m20:23:48.149706 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_benchmark_returns"
[0m20:23:48.150063 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.stg_benchmarks: create or replace   view DBT_DEMO.DEV_pipeline_c.stg_benchmarks
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: stg_benchmarks
-- Description: Benchmark index data for performance comparison

with source as (
    select
        benchmark_id,
        benchmark_name,
        benchmark_ticker,
        asset_class,
        region,
        is_active,
        created_at,
        updated_at
    from DBT_DEMO.DEV.benchmarks
),

-- ISSUE: Subquery for deduplication
deduplicated as (
    select *
    from (
        select
            *,
            row_number() over (partition by benchmark_id order by updated_at desc) as rn
        from source
    )
    where rn = 1
)

select
    benchmark_id,
    trim(benchmark_name) as benchmark_name,
    upper(trim(benchmark_ticker)) as benchmark_ticker,
    upper(asset_class) as asset_class,
    upper(region) as region,
    is_active,
    created_at,
    updated_at
from deduplicated
where is_active = true
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_benchmarks"} */;
[0m20:23:48.150537 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.stg_benchmark_returns: create or replace   view DBT_DEMO.DEV_pipeline_c.stg_benchmark_returns
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: stg_benchmark_returns
-- Description: Daily benchmark return data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-join for cumulative returns (inefficient)
-- 2. Multiple window functions

with source as (
    select
        benchmark_id,
        return_date,
        daily_return,
        index_level,
        created_at
    from DBT_DEMO.DEV.benchmark_returns
    where return_date >= '2020-01-01'
),

-- ISSUE: Multiple window functions that could be consolidated
with_cumulative as (
    select
        benchmark_id,
        return_date,
        daily_return,
        index_level,
        -- ISSUE: Separate window functions for each period
        exp(sum(ln(1 + daily_return)) over (
            partition by benchmark_id
            order by return_date
            rows between unbounded preceding and current row
        )) - 1 as cumulative_return,
        exp(sum(ln(1 + daily_return)) over (
            partition by benchmark_id
            order by return_date
            rows between 29 preceding and current row
        )) - 1 as return_30d,
        exp(sum(ln(1 + daily_return)) over (
            partition by benchmark_id
            order by return_date
            rows between 89 preceding and current row
        )) - 1 as return_90d,
        exp(sum(ln(1 + daily_return)) over (
            partition by benchmark_id
            order by return_date
            rows between 364 preceding and current row
        )) - 1 as return_1y,
        stddev(daily_return) over (
            partition by benchmark_id
            order by return_date
            rows between 251 preceding and current row
        ) * sqrt(252) as annualized_volatility,
        created_at
    from source
)

select * from with_cumulative
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_benchmark_returns"} */;
[0m20:23:48.151689 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_cashflows"
[0m20:23:48.152717 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_brokers"
[0m20:23:48.153488 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.stg_cashflows: create or replace   view DBT_DEMO.DEV_pipeline_a.stg_cashflows
  
  
  
  
  as (
    -- Pipeline A: Simple Cashflow Pipeline
-- Model: stg_cashflows
-- Description: Staging model for raw cashflow data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Unnecessary DISTINCT (source already unique)
-- 2. Late filtering (should push date filter upstream)
-- 3. Non-optimal date casting

with source as (
    select distinct  -- ISSUE: Unnecessary DISTINCT, source has unique constraint
        cashflow_id,
        portfolio_id,
        cashflow_type,
        cashflow_date,
        amount,
        currency,
        created_at,
        updated_at
    from DBT_DEMO.DEV.cashflows
),

-- ISSUE: Heavy transformation before filtering
converted as (
    select
        cashflow_id,
        portfolio_id,
        upper(cashflow_type) as cashflow_type,
        cast(cashflow_date as date) as cashflow_date,
        cast(amount as decimal(18,2)) as amount,
        upper(currency) as currency,
        cast(created_at as timestamp) as created_at,
        cast(updated_at as timestamp) as updated_at
    from source
),

-- ISSUE: Filter applied after transformation, should be earlier
filtered as (
    select *
    from converted
    where cashflow_date >= '2020-01-01'
      and cashflow_date <= '2024-12-31'
)

select * from filtered
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_cashflows"} */;
[0m20:23:48.153941 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.stg_brokers: create or replace   view DBT_DEMO.DEV_pipeline_b.stg_brokers
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: stg_brokers
-- Description: Staging model for broker information

with source as (
    select
        broker_id,
        broker_name,
        broker_type,
        region,
        is_active,
        commission_rate,
        created_at,
        updated_at
    from DBT_DEMO.DEV.brokers
),

deduplicated as (
    select *
    from (
        select
            *,
            row_number() over (partition by broker_id order by updated_at desc) as rn
        from source
    )
    where rn = 1
)

select
    broker_id,
    trim(broker_name) as broker_name,
    upper(broker_type) as broker_type,
    upper(region) as region,
    is_active,
    commission_rate,
    created_at,
    updated_at
from deduplicated
where is_active = true
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_brokers"} */;
[0m20:23:48.408434 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.255 seconds
[0m20:23:48.437740 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 0.283 seconds
[0m20:23:48.438453 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.285 seconds
[0m20:23:48.442585 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d40c0d0>]}
[0m20:23:48.442915 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d5f7610>]}
[0m20:23:48.443758 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d34d610>]}
[0m20:23:48.443517 [info ] [Thread-2  ]: 2 of 32 OK created sql view model DEV_pipeline_c.stg_benchmarks ................ [[32mSUCCESS 1[0m in 0.35s]
[0m20:23:48.444350 [info ] [Thread-3  ]: 3 of 32 OK created sql view model DEV_pipeline_b.stg_brokers ................... [[32mSUCCESS 1[0m in 0.36s]
[0m20:23:48.444845 [info ] [Thread-1  ]: 1 of 32 OK created sql view model DEV_pipeline_c.stg_benchmark_returns ......... [[32mSUCCESS 1[0m in 0.36s]
[0m20:23:48.445337 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.stg_benchmarks
[0m20:23:48.445757 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.stg_brokers
[0m20:23:48.446187 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.stg_benchmark_returns
[0m20:23:48.446539 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.stg_fund_hierarchy
[0m20:23:48.446949 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.stg_market_prices
[0m20:23:48.447354 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks
[0m20:23:48.447804 [info ] [Thread-2  ]: 5 of 32 START sql view model DEV_pipeline_c.stg_fund_hierarchy ................. [RUN]
[0m20:23:48.448228 [info ] [Thread-3  ]: 6 of 32 START sql view model DEV_pipeline_b.stg_market_prices .................. [RUN]
[0m20:23:48.448637 [info ] [Thread-1  ]: 7 of 32 START sql view model DEV_pipeline_c.stg_portfolio_benchmarks ........... [RUN]
[0m20:23:48.449075 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_benchmarks, now model.bain_capital_portfolio_analytics.stg_fund_hierarchy)
[0m20:23:48.449401 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_brokers, now model.bain_capital_portfolio_analytics.stg_market_prices)
[0m20:23:48.449724 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_benchmark_returns, now model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks)
[0m20:23:48.450040 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_fund_hierarchy
[0m20:23:48.450388 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_market_prices
[0m20:23:48.450692 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks
[0m20:23:48.455139 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_fund_hierarchy"
[0m20:23:48.459602 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_market_prices"
[0m20:23:48.462315 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks"
[0m20:23:48.462998 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.stg_fund_hierarchy
[0m20:23:48.466075 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_fund_hierarchy"
[0m20:23:48.466518 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.stg_market_prices
[0m20:23:48.469396 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_market_prices"
[0m20:23:48.469801 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks
[0m20:23:48.472537 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks"
[0m20:23:48.473513 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_fund_hierarchy"
[0m20:23:48.473873 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.stg_fund_hierarchy: create or replace   view DBT_DEMO.DEV_pipeline_c.stg_fund_hierarchy
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: stg_fund_hierarchy
-- Description: Fund and portfolio hierarchy for roll-up reporting

with source as (
    select
        entity_id,
        entity_name,
        entity_type,
        parent_entity_id,
        hierarchy_level,
        is_active,
        created_at,
        updated_at
    from DBT_DEMO.DEV.fund_hierarchy
)

select
    entity_id,
    trim(entity_name) as entity_name,
    upper(entity_type) as entity_type,
    parent_entity_id,
    hierarchy_level,
    is_active,
    created_at,
    updated_at
from source
where is_active = true
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_fund_hierarchy"} */;
[0m20:23:48.476213 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_market_prices"
[0m20:23:48.476753 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.stg_market_prices: create or replace   view DBT_DEMO.DEV_pipeline_b.stg_market_prices
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: stg_market_prices
-- Description: Staging model for daily market prices
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-join for prior day prices (inefficient)
-- 2. Late aggregation
-- 3. Multiple window functions that could be consolidated

with source as (
    select
        security_id,
        price_date,
        open_price,
        high_price,
        low_price,
        close_price,
        volume,
        created_at
    from DBT_DEMO.DEV.market_prices
    where price_date >= '2020-01-01'
),

-- ISSUE: Self-join to get prior day price (should use LAG)
with_prior_day as (
    select
        curr.security_id,
        curr.price_date,
        curr.open_price,
        curr.high_price,
        curr.low_price,
        curr.close_price,
        curr.volume,
        prev.close_price as prior_close,
        prev.volume as prior_volume
    from source curr
    left join source prev
        on curr.security_id = prev.security_id
        and curr.price_date = dateadd('day', 1, prev.price_date)  -- ISSUE: Doesn't handle weekends
),

-- ISSUE: Multiple separate window functions
with_returns as (
    select
        *,
        -- Daily return
        case
            when prior_close > 0
            then (close_price - prior_close) / prior_close
            else null
        end as daily_return,
        -- ISSUE: These could be computed together
        avg(close_price) over (
            partition by security_id
            order by price_date
            rows between 19 preceding and current row
        ) as ma_20,
        avg(close_price) over (
            partition by security_id
            order by price_date
            rows between 49 preceding and current row
        ) as ma_50,
        avg(close_price) over (
            partition by security_id
            order by price_date
            rows between 199 preceding and current row
        ) as ma_200,
        stddev(close_price) over (
            partition by security_id
            order by price_date
            rows between 19 preceding and current row
        ) as volatility_20d,
        avg(volume) over (
            partition by security_id
            order by price_date
            rows between 19 preceding and current row
        ) as avg_volume_20d
    from with_prior_day
),

-- ISSUE: Another pass for more calculations
final as (
    select
        *,
        case
            when ma_20 > ma_50 and ma_50 > ma_200 then 'BULLISH'
            when ma_20 < ma_50 and ma_50 < ma_200 then 'BEARISH'
            else 'NEUTRAL'
        end as trend_signal,
        case
            when volume > avg_volume_20d * 2 then 'HIGH'
            when volume < avg_volume_20d * 0.5 then 'LOW'
            else 'NORMAL'
        end as volume_signal
    from with_returns
)

select * from final
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_market_prices"} */;
[0m20:23:48.477718 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks"
[0m20:23:48.478442 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks: create or replace   view DBT_DEMO.DEV_pipeline_c.stg_portfolio_benchmarks
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: stg_portfolio_benchmarks
-- Description: Mapping of portfolios to their benchmarks

with source as (
    select
        portfolio_id,
        benchmark_id,
        is_primary,
        start_date,
        end_date,
        created_at,
        updated_at
    from DBT_DEMO.DEV.portfolio_benchmarks
)

select
    portfolio_id,
    benchmark_id,
    is_primary,
    cast(start_date as date) as start_date,
    cast(end_date as date) as end_date,
    created_at,
    updated_at
from source
where end_date is null or end_date >= current_date()
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks"} */;
[0m20:23:48.496989 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 0.342 seconds
[0m20:23:48.498655 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d1fdbb0>]}
[0m20:23:48.499180 [info ] [Thread-4  ]: 4 of 32 OK created sql view model DEV_pipeline_a.stg_cashflows ................. [[32mSUCCESS 1[0m in 0.41s]
[0m20:23:48.499653 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.stg_cashflows
[0m20:23:48.499978 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.stg_portfolios
[0m20:23:48.500416 [info ] [Thread-4  ]: 8 of 32 START sql view model DEV_pipeline_a.stg_portfolios ..................... [RUN]
[0m20:23:48.500805 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_cashflows, now model.bain_capital_portfolio_analytics.stg_portfolios)
[0m20:23:48.501112 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_portfolios
[0m20:23:48.559302 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_portfolios"
[0m20:23:48.560128 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.stg_portfolios
[0m20:23:48.563189 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_portfolios"
[0m20:23:48.564558 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_portfolios"
[0m20:23:48.564924 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.stg_portfolios: create or replace   view DBT_DEMO.DEV_pipeline_a.stg_portfolios
  
  
  
  
  as (
    -- Pipeline A: Simple Cashflow Pipeline
-- Model: stg_portfolios
-- Description: Staging model for portfolio master data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Subquery for deduplication instead of QUALIFY
-- 2. Multiple passes over data

with source as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        inception_date,
        status,
        currency,
        created_at,
        updated_at,
        row_number() over (
            partition by portfolio_id
            order by updated_at desc
        ) as rn
    from DBT_DEMO.DEV.portfolios
),

-- ISSUE: Using subquery filter instead of QUALIFY
deduplicated as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        inception_date,
        status,
        currency,
        created_at,
        updated_at
    from source
    where rn = 1  -- ISSUE: Should use QUALIFY in Snowflake
),

-- ISSUE: Another pass just for active filter
active_only as (
    select *
    from deduplicated
    where status = 'ACTIVE'
)

select * from active_only
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_portfolios"} */;
[0m20:23:48.705140 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.226 seconds
[0m20:23:48.707334 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1137ccdf0>]}
[0m20:23:48.707966 [info ] [Thread-1  ]: 7 of 32 OK created sql view model DEV_pipeline_c.stg_portfolio_benchmarks ...... [[32mSUCCESS 1[0m in 0.26s]
[0m20:23:48.708533 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks
[0m20:23:48.708914 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.stg_positions_daily
[0m20:23:48.709447 [info ] [Thread-1  ]: 9 of 32 START sql view model DEV_pipeline_c.stg_positions_daily ................ [RUN]
[0m20:23:48.709940 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks, now model.bain_capital_portfolio_analytics.stg_positions_daily)
[0m20:23:48.710295 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_positions_daily
[0m20:23:48.714115 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_positions_daily"
[0m20:23:48.714938 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.stg_positions_daily
[0m20:23:48.718339 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_positions_daily"
[0m20:23:48.720597 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_positions_daily"
[0m20:23:48.721039 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.stg_positions_daily: create or replace   view DBT_DEMO.DEV_pipeline_c.stg_positions_daily
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: stg_positions_daily
-- Description: Daily position snapshots from source system
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Heavy transformations before filtering
-- 2. Unnecessary type conversions
-- 3. Could push filters upstream

with source as (
    select
        position_id,
        portfolio_id,
        security_id,
        position_date,
        quantity,
        cost_basis_price,
        cost_basis_value,
        market_price,
        market_value,
        market_value_usd,
        unrealized_pnl,
        unrealized_pnl_pct,
        weight_pct,
        created_at,
        updated_at
    from DBT_DEMO.DEV.positions_daily
),

-- ISSUE: Transformations applied to all rows before filter
transformed as (
    select
        position_id,
        portfolio_id,
        security_id,
        cast(position_date as date) as position_date,
        cast(quantity as decimal(18,6)) as quantity,
        cast(cost_basis_price as decimal(18,4)) as cost_basis_price,
        cast(cost_basis_value as decimal(18,2)) as cost_basis_value,
        cast(market_price as decimal(18,4)) as market_price,
        cast(market_value as decimal(18,2)) as market_value,
        cast(market_value_usd as decimal(18,2)) as market_value_usd,
        cast(unrealized_pnl as decimal(18,2)) as unrealized_pnl,
        cast(unrealized_pnl_pct as decimal(10,4)) as unrealized_pnl_pct,
        cast(weight_pct as decimal(10,6)) as weight_pct,
        created_at,
        updated_at
    from source
),

-- ISSUE: Filter applied last
filtered as (
    select *
    from transformed
    where position_date >= '2020-01-01'
)

select * from filtered
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_positions_daily"} */;
[0m20:23:48.738403 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 0.260 seconds
[0m20:23:48.740268 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11376eb80>]}
[0m20:23:48.740838 [info ] [Thread-3  ]: 6 of 32 OK created sql view model DEV_pipeline_b.stg_market_prices ............. [[32mSUCCESS 1[0m in 0.29s]
[0m20:23:48.741327 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.stg_market_prices
[0m20:23:48.741673 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.stg_securities
[0m20:23:48.742130 [info ] [Thread-3  ]: 10 of 32 START sql view model DEV_pipeline_b.stg_securities .................... [RUN]
[0m20:23:48.742542 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_market_prices, now model.bain_capital_portfolio_analytics.stg_securities)
[0m20:23:48.742861 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_securities
[0m20:23:48.746076 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_securities"
[0m20:23:48.746690 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.stg_securities
[0m20:23:48.751391 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_securities"
[0m20:23:48.753502 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_securities"
[0m20:23:48.753918 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.stg_securities: create or replace   view DBT_DEMO.DEV_pipeline_b.stg_securities
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: stg_securities
-- Description: Staging model for security master data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Nested subqueries instead of QUALIFY
-- 2. Multiple deduplication passes

with source as (
    select
        security_id,
        ticker,
        security_name,
        security_type,
        asset_class,
        sector,
        industry,
        currency,
        exchange,
        is_active,
        created_at,
        updated_at
    from DBT_DEMO.DEV.securities
),

-- ISSUE: Complex deduplication using subquery
deduplicated as (
    select *
    from (
        select
            *,
            row_number() over (
                partition by security_id
                order by updated_at desc
            ) as rn
        from source
    ) sub
    where rn = 1  -- ISSUE: Should use QUALIFY
),

-- ISSUE: Separate CTE for type standardization
standardized as (
    select
        security_id,
        upper(trim(ticker)) as ticker,
        trim(security_name) as security_name,
        -- ISSUE: Repeated CASE logic found in other models
        case
            when security_type in ('STOCK', 'EQUITY', 'COMMON') then 'EQUITY'
            when security_type in ('BOND', 'NOTE', 'DEBENTURE') then 'FIXED_INCOME'
            when security_type in ('OPTION', 'FUTURE', 'SWAP') then 'DERIVATIVE'
            when security_type in ('ETF', 'MUTUAL_FUND') then 'FUND'
            else 'OTHER'
        end as security_type_standardized,
        security_type as security_type_original,
        upper(asset_class) as asset_class,
        sector,
        industry,
        upper(currency) as currency,
        exchange,
        is_active,
        created_at,
        updated_at
    from deduplicated
)

select * from standardized
where is_active = true
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_securities"} */;
[0m20:23:48.797153 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.321 seconds
[0m20:23:48.798899 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109db4eb0>]}
[0m20:23:48.799485 [info ] [Thread-2  ]: 5 of 32 OK created sql view model DEV_pipeline_c.stg_fund_hierarchy ............ [[32mSUCCESS 1[0m in 0.35s]
[0m20:23:48.800019 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.stg_fund_hierarchy
[0m20:23:48.800364 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.stg_trades
[0m20:23:48.800851 [info ] [Thread-2  ]: 11 of 32 START sql view model DEV_pipeline_b.stg_trades ........................ [RUN]
[0m20:23:48.801319 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_fund_hierarchy, now model.bain_capital_portfolio_analytics.stg_trades)
[0m20:23:48.801664 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_trades
[0m20:23:48.805230 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_trades"
[0m20:23:48.805885 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.stg_trades
[0m20:23:48.809186 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_trades"
[0m20:23:48.811241 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_trades"
[0m20:23:48.811788 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.stg_trades: create or replace   view DBT_DEMO.DEV_pipeline_b.stg_trades
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: stg_trades
-- Description: Staging model for trade transactions
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Complex CASE statements that repeat
-- 2. Multiple CTEs doing similar transformations
-- 3. Unnecessary string operations

with source as (
    select
        trade_id,
        portfolio_id,
        security_id,
        broker_id,
        trade_date,
        settlement_date,
        trade_type,
        quantity,
        price,
        gross_amount,
        commission,
        fees,
        net_amount,
        currency,
        created_at,
        updated_at
    from DBT_DEMO.DEV.trades
),

-- ISSUE: Repeated CASE statements for trade categorization
categorized as (
    select
        *,
        -- ISSUE: This logic is repeated in multiple models
        case
            when trade_type in ('BUY', 'COVER') then 'PURCHASE'
            when trade_type in ('SELL', 'SHORT') then 'SALE'
            when trade_type in ('DIVIDEND', 'INTEREST') then 'INCOME'
            else 'OTHER'
        end as trade_category,
        case
            when abs(net_amount) >= 10000000 then 'LARGE'
            when abs(net_amount) >= 1000000 then 'MEDIUM'
            when abs(net_amount) >= 100000 then 'SMALL'
            else 'MICRO'
        end as trade_size_bucket,
        -- ISSUE: Redundant string manipulation
        upper(trim(trade_type)) as trade_type_clean,
        upper(trim(currency)) as currency_clean
    from source
),

-- ISSUE: Another pass just for date calculations
with_dates as (
    select
        *,
        datediff('day', trade_date, settlement_date) as settlement_days,
        date_trunc('month', trade_date) as trade_month,
        date_trunc('quarter', trade_date) as trade_quarter,
        extract(year from trade_date) as trade_year,
        extract(month from trade_date) as trade_month_num,
        dayofweek(trade_date) as trade_day_of_week
    from categorized
),

-- ISSUE: Late filtering
filtered as (
    select *
    from with_dates
    where trade_date >= '2020-01-01'
)

select * from filtered
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_trades"} */;
[0m20:23:48.813137 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 0.248 seconds
[0m20:23:48.815132 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d1fdbb0>]}
[0m20:23:48.815732 [info ] [Thread-4  ]: 8 of 32 OK created sql view model DEV_pipeline_a.stg_portfolios ................ [[32mSUCCESS 1[0m in 0.31s]
[0m20:23:48.816606 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.stg_portfolios
[0m20:23:48.817063 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.stg_valuations
[0m20:23:48.817617 [info ] [Thread-4  ]: 12 of 32 START sql view model DEV_pipeline_c.stg_valuations .................... [RUN]
[0m20:23:48.818201 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_portfolios, now model.bain_capital_portfolio_analytics.stg_valuations)
[0m20:23:48.818550 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_valuations
[0m20:23:48.822053 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_valuations"
[0m20:23:48.822701 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.stg_valuations
[0m20:23:48.826618 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_valuations"
[0m20:23:48.828548 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_valuations"
[0m20:23:48.828931 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.stg_valuations: create or replace   view DBT_DEMO.DEV_pipeline_c.stg_valuations
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: stg_valuations
-- Description: Portfolio valuation data (NAV, etc.)
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Deduplication via subquery
-- 2. Heavy calculations before filtering

with source as (
    select
        valuation_id,
        portfolio_id,
        valuation_date,
        nav,
        nav_per_share,
        shares_outstanding,
        gross_assets,
        total_liabilities,
        net_assets,
        currency,
        fx_rate_to_usd,
        nav_usd,
        created_at,
        updated_at
    from DBT_DEMO.DEV.valuations
),

-- ISSUE: Deduplication using subquery
deduplicated as (
    select *
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id, valuation_date
                order by updated_at desc
            ) as rn
        from source
    )
    where rn = 1
),

-- ISSUE: Filter applied after deduplication
filtered as (
    select
        valuation_id,
        portfolio_id,
        cast(valuation_date as date) as valuation_date,
        cast(nav as decimal(18,2)) as nav,
        cast(nav_per_share as decimal(18,6)) as nav_per_share,
        cast(shares_outstanding as decimal(18,6)) as shares_outstanding,
        cast(gross_assets as decimal(18,2)) as gross_assets,
        cast(total_liabilities as decimal(18,2)) as total_liabilities,
        cast(net_assets as decimal(18,2)) as net_assets,
        upper(currency) as currency,
        cast(fx_rate_to_usd as decimal(18,8)) as fx_rate_to_usd,
        cast(nav_usd as decimal(18,2)) as nav_usd,
        created_at,
        updated_at
    from deduplicated
    where valuation_date >= '2020-01-01'
)

select * from filtered
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_valuations"} */;
[0m20:23:49.054829 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.333 seconds
[0m20:23:49.058513 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c110fa0>]}
[0m20:23:49.059486 [info ] [Thread-1  ]: 9 of 32 OK created sql view model DEV_pipeline_c.stg_positions_daily ........... [[32mSUCCESS 1[0m in 0.35s]
[0m20:23:49.060058 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.stg_positions_daily
[0m20:23:49.060532 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m20:23:49.061194 [info ] [Thread-1  ]: 13 of 32 START sql table model DEV_pipeline_a.fact_cashflow_summary ............ [RUN]
[0m20:23:49.061758 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_positions_daily, now model.bain_capital_portfolio_analytics.fact_cashflow_summary)
[0m20:23:49.062200 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m20:23:49.071114 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_cashflow_summary"
[0m20:23:49.073879 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 0.319 seconds
[0m20:23:49.076778 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11376eb80>]}
[0m20:23:49.077525 [info ] [Thread-3  ]: 10 of 32 OK created sql view model DEV_pipeline_b.stg_securities ............... [[32mSUCCESS 1[0m in 0.33s]
[0m20:23:49.078000 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m20:23:49.078566 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.stg_securities
[0m20:23:49.102663 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_cashflow_summary"
[0m20:23:49.103642 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.int_position_attribution
[0m20:23:49.104770 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 0.275 seconds
[0m20:23:49.104112 [info ] [Thread-3  ]: 14 of 32 START sql view model DEV_pipeline_c.int_position_attribution .......... [RUN]
[0m20:23:49.106628 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1137e6eb0>]}
[0m20:23:49.107023 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_securities, now model.bain_capital_portfolio_analytics.int_position_attribution)
[0m20:23:49.107578 [info ] [Thread-4  ]: 12 of 32 OK created sql view model DEV_pipeline_c.stg_valuations ............... [[32mSUCCESS 1[0m in 0.29s]
[0m20:23:49.107937 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.int_position_attribution
[0m20:23:49.108411 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.stg_valuations
[0m20:23:49.112052 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_position_attribution"
[0m20:23:49.119814 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_cashflow_summary"
[0m20:23:49.120747 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.fact_cashflow_summary: create or replace transient table DBT_DEMO.DEV_pipeline_a.fact_cashflow_summary
    
    
    
    as (-- Pipeline A: Simple Cashflow Pipeline
-- Model: fact_cashflow_summary
-- Description: Fact table summarizing cashflows by portfolio and month
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-joins for prior period comparisons (should use LAG)
-- 2. Late aggregation (aggregates after full join)
-- 3. Repeated window functions with same partitions
-- 4. Redundant date calculations per row
-- 5. Correlated subqueries for fund-level totals

with cashflows as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_cashflows
),

portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

-- ISSUE: Full join before aggregation (scans all rows)
joined as (
    select
        c.cashflow_id,
        c.portfolio_id,
        p.portfolio_name,
        p.portfolio_type,
        p.fund_id,
        c.cashflow_type,
        c.cashflow_date,
        c.amount,
        c.currency,
        -- ISSUE: Redundant date calculations done per row
        date_trunc('month', c.cashflow_date) as cashflow_month,
        date_trunc('quarter', c.cashflow_date) as cashflow_quarter,
        date_trunc('year', c.cashflow_date) as cashflow_year,
        extract(year from c.cashflow_date) as year_num,
        extract(month from c.cashflow_date) as month_num,
        extract(quarter from c.cashflow_date) as quarter_num,
        extract(dayofmonth from c.cashflow_date) as day_num
    from cashflows c
    inner join portfolios p
        on c.portfolio_id = p.portfolio_id
),

-- ISSUE: Aggregation happens after full row-level join
aggregated as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        cashflow_month,
        cashflow_quarter,
        cashflow_year,
        year_num,
        month_num,
        quarter_num,
        cashflow_type,
        currency,
        count(*) as transaction_count,
        count(distinct cashflow_id) as unique_transactions,
        sum(amount) as total_amount,
        avg(amount) as avg_amount,
        min(amount) as min_amount,
        max(amount) as max_amount,
        stddev(amount) as stddev_amount,
        -- ISSUE: Percentile calculations (slow)
        percentile_cont(0.25) within group (order by amount) as p25_amount,
        percentile_cont(0.50) within group (order by amount) as median_amount,
        percentile_cont(0.75) within group (order by amount) as p75_amount
    from joined
    group by 1,2,3,4,5,6,7,8,9,10,11,12  -- ISSUE: Non-descriptive GROUP BY
),

-- ISSUE: Self-join for prior month comparisons (should use LAG)
with_prior_months as (
    select
        agg.*,
        -- ISSUE: Self-join for prior month
        agg_m1.total_amount as prior_1m_total,
        agg_m1.transaction_count as prior_1m_count,
        -- ISSUE: Self-join for 3 months ago
        agg_m3.total_amount as prior_3m_total,
        -- ISSUE: Self-join for 6 months ago
        agg_m6.total_amount as prior_6m_total,
        -- ISSUE: Self-join for 12 months ago
        agg_m12.total_amount as prior_12m_total
    from aggregated agg
    left join aggregated agg_m1
        on agg.portfolio_id = agg_m1.portfolio_id
        and agg.cashflow_type = agg_m1.cashflow_type
        and agg.currency = agg_m1.currency
        and agg_m1.cashflow_month = dateadd(month, -1, agg.cashflow_month)
    left join aggregated agg_m3
        on agg.portfolio_id = agg_m3.portfolio_id
        and agg.cashflow_type = agg_m3.cashflow_type
        and agg.currency = agg_m3.currency
        and agg_m3.cashflow_month = dateadd(month, -3, agg.cashflow_month)
    left join aggregated agg_m6
        on agg.portfolio_id = agg_m6.portfolio_id
        and agg.cashflow_type = agg_m6.cashflow_type
        and agg.currency = agg_m6.currency
        and agg_m6.cashflow_month = dateadd(month, -6, agg.cashflow_month)
    left join aggregated agg_m12
        on agg.portfolio_id = agg_m12.portfolio_id
        and agg.cashflow_type = agg_m12.cashflow_type
        and agg.currency = agg_m12.currency
        and agg_m12.cashflow_month = dateadd(month, -12, agg.cashflow_month)
),

-- ISSUE: Multiple window functions with repeated partitions
with_window_calcs as (
    select
        wpm.*,
        -- ISSUE: Running totals (repeated partition)
        sum(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_total,
        sum(transaction_count) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_count,
        -- ISSUE: Moving averages (same partition repeated)
        avg(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 2 preceding and current row
        ) as rolling_3m_avg,
        avg(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 5 preceding and current row
        ) as rolling_6m_avg,
        avg(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_avg,
        -- ISSUE: More window calculations
        stddev(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_stddev,
        min(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_min,
        max(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_max
    from with_prior_months wpm
),

-- ISSUE: Correlated subqueries for fund-level context (very slow)
with_fund_context as (
    select
        wwc.*,
        -- ISSUE: Correlated subquery for fund total
        (
            select sum(total_amount)
            from aggregated agg2
            inner join portfolios p2
                on agg2.portfolio_id = p2.portfolio_id
            where p2.fund_id = wwc.fund_id
            and agg2.cashflow_month = wwc.cashflow_month
            and agg2.cashflow_type = wwc.cashflow_type
        ) as fund_total_amount,
        -- ISSUE: Another correlated subquery for portfolio count
        (
            select count(distinct agg2.portfolio_id)
            from aggregated agg2
            inner join portfolios p2
                on agg2.portfolio_id = p2.portfolio_id
            where p2.fund_id = wwc.fund_id
            and agg2.cashflow_month = wwc.cashflow_month
            and agg2.cashflow_type = wwc.cashflow_type
        ) as fund_portfolio_count
    from with_window_calcs wwc
),

-- ISSUE: Complex derived metrics with repeated CASE statements
final as (
    select
        md5(cast(coalesce(cast(wfc.portfolio_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wfc.cashflow_month as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wfc.cashflow_type as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wfc.currency as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as cashflow_summary_key,
        wfc.*,
        -- ISSUE: Portfolio share of fund (repeated division)
        case
            when wfc.fund_total_amount > 0
            then (wfc.total_amount / wfc.fund_total_amount) * 100
            else null
        end as portfolio_share_of_fund_pct,
        -- ISSUE: Month-over-month growth calculations
        case
            when wfc.prior_1m_total is not null and wfc.prior_1m_total != 0
            then ((wfc.total_amount - wfc.prior_1m_total) / abs(wfc.prior_1m_total)) * 100
            else null
        end as mom_growth_pct,
        case
            when wfc.prior_3m_total is not null and wfc.prior_3m_total != 0
            then ((wfc.total_amount - wfc.prior_3m_total) / abs(wfc.prior_3m_total)) * 100
            else null
        end as growth_3m_pct,
        case
            when wfc.prior_12m_total is not null and wfc.prior_12m_total != 0
            then ((wfc.total_amount - wfc.prior_12m_total) / abs(wfc.prior_12m_total)) * 100
            else null
        end as yoy_growth_pct,
        -- ISSUE: Trend classification (complex nested CASE)
        case
            when wfc.rolling_3m_avg > wfc.rolling_12m_avg * 1.3 then 'ACCELERATING'
            when wfc.rolling_3m_avg > wfc.rolling_12m_avg * 1.1 then 'GROWING'
            when wfc.rolling_3m_avg < wfc.rolling_12m_avg * 0.7 then 'DECLINING_FAST'
            when wfc.rolling_3m_avg < wfc.rolling_12m_avg * 0.9 then 'DECLINING'
            else 'STABLE'
        end as trend_classification,
        -- ISSUE: Volatility classification
        case
            when wfc.rolling_12m_stddev < wfc.rolling_12m_avg * 0.1 then 'LOW_VOLATILITY'
            when wfc.rolling_12m_stddev < wfc.rolling_12m_avg * 0.3 then 'MODERATE_VOLATILITY'
            when wfc.rolling_12m_stddev < wfc.rolling_12m_avg * 0.5 then 'HIGH_VOLATILITY'
            else 'VERY_HIGH_VOLATILITY'
        end as volatility_classification,
        -- ISSUE: Size classification (repeated CASE)
        case
            when abs(wfc.total_amount) >= 10000000 then 'MEGA'
            when abs(wfc.total_amount) >= 5000000 then 'LARGE'
            when abs(wfc.total_amount) >= 1000000 then 'MEDIUM'
            when abs(wfc.total_amount) >= 100000 then 'SMALL'
            else 'MICRO'
        end as transaction_size_category
    from with_fund_context wfc
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_cashflow_summary"} */;
[0m20:23:49.121475 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.int_benchmark_aligned
[0m20:23:49.122095 [info ] [Thread-4  ]: 15 of 32 START sql view model DEV_pipeline_c.int_benchmark_aligned ............. [RUN]
[0m20:23:49.122808 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_valuations, now model.bain_capital_portfolio_analytics.int_benchmark_aligned)
[0m20:23:49.123508 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.int_benchmark_aligned
[0m20:23:49.128065 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_benchmark_aligned"
[0m20:23:49.129247 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.317 seconds
[0m20:23:49.130937 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109db4eb0>]}
[0m20:23:49.131387 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.int_position_attribution
[0m20:23:49.135162 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_position_attribution"
[0m20:23:49.131830 [info ] [Thread-2  ]: 11 of 32 OK created sql view model DEV_pipeline_b.stg_trades ................... [[32mSUCCESS 1[0m in 0.33s]
[0m20:23:49.135789 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.int_benchmark_aligned
[0m20:23:49.136354 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.stg_trades
[0m20:23:49.139370 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_benchmark_aligned"
[0m20:23:49.140051 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.int_portfolio_returns_daily
[0m20:23:49.142370 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_position_attribution"
[0m20:23:49.142801 [info ] [Thread-2  ]: 16 of 32 START sql view model DEV_pipeline_c.int_portfolio_returns_daily ....... [RUN]
[0m20:23:49.144352 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_benchmark_aligned"
[0m20:23:49.144787 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.int_position_attribution: create or replace   view DBT_DEMO.DEV_pipeline_c.int_position_attribution
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: int_position_attribution
-- Description: Attribution analysis at position level
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Heavy multi-way join
-- 2. Complex attribution calculations
-- 3. Multiple window functions

with positions as (
    select * from DBT_DEMO.DEV_pipeline_c.stg_positions_daily
),

securities as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_securities
),

market_prices as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_market_prices
),

-- ISSUE: Heavy 3-way join
enriched_positions as (
    select
        p.portfolio_id,
        p.security_id,
        p.position_date,
        p.quantity,
        p.market_value_usd,
        p.weight_pct,
        s.ticker,
        s.security_type_standardized as security_type,
        s.asset_class,
        s.sector,
        s.industry,
        mp.daily_return as security_return,
        mp.close_price,
        mp.ma_20,
        mp.ma_50,
        mp.volatility_20d
    from positions p
    inner join securities s
        on p.security_id = s.security_id
    left join market_prices mp
        on p.security_id = mp.security_id
        and p.position_date = mp.price_date
),

-- ISSUE: Window functions for prior day weight
with_prior_weight as (
    select
        *,
        lag(weight_pct, 1) over (
            partition by portfolio_id, security_id
            order by position_date
        ) as prior_weight_pct,
        lag(market_value_usd, 1) over (
            partition by portfolio_id, security_id
            order by position_date
        ) as prior_market_value
    from enriched_positions
),

-- ISSUE: Attribution calculations
with_attribution as (
    select
        *,
        -- Contribution to return
        coalesce(prior_weight_pct, weight_pct) * coalesce(security_return, 0) as contribution_to_return,
        -- Allocation effect (simplified Brinson)
        (weight_pct - coalesce(prior_weight_pct, weight_pct)) * coalesce(security_return, 0) as allocation_effect,
        -- Position P&L
        market_value_usd - coalesce(prior_market_value, market_value_usd) as position_pnl
    from with_prior_weight
)

select * from with_attribution
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_position_attribution"} */;
[0m20:23:49.145272 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_trades, now model.bain_capital_portfolio_analytics.int_portfolio_returns_daily)
[0m20:23:49.145654 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.int_benchmark_aligned: create or replace   view DBT_DEMO.DEV_pipeline_c.int_benchmark_aligned
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: int_benchmark_aligned
-- Description: Align benchmark returns with portfolio dates for comparison
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Heavy join between portfolios and benchmarks
-- 2. Could pre-filter benchmarks

with portfolio_dates as (
    select distinct
        portfolio_id,
        valuation_date
    from DBT_DEMO.DEV_pipeline_c.stg_valuations
),

portfolio_benchmarks as (
    select * from DBT_DEMO.DEV_pipeline_c.stg_portfolio_benchmarks
),

benchmark_returns as (
    select * from DBT_DEMO.DEV_pipeline_c.stg_benchmark_returns
),

-- ISSUE: Cross-join like pattern (portfolio dates x benchmarks)
aligned as (
    select
        pd.portfolio_id,
        pd.valuation_date,
        pb.benchmark_id,
        1.0 as benchmark_weight,  -- Default weight since not in source
        pb.is_primary,
        br.daily_return as benchmark_daily_return,
        br.cumulative_return as benchmark_cumulative_return,
        br.return_30d as benchmark_return_30d,
        br.return_90d as benchmark_return_90d,
        br.return_1y as benchmark_return_1y,
        br.annualized_volatility as benchmark_volatility
    from portfolio_dates pd
    inner join portfolio_benchmarks pb
        on pd.portfolio_id = pb.portfolio_id
        and pd.valuation_date >= pb.start_date
        and (pb.end_date is null or pd.valuation_date <= pb.end_date)
    left join benchmark_returns br
        on pb.benchmark_id = br.benchmark_id
        and pd.valuation_date = br.return_date
)

select * from aligned
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_benchmark_aligned"} */;
[0m20:23:49.146140 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.int_portfolio_returns_daily
[0m20:23:49.150367 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_portfolio_returns_daily"
[0m20:23:49.151699 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.int_portfolio_returns_daily
[0m20:23:49.155134 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_portfolio_returns_daily"
[0m20:23:49.159502 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_portfolio_returns_daily"
[0m20:23:49.160121 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.int_portfolio_returns_daily: create or replace   view DBT_DEMO.DEV_pipeline_c.int_portfolio_returns_daily
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: int_portfolio_returns_daily
-- Description: Calculate daily portfolio returns from NAV
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-join for prior day NAV (should use LAG)
-- 2. Multiple passes for return calculations
-- 3. Complex window functions

with valuations as (
    select * from DBT_DEMO.DEV_pipeline_c.stg_valuations
),

cashflows as (
    select
        portfolio_id,
        cashflow_date,
        sum(case when cashflow_type = 'CONTRIBUTION' then amount else 0 end) as contributions,
        sum(case when cashflow_type = 'DISTRIBUTION' then amount else 0 end) as distributions
    from DBT_DEMO.DEV_pipeline_a.stg_cashflows
    group by 1, 2
),

-- ISSUE: Self-join instead of LAG for prior NAV
with_prior_nav as (
    select
        curr.portfolio_id,
        curr.valuation_date,
        curr.nav,
        curr.nav_usd,
        prev.nav as prior_nav,
        prev.nav_usd as prior_nav_usd,
        coalesce(cf.contributions, 0) as contributions,
        coalesce(cf.distributions, 0) as distributions
    from valuations curr
    left join valuations prev
        on curr.portfolio_id = prev.portfolio_id
        and curr.valuation_date = dateadd('day', 1, prev.valuation_date)
    left join cashflows cf
        on curr.portfolio_id = cf.portfolio_id
        and curr.valuation_date = cf.cashflow_date
),

-- ISSUE: Modified Dietz calculation done inefficiently
with_daily_return as (
    select
        portfolio_id,
        valuation_date,
        nav,
        nav_usd,
        prior_nav,
        prior_nav_usd,
        contributions,
        distributions,
        -- Simple return
        case
            when prior_nav > 0
            then (nav - prior_nav - contributions + distributions) / prior_nav
            else null
        end as daily_return_simple,
        -- Modified Dietz (approximation)
        case
            when (prior_nav + contributions * 0.5) > 0
            then (nav - prior_nav - contributions + distributions) / (prior_nav + contributions * 0.5 - distributions * 0.5)
            else null
        end as daily_return_mod_dietz
    from with_prior_nav
),

-- ISSUE: Multiple window functions for different periods
with_rolling_returns as (
    select
        *,
        -- Cumulative return using log returns
        exp(sum(ln(1 + coalesce(daily_return_mod_dietz, 0))) over (
            partition by portfolio_id
            order by valuation_date
            rows between unbounded preceding and current row
        )) - 1 as cumulative_return,
        -- Rolling period returns
        exp(sum(ln(1 + coalesce(daily_return_mod_dietz, 0))) over (
            partition by portfolio_id
            order by valuation_date
            rows between 6 preceding and current row
        )) - 1 as return_1w,
        exp(sum(ln(1 + coalesce(daily_return_mod_dietz, 0))) over (
            partition by portfolio_id
            order by valuation_date
            rows between 29 preceding and current row
        )) - 1 as return_1m,
        exp(sum(ln(1 + coalesce(daily_return_mod_dietz, 0))) over (
            partition by portfolio_id
            order by valuation_date
            rows between 89 preceding and current row
        )) - 1 as return_3m,
        exp(sum(ln(1 + coalesce(daily_return_mod_dietz, 0))) over (
            partition by portfolio_id
            order by valuation_date
            rows between 179 preceding and current row
        )) - 1 as return_6m,
        exp(sum(ln(1 + coalesce(daily_return_mod_dietz, 0))) over (
            partition by portfolio_id
            order by valuation_date
            rows between 364 preceding and current row
        )) - 1 as return_1y,
        -- Rolling volatility
        stddev(daily_return_mod_dietz) over (
            partition by portfolio_id
            order by valuation_date
            rows between 29 preceding and current row
        ) * sqrt(252) as volatility_1m,
        stddev(daily_return_mod_dietz) over (
            partition by portfolio_id
            order by valuation_date
            rows between 251 preceding and current row
        ) * sqrt(252) as volatility_1y
    from with_daily_return
)

select * from with_rolling_returns
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_portfolio_returns_daily"} */;
[0m20:23:49.540507 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.379 seconds
[0m20:23:49.545406 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109db4eb0>]}
[0m20:23:49.546460 [info ] [Thread-2  ]: 16 of 32 OK created sql view model DEV_pipeline_c.int_portfolio_returns_daily .. [[32mSUCCESS 1[0m in 0.40s]
[0m20:23:49.547256 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.int_portfolio_returns_daily
[0m20:23:49.547937 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.int_trades_enriched
[0m20:23:49.548703 [info ] [Thread-2  ]: 17 of 32 START sql view model DEV_pipeline_b.int_trades_enriched ............... [RUN]
[0m20:23:49.549467 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_portfolio_returns_daily, now model.bain_capital_portfolio_analytics.int_trades_enriched)
[0m20:23:49.549934 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.int_trades_enriched
[0m20:23:49.557160 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_trades_enriched"
[0m20:23:49.558361 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.int_trades_enriched
[0m20:23:49.561372 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_trades_enriched"
[0m20:23:49.563927 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_trades_enriched"
[0m20:23:49.564353 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.int_trades_enriched: create or replace   view DBT_DEMO.DEV_pipeline_b.int_trades_enriched
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: int_trades_enriched
-- Description: Intermediate model enriching trades with security and price data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Multiple heavy joins done row-by-row
-- 2. Price lookup repeated for every trade
-- 3. Could pre-aggregate before joining

with trades as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_trades
),

securities as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_securities
),

market_prices as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_market_prices
),

brokers as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_brokers
),

-- ISSUE: Heavy multi-way join before any aggregation
enriched as (
    select
        t.trade_id,
        t.portfolio_id,
        t.security_id,
        t.trade_date,
        t.settlement_date,
        t.trade_type,
        t.trade_category,
        t.trade_size_bucket,
        t.quantity,
        t.price as execution_price,
        t.gross_amount,
        t.commission,
        t.fees,
        t.net_amount,
        t.currency,
        t.settlement_days,
        t.trade_month,
        t.trade_quarter,
        t.trade_year,
        -- Security attributes
        s.ticker,
        s.security_name,
        s.security_type_standardized as security_type,
        s.asset_class,
        s.sector,
        s.industry,
        -- Broker attributes
        b.broker_name,
        b.broker_type,
        b.region as broker_region,
        b.commission_rate as standard_commission_rate,
        -- Market price on trade date
        mp.close_price as market_close_price,
        mp.ma_20,
        mp.ma_50,
        mp.volatility_20d,
        mp.trend_signal,
        mp.volume_signal,
        -- ISSUE: These calculations done per row
        case
            when mp.close_price > 0
            then (t.price - mp.close_price) / mp.close_price * 100
            else null
        end as execution_vs_close_pct,
        case
            when t.price > mp.close_price then 'ABOVE_MARKET'
            when t.price < mp.close_price then 'BELOW_MARKET'
            else 'AT_MARKET'
        end as execution_quality,
        -- Cost analysis
        t.commission + t.fees as total_costs,
        case
            when t.gross_amount > 0
            then (t.commission + t.fees) / t.gross_amount * 10000
            else null
        end as cost_bps
    from trades t
    inner join securities s
        on t.security_id = s.security_id
    left join brokers b
        on t.broker_id = b.broker_id
    left join market_prices mp
        on t.security_id = mp.security_id
        and t.trade_date = mp.price_date
)

select * from enriched
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_trades_enriched"} */;
[0m20:23:49.597353 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 0.451 seconds
[0m20:23:49.599551 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11370beb0>]}
[0m20:23:49.600131 [info ] [Thread-3  ]: 14 of 32 OK created sql view model DEV_pipeline_c.int_position_attribution ..... [[32mSUCCESS 1[0m in 0.49s]
[0m20:23:49.600617 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.int_position_attribution
[0m20:23:49.600956 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.int_risk_metrics
[0m20:23:49.601341 [info ] [Thread-3  ]: 18 of 32 START sql view model DEV_pipeline_c.int_risk_metrics .................. [RUN]
[0m20:23:49.601852 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_position_attribution, now model.bain_capital_portfolio_analytics.int_risk_metrics)
[0m20:23:49.602197 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.int_risk_metrics
[0m20:23:49.605766 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_risk_metrics"
[0m20:23:49.606615 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.int_risk_metrics
[0m20:23:49.609703 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_risk_metrics"
[0m20:23:49.612366 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_risk_metrics"
[0m20:23:49.612821 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.int_risk_metrics: create or replace   view DBT_DEMO.DEV_pipeline_c.int_risk_metrics
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: int_risk_metrics
-- Description: Calculate risk metrics for portfolios
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Multiple window functions with same partition
-- 2. VaR calculation inefficiencies
-- 3. Could pre-compute some metrics

with portfolio_returns as (
    select * from DBT_DEMO.DEV_pipeline_c.int_portfolio_returns_daily
),

-- ISSUE: Multiple passes for different risk calculations
with_risk_metrics as (
    select
        portfolio_id,
        valuation_date,
        daily_return_mod_dietz as daily_return,
        nav_usd,
        -- ISSUE: Repeated window frame definitions
        -- Max drawdown components
        max(nav_usd) over (
            partition by portfolio_id
            order by valuation_date
            rows between unbounded preceding and current row
        ) as running_max_nav,
        -- Downside deviation
        sqrt(avg(
            case when daily_return_mod_dietz < 0 then power(daily_return_mod_dietz, 2) else 0 end
        ) over (
            partition by portfolio_id
            order by valuation_date
            rows between 251 preceding and current row
        )) * sqrt(252) as downside_deviation_1y,
        -- Sortino components
        avg(daily_return_mod_dietz) over (
            partition by portfolio_id
            order by valuation_date
            rows between 251 preceding and current row
        ) * 252 as annualized_return_1y,
        volatility_1y
    from portfolio_returns
),

-- ISSUE: Another CTE for derived metrics
with_derived as (
    select
        *,
        -- Drawdown
        (nav_usd - running_max_nav) / nullif(running_max_nav, 0) as drawdown,
        -- Sortino ratio (assuming 0% risk-free)
        case
            when downside_deviation_1y > 0
            then annualized_return_1y / downside_deviation_1y
            else null
        end as sortino_ratio,
        -- Sharpe ratio (assuming 0% risk-free)
        case
            when volatility_1y > 0
            then annualized_return_1y / volatility_1y
            else null
        end as sharpe_ratio
    from with_risk_metrics
),

-- ISSUE: Max drawdown calculation
with_max_drawdown as (
    select
        *,
        min(drawdown) over (
            partition by portfolio_id
            order by valuation_date
            rows between unbounded preceding and current row
        ) as max_drawdown
    from with_derived
),

-- ISSUE: VaR calculation (simplified parametric)
final as (
    select
        *,
        -- Parametric VaR (95%)
        nav_usd * volatility_1y / sqrt(252) * 1.645 as var_95_1d,
        -- Parametric VaR (99%)
        nav_usd * volatility_1y / sqrt(252) * 2.326 as var_99_1d
    from with_max_drawdown
)

select * from final
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_risk_metrics"} */;
[0m20:23:49.659803 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 0.513 seconds
[0m20:23:49.661987 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113705430>]}
[0m20:23:49.662621 [info ] [Thread-4  ]: 15 of 32 OK created sql view model DEV_pipeline_c.int_benchmark_aligned ........ [[32mSUCCESS 1[0m in 0.54s]
[0m20:23:49.663182 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.int_benchmark_aligned
[0m20:23:49.663575 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.fact_position_snapshot
[0m20:23:49.664032 [info ] [Thread-4  ]: 19 of 32 START sql table model DEV_pipeline_c.fact_position_snapshot ........... [RUN]
[0m20:23:49.664545 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_benchmark_aligned, now model.bain_capital_portfolio_analytics.fact_position_snapshot)
[0m20:23:49.664964 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_position_snapshot
[0m20:23:49.670241 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_position_snapshot"
[0m20:23:49.671069 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.fact_position_snapshot
[0m20:23:49.674400 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_position_snapshot"
[0m20:23:49.676393 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_position_snapshot"
[0m20:23:49.676778 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.fact_position_snapshot: create or replace transient table DBT_DEMO.DEV_pipeline_c.fact_position_snapshot
    
    
    
    as (-- Pipeline C: Complex Portfolio Analytics
-- Model: fact_position_snapshot
-- Description: Position-level fact table with attribution
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Heavy joins that duplicate upstream work
-- 2. Could be more selective in columns

with position_attribution as (
    select * from DBT_DEMO.DEV_pipeline_c.int_position_attribution
),

portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

-- ISSUE: Re-joining portfolio data
final as (
    select
        md5(cast(coalesce(cast(pa.portfolio_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(pa.security_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(pa.position_date as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as position_snapshot_key,
        p.portfolio_name,
        p.portfolio_type,
        p.fund_id,
        pa.portfolio_id,
        pa.security_id,
        pa.position_date,
        pa.ticker,
        pa.security_type,
        pa.asset_class,
        pa.sector,
        pa.industry,
        pa.quantity,
        pa.market_value_usd,
        pa.weight_pct,
        pa.close_price,
        pa.ma_20,
        pa.ma_50,
        pa.volatility_20d,
        pa.security_return,
        pa.contribution_to_return,
        pa.allocation_effect,
        pa.position_pnl,
        -- ISSUE: More date extractions
        extract(year from pa.position_date) as position_year,
        extract(month from pa.position_date) as position_month,
        date_trunc('month', pa.position_date) as position_month_start
    from position_attribution pa
    inner join portfolios p
        on pa.portfolio_id = p.portfolio_id
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_position_snapshot"} */;
[0m20:23:50.007714 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 0.394 seconds
[0m20:23:50.011485 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11370beb0>]}
[0m20:23:50.012615 [info ] [Thread-3  ]: 18 of 32 OK created sql view model DEV_pipeline_c.int_risk_metrics ............. [[32mSUCCESS 1[0m in 0.41s]
[0m20:23:50.013395 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.int_risk_metrics
[0m20:23:50.013925 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.int_sector_attribution
[0m20:23:50.014699 [info ] [Thread-3  ]: 20 of 32 START sql view model DEV_pipeline_c.int_sector_attribution ............ [RUN]
[0m20:23:50.015460 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_risk_metrics, now model.bain_capital_portfolio_analytics.int_sector_attribution)
[0m20:23:50.015892 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.int_sector_attribution
[0m20:23:50.021636 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_sector_attribution"
[0m20:23:50.022573 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.int_sector_attribution
[0m20:23:50.026934 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_sector_attribution"
[0m20:23:50.029293 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_sector_attribution"
[0m20:23:50.029745 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.int_sector_attribution: create or replace   view DBT_DEMO.DEV_pipeline_c.int_sector_attribution
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: int_sector_attribution
-- Description: Aggregate attribution to sector level
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Re-aggregation of position data
-- 2. Complex grouping logic
-- 3. Could be combined with position attribution

with position_attribution as (
    select * from DBT_DEMO.DEV_pipeline_c.int_position_attribution
),

-- ISSUE: Aggregation that could be pushed upstream
sector_daily as (
    select
        portfolio_id,
        position_date,
        sector,
        count(distinct security_id) as position_count,
        sum(market_value_usd) as sector_market_value,
        sum(weight_pct) as sector_weight,
        sum(contribution_to_return) as sector_contribution,
        sum(allocation_effect) as sector_allocation_effect,
        sum(position_pnl) as sector_pnl,
        avg(security_return) as avg_security_return
    from position_attribution
    group by 1, 2, 3
),

-- ISSUE: Window functions for rolling metrics
with_rolling as (
    select
        *,
        sum(sector_contribution) over (
            partition by portfolio_id, sector
            order by position_date
            rows between 29 preceding and current row
        ) as sector_contribution_30d,
        avg(sector_weight) over (
            partition by portfolio_id, sector
            order by position_date
            rows between 29 preceding and current row
        ) as avg_sector_weight_30d,
        lag(sector_weight, 1) over (
            partition by portfolio_id, sector
            order by position_date
        ) as prior_sector_weight
    from sector_daily
)

select * from with_rolling
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_sector_attribution"} */;
[0m20:23:50.181081 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.616 seconds
[0m20:23:50.186379 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1133be9a0>]}
[0m20:23:50.187029 [info ] [Thread-2  ]: 17 of 32 OK created sql view model DEV_pipeline_b.int_trades_enriched .......... [[32mSUCCESS 1[0m in 0.64s]
[0m20:23:50.187534 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.int_trades_enriched
[0m20:23:50.187889 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark
[0m20:23:50.188383 [info ] [Thread-2  ]: 21 of 32 START sql view model DEV_pipeline_c.int_portfolio_vs_benchmark ........ [RUN]
[0m20:23:50.188815 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_trades_enriched, now model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark)
[0m20:23:50.189211 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark
[0m20:23:50.193214 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark"
[0m20:23:50.193891 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark
[0m20:23:50.198310 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark"
[0m20:23:50.201199 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark"
[0m20:23:50.201674 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark: create or replace   view DBT_DEMO.DEV_pipeline_c.int_portfolio_vs_benchmark
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: int_portfolio_vs_benchmark
-- Description: Compare portfolio returns to benchmark
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Re-joins data that could be joined once upstream
-- 2. Excess return calculation repeated
-- 3. Complex rolling calculations

with portfolio_returns as (
    select * from DBT_DEMO.DEV_pipeline_c.int_portfolio_returns_daily
),

benchmark_aligned as (
    select * from DBT_DEMO.DEV_pipeline_c.int_benchmark_aligned
    where is_primary = true
),

-- ISSUE: Another join that combines already-processed data
combined as (
    select
        pr.portfolio_id,
        pr.valuation_date,
        pr.nav,
        pr.nav_usd,
        pr.daily_return_mod_dietz as portfolio_daily_return,
        pr.cumulative_return as portfolio_cumulative_return,
        pr.return_1m as portfolio_return_1m,
        pr.return_3m as portfolio_return_3m,
        pr.return_1y as portfolio_return_1y,
        pr.volatility_1y as portfolio_volatility,
        ba.benchmark_id,
        ba.benchmark_daily_return,
        ba.benchmark_cumulative_return,
        ba.benchmark_return_30d as benchmark_return_1m,
        ba.benchmark_return_90d as benchmark_return_3m,
        ba.benchmark_return_1y,
        ba.benchmark_volatility
    from portfolio_returns pr
    left join benchmark_aligned ba
        on pr.portfolio_id = ba.portfolio_id
        and pr.valuation_date = ba.valuation_date
),

-- ISSUE: Excess return calculations
with_excess as (
    select
        *,
        portfolio_daily_return - coalesce(benchmark_daily_return, 0) as daily_excess_return,
        portfolio_cumulative_return - coalesce(benchmark_cumulative_return, 0) as cumulative_excess_return,
        portfolio_return_1m - coalesce(benchmark_return_1m, 0) as excess_return_1m,
        portfolio_return_3m - coalesce(benchmark_return_3m, 0) as excess_return_3m,
        portfolio_return_1y - coalesce(benchmark_return_1y, 0) as excess_return_1y
    from combined
),

-- ISSUE: Rolling tracking error calculation
with_tracking_error as (
    select
        *,
        stddev(daily_excess_return) over (
            partition by portfolio_id
            order by valuation_date
            rows between 251 preceding and current row
        ) * sqrt(252) as tracking_error_1y,
        avg(daily_excess_return) over (
            partition by portfolio_id
            order by valuation_date
            rows between 251 preceding and current row
        ) * 252 as annualized_alpha
    from with_excess
),

-- ISSUE: Information ratio
final as (
    select
        *,
        case
            when tracking_error_1y > 0
            then annualized_alpha / tracking_error_1y
            else null
        end as information_ratio
    from with_tracking_error
)

select * from final
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark"} */;
[0m20:23:50.777398 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 0.747 seconds
[0m20:23:50.781880 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1133d4c10>]}
[0m20:23:50.783040 [info ] [Thread-3  ]: 20 of 32 OK created sql view model DEV_pipeline_c.int_sector_attribution ....... [[32mSUCCESS 1[0m in 0.77s]
[0m20:23:50.783793 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.int_sector_attribution
[0m20:23:50.784336 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.int_fund_rollup
[0m20:23:50.785116 [info ] [Thread-3  ]: 22 of 32 START sql view model DEV_pipeline_c.int_fund_rollup ................... [RUN]
[0m20:23:50.785907 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_sector_attribution, now model.bain_capital_portfolio_analytics.int_fund_rollup)
[0m20:23:50.786391 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.int_fund_rollup
[0m20:23:50.793262 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_fund_rollup"
[0m20:23:50.794125 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.int_fund_rollup
[0m20:23:50.798158 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_fund_rollup"
[0m20:23:50.801284 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_fund_rollup"
[0m20:23:50.801766 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.int_fund_rollup: create or replace   view DBT_DEMO.DEV_pipeline_c.int_fund_rollup
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: int_fund_rollup
-- Description: Roll up portfolio metrics to fund level
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Recursive-like hierarchy traversal
-- 2. Multiple aggregation levels
-- 3. Heavy joins

with fund_hierarchy as (
    select * from DBT_DEMO.DEV_pipeline_c.stg_fund_hierarchy
),

portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

risk_metrics as (
    select * from DBT_DEMO.DEV_pipeline_c.int_risk_metrics
),

-- Get portfolio to fund mapping
portfolio_fund_map as (
    select
        p.portfolio_id,
        p.portfolio_name,
        p.fund_id,
        fh.entity_name as fund_name,
        fh.parent_entity_id,
        fh.hierarchy_level
    from portfolios p
    left join fund_hierarchy fh
        on p.fund_id = fh.entity_id
),

-- Get latest risk metrics per portfolio
latest_metrics as (
    select *
    from (
        select
            *,
            row_number() over (partition by portfolio_id order by valuation_date desc) as rn
        from risk_metrics
    )
    where rn = 1  -- ISSUE: Should use QUALIFY
),

-- ISSUE: Join and aggregate
fund_aggregated as (
    select
        pfm.fund_id,
        pfm.fund_name,
        pfm.parent_entity_id,
        pfm.hierarchy_level,
        lm.valuation_date,
        count(distinct pfm.portfolio_id) as portfolio_count,
        sum(lm.nav_usd) as total_nav_usd,
        -- Weighted average metrics
        sum(lm.nav_usd * lm.annualized_return_1y) / nullif(sum(lm.nav_usd), 0) as weighted_return_1y,
        sum(lm.nav_usd * lm.volatility_1y) / nullif(sum(lm.nav_usd), 0) as weighted_volatility_1y,
        sum(lm.nav_usd * lm.sharpe_ratio) / nullif(sum(lm.nav_usd), 0) as weighted_sharpe_ratio,
        min(lm.max_drawdown) as worst_drawdown,
        sum(lm.var_95_1d) as total_var_95
    from portfolio_fund_map pfm
    inner join latest_metrics lm
        on pfm.portfolio_id = lm.portfolio_id
    group by 1, 2, 3, 4, 5
)

select * from fund_aggregated
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_fund_rollup"} */;
[0m20:23:50.820625 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.618 seconds
[0m20:23:50.823027 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113676f40>]}
[0m20:23:50.823704 [info ] [Thread-2  ]: 21 of 32 OK created sql view model DEV_pipeline_c.int_portfolio_vs_benchmark ... [[32mSUCCESS 1[0m in 0.63s]
[0m20:23:50.824231 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark
[0m20:23:50.824576 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.int_trade_pnl
[0m20:23:50.825039 [info ] [Thread-2  ]: 23 of 32 START sql view model DEV_pipeline_b.int_trade_pnl ..................... [RUN]
[0m20:23:50.825435 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark, now model.bain_capital_portfolio_analytics.int_trade_pnl)
[0m20:23:50.825773 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.int_trade_pnl
[0m20:23:50.829575 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_trade_pnl"
[0m20:23:50.830230 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.int_trade_pnl
[0m20:23:50.833321 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_trade_pnl"
[0m20:23:50.836242 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_trade_pnl"
[0m20:23:50.836666 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.int_trade_pnl: create or replace   view DBT_DEMO.DEV_pipeline_b.int_trade_pnl
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: int_trade_pnl
-- Description: Calculate P&L for each trade
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Complex position tracking logic that could be simplified
-- 2. Multiple self-joins for cost basis calculation
-- 3. Window functions recalculated multiple times

with trades as (
    select * from DBT_DEMO.DEV_pipeline_b.int_trades_enriched
),

-- ISSUE: Running position calculation done inefficiently
positions as (
    select
        trade_id,
        portfolio_id,
        security_id,
        ticker,
        security_name,
        security_type,
        asset_class,
        sector,
        industry,
        trade_date,
        trade_type,
        trade_category,
        quantity,
        execution_price,
        net_amount,
        commission,
        -- ISSUE: Multiple window functions with same partition
        sum(case
            when trade_category = 'PURCHASE' then quantity
            when trade_category = 'SALE' then -quantity
            else 0
        end) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as running_position,
        sum(case
            when trade_category = 'PURCHASE' then net_amount
            when trade_category = 'SALE' then -net_amount
            else 0
        end) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as cumulative_cost,
        -- ISSUE: Another separate window for purchase-only
        sum(case when trade_category = 'PURCHASE' then quantity else 0 end) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as cumulative_purchased_qty,
        sum(case when trade_category = 'PURCHASE' then net_amount else 0 end) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as cumulative_purchase_cost
    from trades
),

-- ISSUE: Separate CTE for cost basis
with_cost_basis as (
    select
        *,
        case
            when cumulative_purchased_qty > 0
            then cumulative_purchase_cost / cumulative_purchased_qty
            else null
        end as avg_cost_basis
    from positions
),

-- ISSUE: Another pass for realized P&L
with_pnl as (
    select
        *,
        case
            when trade_category = 'SALE' and avg_cost_basis is not null
            then (execution_price - avg_cost_basis) * quantity
            else null
        end as realized_pnl,
        case
            when trade_category = 'SALE' and avg_cost_basis is not null and avg_cost_basis > 0
            then (execution_price - avg_cost_basis) / avg_cost_basis * 100
            else null
        end as realized_pnl_pct
    from with_cost_basis
)

select * from with_pnl
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_trade_pnl"} */;
[0m20:23:51.334011 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.497 seconds
[0m20:23:51.338532 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113837b50>]}
[0m20:23:51.339707 [info ] [Thread-2  ]: 23 of 32 OK created sql view model DEV_pipeline_b.int_trade_pnl ................ [[32mSUCCESS 1[0m in 0.51s]
[0m20:23:51.340354 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.int_trade_pnl
[0m20:23:51.340834 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.fact_sector_performance
[0m20:23:51.341545 [info ] [Thread-2  ]: 24 of 32 START sql table model DEV_pipeline_c.fact_sector_performance .......... [RUN]
[0m20:23:51.342296 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_trade_pnl, now model.bain_capital_portfolio_analytics.fact_sector_performance)
[0m20:23:51.342747 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_sector_performance
[0m20:23:51.351451 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_sector_performance"
[0m20:23:51.352723 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.fact_sector_performance
[0m20:23:51.356479 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_sector_performance"
[0m20:23:51.358401 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_sector_performance"
[0m20:23:51.358780 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.fact_sector_performance: create or replace transient table DBT_DEMO.DEV_pipeline_c.fact_sector_performance
    
    
    
    as (-- Pipeline C: Complex Portfolio Analytics
-- Model: fact_sector_performance
-- Description: Sector-level performance aggregation
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Re-aggregates data from upstream
-- 2. Complex window functions

with sector_attribution as (
    select * from DBT_DEMO.DEV_pipeline_c.int_sector_attribution
),

portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

-- ISSUE: Another portfolio join
with_portfolio_info as (
    select
        sa.*,
        p.portfolio_name,
        p.portfolio_type,
        p.fund_id
    from sector_attribution sa
    inner join portfolios p
        on sa.portfolio_id = p.portfolio_id
),

-- ISSUE: More window functions for sector ranking
with_rankings as (
    select
        *,
        rank() over (
            partition by portfolio_id, position_date
            order by sector_weight desc
        ) as sector_weight_rank,
        rank() over (
            partition by portfolio_id, position_date
            order by sector_contribution desc
        ) as sector_contribution_rank
    from with_portfolio_info
),

final as (
    select
        md5(cast(coalesce(cast(portfolio_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(sector as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(position_date as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as sector_performance_key,
        *,
        case
            when sector_weight_rank <= 3 then 'TOP_3'
            when sector_weight_rank <= 5 then 'TOP_5'
            else 'OTHER'
        end as sector_weight_tier
    from with_rankings
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_sector_performance"} */;
[0m20:23:51.642567 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 0.840 seconds
[0m20:23:51.648619 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1133d4c10>]}
[0m20:23:52.087280 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 2.410 seconds
[0m20:23:52.091453 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1133fd940>]}
[0m20:23:52.116643 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 2.995 seconds
[0m20:23:52.119767 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d1cc7f0>]}
[0m20:23:52.737931 [debug] [Thread-3  ]: An error was encountered while trying to send an event
[0m20:23:52.740185 [info ] [Thread-4  ]: 19 of 32 OK created sql table model DEV_pipeline_c.fact_position_snapshot ...... [[32mSUCCESS 1[0m in 2.43s]
[0m20:23:52.741268 [info ] [Thread-1  ]: 13 of 32 OK created sql table model DEV_pipeline_a.fact_cashflow_summary ....... [[32mSUCCESS 1[0m in 3.06s]
[0m20:23:52.742191 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.fact_position_snapshot
[0m20:23:52.744412 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m20:23:52.743938 [info ] [Thread-3  ]: 22 of 32 OK created sql view model DEV_pipeline_c.int_fund_rollup .............. [[32mSUCCESS 1[0m in 0.86s]
[0m20:23:52.745010 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.fact_trade_summary
[0m20:23:52.745726 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.int_fund_rollup
[0m20:23:52.746282 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.report_monthly_cashflows
[0m20:23:52.746979 [info ] [Thread-4  ]: 25 of 32 START sql table model DEV_pipeline_b.fact_trade_summary ............... [RUN]
[0m20:23:52.747802 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.fact_portfolio_positions
[0m20:23:52.748680 [info ] [Thread-1  ]: 26 of 32 START sql table model DEV_pipeline_a.report_monthly_cashflows ......... [RUN]
[0m20:23:52.749531 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.fact_position_snapshot, now model.bain_capital_portfolio_analytics.fact_trade_summary)
[0m20:23:52.750193 [info ] [Thread-3  ]: 27 of 32 START sql table model DEV_pipeline_b.fact_portfolio_positions ......... [RUN]
[0m20:23:52.750663 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.fact_cashflow_summary, now model.bain_capital_portfolio_analytics.report_monthly_cashflows)
[0m20:23:52.751056 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_trade_summary
[0m20:23:52.752032 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_fund_rollup, now model.bain_capital_portfolio_analytics.fact_portfolio_positions)
[0m20:23:52.752448 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.report_monthly_cashflows
[0m20:23:52.759006 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_trade_summary"
[0m20:23:52.759405 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_portfolio_positions
[0m20:23:52.762674 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.report_monthly_cashflows"
[0m20:23:52.768021 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_portfolio_positions"
[0m20:23:52.768911 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.report_monthly_cashflows
[0m20:23:52.772939 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.report_monthly_cashflows"
[0m20:23:52.773289 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.fact_trade_summary
[0m20:23:52.773620 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.fact_portfolio_positions
[0m20:23:52.776403 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_trade_summary"
[0m20:23:52.779249 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_portfolio_positions"
[0m20:23:52.782282 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.report_monthly_cashflows"
[0m20:23:52.782735 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.report_monthly_cashflows: create or replace transient table DBT_DEMO.DEV_pipeline_a.report_monthly_cashflows
    
    
    
    as (-- Pipeline A: Simple Cashflow Pipeline
-- Model: report_monthly_cashflows
-- Description: LP reporting view for monthly cashflow analysis
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Re-aggregates data that's already in fact table
-- 2. Repeated window functions
-- 3. Suboptimal pivot pattern

with fact_data as (
    select * from DBT_DEMO.DEV_pipeline_a.fact_cashflow_summary
),

-- ISSUE: Re-aggregating already aggregated data
monthly_totals as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        cashflow_month,
        year_num,
        month_num,
        sum(case when cashflow_type = 'CONTRIBUTION' then total_amount else 0 end) as contributions,
        sum(case when cashflow_type = 'DISTRIBUTION' then total_amount else 0 end) as distributions,
        sum(case when cashflow_type = 'DIVIDEND' then total_amount else 0 end) as dividends,
        sum(case when cashflow_type = 'FEE' then total_amount else 0 end) as fees,
        sum(total_amount) as total_cashflow,
        sum(transaction_count) as total_transactions
    from fact_data
    group by 1,2,3,4,5,6,7
),

-- ISSUE: Window functions recalculated multiple times
with_running_totals as (
    select
        *,
        -- Running totals (repeated pattern)
        sum(contributions) over (
            partition by portfolio_id
            order by cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_contributions,
        sum(distributions) over (
            partition by portfolio_id
            order by cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_distributions,
        sum(total_cashflow) over (
            partition by portfolio_id
            order by cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_net_cashflow,
        -- Prior period comparisons (another repeated pattern)
        lag(contributions, 1) over (partition by portfolio_id order by cashflow_month) as prior_month_contributions,
        lag(distributions, 1) over (partition by portfolio_id order by cashflow_month) as prior_month_distributions,
        lag(total_cashflow, 1) over (partition by portfolio_id order by cashflow_month) as prior_month_total,
        -- YoY comparison
        lag(contributions, 12) over (partition by portfolio_id order by cashflow_month) as prior_year_contributions,
        lag(distributions, 12) over (partition by portfolio_id order by cashflow_month) as prior_year_distributions
    from monthly_totals
),

-- ISSUE: Calculated columns that could be simplified
final as (
    select
        *,
        contributions - coalesce(prior_month_contributions, 0) as mom_contribution_change,
        distributions - coalesce(prior_month_distributions, 0) as mom_distribution_change,
        case
            when prior_year_contributions > 0
            then (contributions - prior_year_contributions) / prior_year_contributions * 100
            else null
        end as yoy_contribution_pct_change,
        contributions - distributions as net_inflow
    from with_running_totals
)

select * from final
order by portfolio_id, cashflow_month
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.report_monthly_cashflows"} */;
[0m20:23:52.797203 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_portfolio_positions"
[0m20:23:52.798614 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_trade_summary"
[0m20:23:52.799371 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.fact_portfolio_positions: create or replace transient table DBT_DEMO.DEV_pipeline_b.fact_portfolio_positions
    
    
    
    as (-- Pipeline B: Trade Analytics Pipeline
-- Model: fact_portfolio_positions
-- Description: Current position snapshot by portfolio and security
-- DEPENDENCY: Uses fact_cashflow_summary from Pipeline A for portfolio cash context
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Gets latest position via subquery (should use QUALIFY)
-- 2. Self-joins for historical position lookups
-- 3. Correlated subqueries for portfolio-level aggregations
-- 4. Repeated window functions

with trade_pnl as (
    select * from DBT_DEMO.DEV_pipeline_b.int_trade_pnl
),

-- DEPENDENCY ON PIPELINE A: Get cashflow context for each portfolio
cashflow_summary as (
    select * from DBT_DEMO.DEV_pipeline_a.fact_cashflow_summary
),

-- Aggregate cashflows by portfolio to get total contributions/distributions
portfolio_cashflows as (
    select
        portfolio_id,
        sum(case when cashflow_type = 'CONTRIBUTION' then cumulative_total else 0 end) as total_contributions,
        sum(case when cashflow_type = 'DISTRIBUTION' then abs(cumulative_total) else 0 end) as total_distributions,
        max(cashflow_month) as last_cashflow_date
    from cashflow_summary
    group by portfolio_id
),

latest_positions as (
    select *
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id, security_id
                order by trade_date desc, trade_id desc
            ) as rn
        from trade_pnl
    )
    where rn = 1
),

-- ISSUE: Self-join to get position 30 days ago
positions_30d_ago as (
    select
        portfolio_id,
        security_id,
        running_position as position_30d_ago,
        avg_cost_basis as cost_basis_30d_ago
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id, security_id
                order by trade_date desc, trade_id desc
            ) as rn
        from trade_pnl
        where trade_date <= dateadd(day, -30, current_date())
    )
    where rn = 1
),

-- ISSUE: Self-join to get position 90 days ago
positions_90d_ago as (
    select
        portfolio_id,
        security_id,
        running_position as position_90d_ago,
        avg_cost_basis as cost_basis_90d_ago
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id, security_id
                order by trade_date desc, trade_id desc
            ) as rn
        from trade_pnl
        where trade_date <= dateadd(day, -90, current_date())
    )
    where rn = 1
),

market_prices as (
    select
        security_id,
        close_price as current_price,
        price_date
    from (
        select
            security_id,
            close_price,
            price_date,
            row_number() over (partition by security_id order by price_date desc) as rn
        from DBT_DEMO.DEV_pipeline_b.stg_market_prices
    )
    where rn = 1  -- ISSUE: Again, should use QUALIFY
),

-- ISSUE: Get historical prices for comparison
market_prices_30d_ago as (
    select
        security_id,
        close_price as price_30d_ago
    from (
        select
            security_id,
            close_price,
            row_number() over (partition by security_id order by price_date desc) as rn
        from DBT_DEMO.DEV_pipeline_b.stg_market_prices
        where price_date <= dateadd(day, -30, current_date())
    )
    where rn = 1
),

-- ISSUE: Join all the position snapshots together
enriched_positions as (
    select
        lp.*,
        mp.current_price,
        mp.price_date as price_as_of_date,
        p30.position_30d_ago,
        p30.cost_basis_30d_ago,
        p90.position_90d_ago,
        p90.cost_basis_90d_ago,
        mp30.price_30d_ago,
        pcf.total_contributions,
        pcf.total_distributions,
        pcf.last_cashflow_date
    from latest_positions lp
    left join market_prices mp
        on lp.security_id = mp.security_id
    left join positions_30d_ago p30
        on lp.portfolio_id = p30.portfolio_id
        and lp.security_id = p30.security_id
    left join positions_90d_ago p90
        on lp.portfolio_id = p90.portfolio_id
        and lp.security_id = p90.security_id
    left join market_prices_30d_ago mp30
        on lp.security_id = mp30.security_id
    left join portfolio_cashflows pcf
        on lp.portfolio_id = pcf.portfolio_id
    where lp.running_position != 0
),

-- ISSUE: Window functions for portfolio-level context
with_portfolio_context as (
    select
        ep.*,
        -- ISSUE: Repeated partition by portfolio_id
        sum(ep.running_position * ep.current_price) over (
            partition by ep.portfolio_id
        ) as portfolio_total_market_value,
        sum(ep.running_position * ep.avg_cost_basis) over (
            partition by ep.portfolio_id
        ) as portfolio_total_cost_basis,
        count(*) over (
            partition by ep.portfolio_id
        ) as portfolio_position_count,
        -- ISSUE: Rankings
        row_number() over (
            partition by ep.portfolio_id
            order by (ep.running_position * ep.current_price) desc
        ) as position_size_rank,
        row_number() over (
            partition by ep.portfolio_id
            order by ((ep.current_price - ep.avg_cost_basis) / nullif(ep.avg_cost_basis, 0)) desc
        ) as position_return_rank
    from enriched_positions ep
),

-- ISSUE: Separate aggregation for sector context (should use window functions)
sector_aggs as (
    select
        portfolio_id,
        sector,
        sum(running_position * current_price) as sector_market_value,
        count(*) as sector_position_count
    from enriched_positions
    group by 1, 2
),

with_sector_context as (
    select
        wpc.*,
        sa.sector_market_value,
        sa.sector_position_count
    from with_portfolio_context wpc
    left join sector_aggs sa
        on wpc.portfolio_id = sa.portfolio_id
        and wpc.sector = sa.sector
),

-- ISSUE: Complex calculations in final select
final as (
    select
        md5(cast(coalesce(cast(wsc.portfolio_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wsc.security_id as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as position_key,
        wsc.portfolio_id,
        wsc.security_id,
        wsc.ticker,
        wsc.security_name,
        wsc.sector,
        wsc.asset_class,
        wsc.running_position as current_quantity,
        wsc.avg_cost_basis,
        wsc.current_price,
        wsc.price_as_of_date,
        -- Core calculations
        wsc.running_position * wsc.avg_cost_basis as cost_basis_value,
        wsc.running_position * wsc.current_price as market_value,
        (wsc.running_position * wsc.current_price) - (wsc.running_position * wsc.avg_cost_basis) as unrealized_pnl,
        -- ISSUE: Repeated division logic
        case
            when wsc.avg_cost_basis > 0
            then ((wsc.current_price - wsc.avg_cost_basis) / wsc.avg_cost_basis) * 100
            else null
        end as unrealized_pnl_pct,
        -- Portfolio context
        wsc.portfolio_total_market_value,
        wsc.portfolio_total_cost_basis,
        wsc.portfolio_position_count,
        -- ISSUE: Weight calculation (repeated division)
        case
            when wsc.portfolio_total_market_value > 0
            then ((wsc.running_position * wsc.current_price) / wsc.portfolio_total_market_value) * 100
            else null
        end as portfolio_weight_pct,
        -- Sector context
        wsc.sector_market_value,
        wsc.sector_position_count,
        case
            when wsc.sector_market_value > 0
            then ((wsc.running_position * wsc.current_price) / wsc.sector_market_value) * 100
            else null
        end as sector_weight_pct,
        -- Historical comparison
        wsc.position_30d_ago,
        wsc.position_90d_ago,
        wsc.position_30d_ago - wsc.running_position as position_change_30d,
        wsc.position_90d_ago - wsc.running_position as position_change_90d,
        -- Price momentum
        wsc.price_30d_ago,
        case
            when wsc.price_30d_ago > 0
            then ((wsc.current_price - wsc.price_30d_ago) / wsc.price_30d_ago) * 100
            else null
        end as price_change_30d_pct,
        -- Cashflow context from Pipeline A
        wsc.total_contributions,
        wsc.total_distributions,
        wsc.last_cashflow_date,
        -- Rankings
        wsc.position_size_rank,
        wsc.position_return_rank,
        -- ISSUE: Complex classification
        case
            when ((wsc.running_position * wsc.current_price) / nullif(wsc.portfolio_total_market_value, 0)) > 0.10 then 'CONCENTRATED'
            when ((wsc.running_position * wsc.current_price) / nullif(wsc.portfolio_total_market_value, 0)) > 0.05 then 'SIGNIFICANT'
            when ((wsc.running_position * wsc.current_price) / nullif(wsc.portfolio_total_market_value, 0)) > 0.02 then 'MODERATE'
            else 'SMALL'
        end as position_size_category,
        wsc.cumulative_purchase_cost as total_invested,
        current_timestamp() as snapshot_timestamp
    from with_sector_context wsc
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_portfolio_positions"} */;
[0m20:23:52.800447 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.fact_trade_summary: create or replace transient table DBT_DEMO.DEV_pipeline_b.fact_trade_summary
    
    
    
    as (-- Pipeline B: Trade Analytics Pipeline
-- Model: fact_trade_summary
-- Description: Fact table for trade-level analysis
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-joins for prior trade lookups (should use LAG)
-- 2. Repeated window functions with same partitions
-- 3. Correlated subqueries for security-level aggregations
-- 4. Complex CASE statements repeated multiple times

with trade_pnl as (
    select * from DBT_DEMO.DEV_pipeline_b.int_trade_pnl
),

-- ISSUE: Getting portfolio data again (already available through joins upstream)
portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

-- ISSUE: Join that adds overhead
enriched as (
    select
        t.trade_id,
        t.portfolio_id,
        p.portfolio_name,
        p.portfolio_type,
        p.fund_id,
        t.security_id,
        t.ticker,
        t.security_name,
        t.security_type,
        t.asset_class,
        t.sector,
        t.trade_date,
        t.trade_type,
        t.trade_category,
        t.quantity,
        t.execution_price,
        t.net_amount,
        t.commission,
        t.running_position,
        t.avg_cost_basis,
        t.realized_pnl,
        t.realized_pnl_pct,
        -- ISSUE: Redundant date extractions (already done upstream)
        extract(year from t.trade_date) as trade_year,
        extract(month from t.trade_date) as trade_month,
        extract(quarter from t.trade_date) as trade_quarter,
        extract(dayofweek from t.trade_date) as trade_day_of_week,
        date_trunc('week', t.trade_date) as trade_week,
        date_trunc('month', t.trade_date) as trade_month_start
    from trade_pnl t
    left join portfolios p
        on t.portfolio_id = p.portfolio_id
),

-- ISSUE: Self-joins for prior trade comparisons (should use LAG)
-- Pre-compute trade sequence for self-join lookups
trade_sequences as (
    select
        *,
        row_number() over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
        ) as trade_seq
    from enriched
),

with_prior_trades as (
    select
        ts.*,
        -- ISSUE: Self-join for prior trade same security (should use LAG)
        ts_prior.execution_price as prior_trade_price,
        ts_prior.trade_date as prior_trade_date,
        ts_prior.quantity as prior_trade_quantity,
        -- ISSUE: Self-join for 5 trades ago (should use LAG offset)
        ts_5.execution_price as price_5_trades_ago,
        -- ISSUE: Self-join for 10 trades ago (should use LAG offset)
        ts_10.execution_price as price_10_trades_ago
    from trade_sequences ts
    left join trade_sequences ts_prior
        on ts.portfolio_id = ts_prior.portfolio_id
        and ts.security_id = ts_prior.security_id
        and ts_prior.trade_seq = ts.trade_seq - 1
    left join trade_sequences ts_5
        on ts.portfolio_id = ts_5.portfolio_id
        and ts.security_id = ts_5.security_id
        and ts_5.trade_seq = ts.trade_seq - 5
    left join trade_sequences ts_10
        on ts.portfolio_id = ts_10.portfolio_id
        and ts.security_id = ts_10.security_id
        and ts_10.trade_seq = ts.trade_seq - 10
),

-- ISSUE: Multiple window functions with repeated partitions
with_window_calcs as (
    select
        wpt.*,
        -- ISSUE: Running aggregations (repeated partition by portfolio_id, security_id)
        sum(quantity) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between unbounded preceding and current row
        ) as cumulative_quantity,
        sum(abs(net_amount)) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between unbounded preceding and current row
        ) as cumulative_trade_value,
        sum(coalesce(realized_pnl, 0)) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between unbounded preceding and current row
        ) as cumulative_realized_pnl,
        -- ISSUE: Moving averages (same partition repeated)
        avg(execution_price) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between 4 preceding and current row
        ) as rolling_5_trade_avg_price,
        avg(execution_price) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between 9 preceding and current row
        ) as rolling_10_trade_avg_price,
        avg(execution_price) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between 19 preceding and current row
        ) as rolling_20_trade_avg_price,
        -- ISSUE: More window calculations
        stddev(execution_price) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between 9 preceding and current row
        ) as rolling_10_trade_price_stddev,
        count(*) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between unbounded preceding and current row
        ) as trade_sequence_number,
        -- ISSUE: Rankings (same partition again)
        row_number() over (
            partition by wpt.portfolio_id, wpt.security_id, wpt.trade_category
            order by abs(wpt.net_amount) desc
        ) as size_rank_within_category
    from with_prior_trades wpt
),

-- ISSUE: Separate CTE for running trade stats (should be combined with window calcs above)
security_trade_aggs as (
    select
        portfolio_id,
        security_id,
        trade_date,
        trade_id,
        -- ISSUE: These window functions duplicate the partition from with_window_calcs
        count(*) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as total_portfolio_trades_this_security,
        avg(execution_price) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as avg_portfolio_price_this_security
    from enriched
),

-- ISSUE: Separate aggregation for fund-level volume (should be combined upstream)
fund_daily_volume as (
    select
        fund_id,
        security_id,
        trade_date,
        sum(abs(net_amount)) as fund_total_volume_same_security_same_day
    from enriched
    group by 1, 2, 3
),

with_security_context as (
    select
        wwc.*,
        sta.total_portfolio_trades_this_security,
        sta.avg_portfolio_price_this_security,
        fdv.fund_total_volume_same_security_same_day
    from with_window_calcs wwc
    left join security_trade_aggs sta
        on wwc.portfolio_id = sta.portfolio_id
        and wwc.security_id = sta.security_id
        and wwc.trade_id = sta.trade_id
    left join fund_daily_volume fdv
        on wwc.fund_id = fdv.fund_id
        and wwc.security_id = fdv.security_id
        and wwc.trade_date = fdv.trade_date
),

-- ISSUE: Complex derived metrics with repeated CASE statements
final as (
    select
        md5(cast(coalesce(cast(wsc.trade_id as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as trade_key,
        wsc.*,
        -- ISSUE: Price change calculations (repeated division logic)
        case
            when wsc.prior_trade_price is not null and wsc.prior_trade_price > 0
            then ((wsc.execution_price - wsc.prior_trade_price) / wsc.prior_trade_price) * 100
            else null
        end as price_change_from_prior_pct,
        case
            when wsc.price_5_trades_ago is not null and wsc.price_5_trades_ago > 0
            then ((wsc.execution_price - wsc.price_5_trades_ago) / wsc.price_5_trades_ago) * 100
            else null
        end as price_change_from_5_trades_ago_pct,
        case
            when wsc.rolling_20_trade_avg_price is not null and wsc.rolling_20_trade_avg_price > 0
            then ((wsc.execution_price - wsc.rolling_20_trade_avg_price) / wsc.rolling_20_trade_avg_price) * 100
            else null
        end as deviation_from_20_trade_avg_pct,
        -- ISSUE: Trade size classification (repeated CASE)
        case
            when abs(wsc.net_amount) >= 10000000 then 'BLOCK_TRADE'
            when abs(wsc.net_amount) >= 1000000 then 'LARGE'
            when abs(wsc.net_amount) >= 100000 then 'MEDIUM'
            when abs(wsc.net_amount) >= 10000 then 'SMALL'
            else 'MICRO'
        end as trade_size_category,
        -- ISSUE: Trade timing classification (complex nested CASE)
        case
            when wsc.execution_price > wsc.rolling_10_trade_avg_price * 1.1 then 'BOUGHT_HIGH'
            when wsc.execution_price > wsc.rolling_10_trade_avg_price * 1.03 then 'ABOVE_AVERAGE'
            when wsc.execution_price < wsc.rolling_10_trade_avg_price * 0.9 then 'BOUGHT_LOW'
            when wsc.execution_price < wsc.rolling_10_trade_avg_price * 0.97 then 'BELOW_AVERAGE'
            else 'AVERAGE'
        end as execution_quality,
        -- ISSUE: Momentum signal (repeated logic)
        case
            when wsc.rolling_5_trade_avg_price > wsc.rolling_20_trade_avg_price then 'UPTREND'
            when wsc.rolling_5_trade_avg_price < wsc.rolling_20_trade_avg_price then 'DOWNTREND'
            else 'NEUTRAL'
        end as price_momentum,
        -- ISSUE: Volatility classification
        case
            when wsc.rolling_10_trade_price_stddev < wsc.rolling_10_trade_avg_price * 0.02 then 'LOW_VOLATILITY'
            when wsc.rolling_10_trade_price_stddev < wsc.rolling_10_trade_avg_price * 0.05 then 'MODERATE_VOLATILITY'
            when wsc.rolling_10_trade_price_stddev < wsc.rolling_10_trade_avg_price * 0.10 then 'HIGH_VOLATILITY'
            else 'VERY_HIGH_VOLATILITY'
        end as price_volatility_regime,
        -- ISSUE: Trade frequency indicator
        case
            when wsc.trade_sequence_number >= 100 then 'VERY_ACTIVE'
            when wsc.trade_sequence_number >= 50 then 'ACTIVE'
            when wsc.trade_sequence_number >= 20 then 'MODERATE'
            when wsc.trade_sequence_number >= 5 then 'LIGHT'
            else 'FIRST_FEW_TRADES'
        end as trading_activity_level
    from with_security_context wsc
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_trade_summary"} */;
[0m20:23:53.856212 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 2.497 seconds
[0m20:23:53.861701 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113837b50>]}
[0m20:23:53.862442 [info ] [Thread-2  ]: 24 of 32 OK created sql table model DEV_pipeline_c.fact_sector_performance ..... [[32mSUCCESS 1[0m in 2.52s]
[0m20:23:53.862968 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.fact_sector_performance
[0m20:23:53.863323 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.fact_fund_summary
[0m20:23:53.863826 [info ] [Thread-2  ]: 28 of 32 START sql table model DEV_pipeline_c.fact_fund_summary ................ [RUN]
[0m20:23:53.864299 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.fact_sector_performance, now model.bain_capital_portfolio_analytics.fact_fund_summary)
[0m20:23:53.864643 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_fund_summary
[0m20:23:53.870382 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_fund_summary"
[0m20:23:53.871274 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.fact_fund_summary
[0m20:23:53.874608 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_fund_summary"
[0m20:23:53.876635 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_fund_summary"
[0m20:23:53.877053 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.fact_fund_summary: create or replace transient table DBT_DEMO.DEV_pipeline_c.fact_fund_summary
    
    
    
    as (-- Pipeline C: Complex Portfolio Analytics
-- Model: fact_fund_summary
-- Description: Fund-level summary metrics
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Based on already-aggregated data
-- 2. Could push more logic upstream

with fund_rollup as (
    select * from DBT_DEMO.DEV_pipeline_c.int_fund_rollup
),

fund_hierarchy as (
    select * from DBT_DEMO.DEV_pipeline_c.stg_fund_hierarchy
),

-- Get parent fund info
with_parent as (
    select
        fr.*,
        parent.entity_name as parent_fund_name
    from fund_rollup fr
    left join fund_hierarchy parent
        on fr.parent_entity_id = parent.entity_id
),

final as (
    select
        md5(cast(coalesce(cast(fund_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(valuation_date as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as fund_summary_key,
        fund_id,
        fund_name,
        parent_entity_id as parent_fund_id,
        parent_fund_name,
        hierarchy_level,
        valuation_date,
        portfolio_count,
        total_nav_usd,
        weighted_return_1y,
        weighted_volatility_1y,
        weighted_sharpe_ratio,
        worst_drawdown,
        total_var_95,
        -- ISSUE: Calculated fields
        case
            when weighted_return_1y >= 0.15 then 'HIGH'
            when weighted_return_1y >= 0.08 then 'MEDIUM'
            when weighted_return_1y >= 0 then 'LOW'
            else 'NEGATIVE'
        end as return_tier,
        case
            when weighted_sharpe_ratio >= 1.5 then 'EXCELLENT'
            when weighted_sharpe_ratio >= 1.0 then 'GOOD'
            when weighted_sharpe_ratio >= 0.5 then 'FAIR'
            else 'POOR'
        end as risk_adjusted_tier
    from with_parent
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_fund_summary"} */;
[0m20:23:53.970335 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 1.187 seconds
[0m20:23:53.973684 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11358cfd0>]}
[0m20:23:53.974719 [info ] [Thread-1  ]: 26 of 32 OK created sql table model DEV_pipeline_a.report_monthly_cashflows .... [[32mSUCCESS 1[0m in 1.22s]
[0m20:23:53.975465 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.report_monthly_cashflows
[0m20:23:57.924179 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 4.046 seconds
[0m20:23:57.928125 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1138bde20>]}
[0m20:23:57.929271 [info ] [Thread-2  ]: 28 of 32 OK created sql table model DEV_pipeline_c.fact_fund_summary ........... [[32mSUCCESS 1[0m in 4.06s]
[0m20:23:57.929987 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.fact_fund_summary
[0m20:24:03.466129 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 10.664 seconds
[0m20:24:03.469118 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113835fa0>]}
[0m20:24:03.469893 [info ] [Thread-3  ]: 27 of 32 OK created sql table model DEV_pipeline_b.fact_portfolio_positions .... [[32mSUCCESS 1[0m in 10.72s]
[0m20:24:03.470499 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.fact_portfolio_positions
[0m20:24:24.010004 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 31.207 seconds
[0m20:24:24.013414 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1137fcca0>]}
[0m20:24:24.014882 [info ] [Thread-4  ]: 25 of 32 OK created sql table model DEV_pipeline_b.fact_trade_summary .......... [[32mSUCCESS 1[0m in 31.26s]
[0m20:24:24.016391 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.fact_trade_summary
[0m20:24:24.017587 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.fact_portfolio_performance
[0m20:24:24.018098 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.report_trading_performance
[0m20:24:24.018888 [info ] [Thread-1  ]: 29 of 32 START sql table model DEV_pipeline_c.fact_portfolio_performance ....... [RUN]
[0m20:24:24.019667 [info ] [Thread-2  ]: 30 of 32 START sql table model DEV_pipeline_b.report_trading_performance ....... [RUN]
[0m20:24:24.021051 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.fact_fund_summary, now model.bain_capital_portfolio_analytics.report_trading_performance)
[0m20:24:24.020323 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.report_monthly_cashflows, now model.bain_capital_portfolio_analytics.fact_portfolio_performance)
[0m20:24:24.022373 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_portfolio_performance
[0m20:24:24.021933 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.report_trading_performance
[0m20:24:24.032374 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_portfolio_performance"
[0m20:24:24.038698 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.report_trading_performance"
[0m20:24:24.039904 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.report_trading_performance
[0m20:24:24.040284 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.fact_portfolio_performance
[0m20:24:24.044600 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.report_trading_performance"
[0m20:24:24.048515 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_portfolio_performance"
[0m20:24:24.052068 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.report_trading_performance"
[0m20:24:24.052556 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.report_trading_performance: create or replace transient table DBT_DEMO.DEV_pipeline_b.report_trading_performance
    
    
    
    as (-- Pipeline B: Trade Analytics Pipeline
-- Model: report_trading_performance
-- Description: Trading performance report for IC dashboard
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Re-aggregates fact data that could be pre-computed
-- 2. Complex window functions repeated from other models
-- 3. Multiple CTEs that could be consolidated

with trades as (
    select * from DBT_DEMO.DEV_pipeline_b.fact_trade_summary
),

positions as (
    select * from DBT_DEMO.DEV_pipeline_b.fact_portfolio_positions
),

-- ISSUE: Re-aggregating trade data by portfolio/month
trade_metrics as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        trade_year,
        trade_month,
        count(distinct trade_id) as trade_count,
        count(distinct security_id) as securities_traded,
        sum(case when trade_category = 'PURCHASE' then 1 else 0 end) as buy_count,
        sum(case when trade_category = 'SALE' then 1 else 0 end) as sell_count,
        sum(case when trade_category = 'PURCHASE' then net_amount else 0 end) as total_purchases,
        sum(case when trade_category = 'SALE' then abs(net_amount) else 0 end) as total_sales,
        sum(coalesce(realized_pnl, 0)) as total_realized_pnl,
        avg(case when realized_pnl is not null then realized_pnl_pct else null end) as avg_realized_return_pct
    from trades
    group by 1,2,3,4,5,6
),

-- ISSUE: Aggregating positions separately
position_metrics as (
    select
        portfolio_id,
        count(distinct security_id) as position_count,
        sum(market_value) as total_market_value,
        sum(cost_basis_value) as total_cost_basis,
        sum(unrealized_pnl) as total_unrealized_pnl,
        avg(unrealized_pnl_pct) as avg_unrealized_return_pct
    from positions
    group by 1
),

-- ISSUE: Window functions for running totals (repeated pattern)
with_running_totals as (
    select
        tm.*,
        sum(total_realized_pnl) over (
            partition by tm.portfolio_id
            order by tm.trade_year, tm.trade_month
            rows between unbounded preceding and current row
        ) as cumulative_realized_pnl,
        sum(total_purchases) over (
            partition by tm.portfolio_id
            order by tm.trade_year, tm.trade_month
            rows between unbounded preceding and current row
        ) as cumulative_invested,
        -- ISSUE: Multiple LAG functions
        lag(total_realized_pnl, 1) over (
            partition by tm.portfolio_id
            order by tm.trade_year, tm.trade_month
        ) as prior_month_pnl,
        lag(trade_count, 1) over (
            partition by tm.portfolio_id
            order by tm.trade_year, tm.trade_month
        ) as prior_month_trades
    from trade_metrics tm
),

-- ISSUE: Final join adds more complexity
final as (
    select
        wrt.*,
        pm.position_count,
        pm.total_market_value,
        pm.total_cost_basis,
        pm.total_unrealized_pnl,
        pm.avg_unrealized_return_pct,
        -- Combined metrics
        wrt.total_realized_pnl + coalesce(pm.total_unrealized_pnl, 0) as total_pnl,
        case
            when pm.total_cost_basis > 0
            then ((pm.total_market_value + wrt.cumulative_realized_pnl) - pm.total_cost_basis) / pm.total_cost_basis * 100
            else null
        end as total_return_pct
    from with_running_totals wrt
    left join position_metrics pm
        on wrt.portfolio_id = pm.portfolio_id
)

select * from final
order by portfolio_id, trade_year, trade_month
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.report_trading_performance"} */;
[0m20:24:24.062488 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_portfolio_performance"
[0m20:24:24.063470 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.fact_portfolio_performance: create or replace transient table DBT_DEMO.DEV_pipeline_c.fact_portfolio_performance
    
    
    
    as (-- Pipeline C: Complex Portfolio Analytics
-- Model: fact_portfolio_performance
-- Description: Main performance fact table
-- DEPENDENCIES:
--   - Pipeline A: fact_cashflow_summary (for cashflow context)
--   - Pipeline B: fact_trade_summary, fact_portfolio_positions (for trading/position context)
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Multiple self-joins for period comparisons
-- 2. Repeated window functions with same partitions
-- 3. Complex CASE statements repeated multiple times
-- 4. Correlated subqueries
-- 5. Late filtering and unnecessary full table scans

with portfolio_vs_benchmark as (
    select * from DBT_DEMO.DEV_pipeline_c.int_portfolio_vs_benchmark
),

risk_metrics as (
    select * from DBT_DEMO.DEV_pipeline_c.int_risk_metrics
),

portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

-- DEPENDENCY ON PIPELINE A: Cashflow summary for capital deployment context
cashflow_summary as (
    select * from DBT_DEMO.DEV_pipeline_a.fact_cashflow_summary
),

-- Aggregate cashflows to portfolio level
portfolio_cashflow_totals as (
    select
        portfolio_id,
        sum(case when cashflow_type = 'CONTRIBUTION' then cumulative_total else 0 end) as total_contributions,
        sum(case when cashflow_type = 'DISTRIBUTION' then abs(cumulative_total) else 0 end) as total_distributions,
        sum(cumulative_total) as net_cashflow
    from cashflow_summary
    group by portfolio_id
),

-- DEPENDENCY ON PIPELINE B: Trade summary for trading activity context
trade_summary as (
    select * from DBT_DEMO.DEV_pipeline_b.fact_trade_summary
),

-- Aggregate trades to portfolio/date level
portfolio_trade_activity as (
    select
        portfolio_id,
        trade_month_start as activity_month,
        count(*) as trade_count,
        sum(case when trade_category = 'PURCHASE' then abs(net_amount) else 0 end) as total_purchases,
        sum(case when trade_category = 'SALE' then abs(net_amount) else 0 end) as total_sales,
        sum(coalesce(realized_pnl, 0)) as realized_pnl
    from trade_summary
    group by portfolio_id, trade_month_start
),

-- DEPENDENCY ON PIPELINE B: Position snapshot for current holdings context
portfolio_positions as (
    select * from DBT_DEMO.DEV_pipeline_b.fact_portfolio_positions
),

-- Aggregate positions to portfolio level
portfolio_position_totals as (
    select
        portfolio_id,
        sum(market_value) as total_market_value,
        sum(cost_basis_value) as total_cost_basis,
        sum(unrealized_pnl) as total_unrealized_pnl,
        count(*) as position_count
    from portfolio_positions
    group by portfolio_id
),

-- ISSUE: Another join when data could flow from upstream
combined as (
    select
        pvb.portfolio_id,
        pvb.valuation_date,
        pvb.nav,
        pvb.nav_usd,
        -- Portfolio returns
        pvb.portfolio_daily_return,
        pvb.portfolio_cumulative_return,
        pvb.portfolio_return_1m,
        pvb.portfolio_return_3m,
        pvb.portfolio_return_1y,
        pvb.portfolio_volatility,
        -- Benchmark comparison
        pvb.benchmark_id,
        pvb.benchmark_daily_return,
        pvb.benchmark_cumulative_return,
        pvb.benchmark_return_1m,
        pvb.benchmark_return_3m,
        pvb.benchmark_return_1y,
        pvb.benchmark_volatility,
        -- Relative performance
        pvb.daily_excess_return,
        pvb.cumulative_excess_return,
        pvb.excess_return_1m,
        pvb.excess_return_3m,
        pvb.excess_return_1y,
        pvb.tracking_error_1y,
        pvb.annualized_alpha,
        pvb.information_ratio,
        -- Risk metrics
        rm.drawdown,
        rm.max_drawdown,
        rm.downside_deviation_1y,
        rm.sharpe_ratio,
        rm.sortino_ratio,
        rm.var_95_1d,
        rm.var_99_1d
    from portfolio_vs_benchmark pvb
    inner join risk_metrics rm
        on pvb.portfolio_id = rm.portfolio_id
        and pvb.valuation_date = rm.valuation_date
),

-- ISSUE: Self-join for prior period comparisons (should use LAG)
with_prior_periods as (
    select
        c.*,
        -- ISSUE: Self-join for 1 day ago
        c1d.nav_usd as nav_1d_ago,
        c1d.portfolio_cumulative_return as return_1d_ago,
        -- ISSUE: Self-join for 1 week ago
        c1w.nav_usd as nav_1w_ago,
        c1w.portfolio_cumulative_return as return_1w_ago,
        -- ISSUE: Self-join for 1 month ago
        c1m.nav_usd as nav_1m_ago,
        c1m.portfolio_cumulative_return as return_1m_ago,
        -- ISSUE: Self-join for 3 months ago
        c3m.nav_usd as nav_3m_ago,
        c3m.portfolio_cumulative_return as return_3m_ago,
        -- ISSUE: Self-join for 1 year ago
        c1y.nav_usd as nav_1y_ago,
        c1y.portfolio_cumulative_return as return_1y_ago
    from combined c
    left join combined c1d
        on c.portfolio_id = c1d.portfolio_id
        and c1d.valuation_date = dateadd(day, -1, c.valuation_date)
    left join combined c1w
        on c.portfolio_id = c1w.portfolio_id
        and c1w.valuation_date = dateadd(day, -7, c.valuation_date)
    left join combined c1m
        on c.portfolio_id = c1m.portfolio_id
        and c1m.valuation_date = dateadd(month, -1, c.valuation_date)
    left join combined c3m
        on c.portfolio_id = c3m.portfolio_id
        and c3m.valuation_date = dateadd(month, -3, c.valuation_date)
    left join combined c1y
        on c.portfolio_id = c1y.portfolio_id
        and c1y.valuation_date = dateadd(year, -1, c.valuation_date)
),

-- ISSUE: Multiple window functions with repeated partitions
with_rankings as (
    select
        wpp.*,
        -- ISSUE: Multiple ROW_NUMBER with same partition
        row_number() over (
            partition by wpp.portfolio_id
            order by wpp.portfolio_cumulative_return desc
        ) as best_performance_rank,
        row_number() over (
            partition by wpp.portfolio_id
            order by wpp.portfolio_cumulative_return asc
        ) as worst_performance_rank,
        row_number() over (
            partition by wpp.portfolio_id
            order by wpp.sharpe_ratio desc nulls last
        ) as best_sharpe_rank,
        -- ISSUE: DENSE_RANK with same partition
        dense_rank() over (
            partition by wpp.portfolio_id
            order by wpp.nav_usd desc
        ) as nav_size_rank,
        -- ISSUE: More window calculations
        avg(wpp.portfolio_daily_return) over (
            partition by wpp.portfolio_id
            order by wpp.valuation_date
            rows between 20 preceding and current row
        ) as rolling_20d_avg_return,
        avg(wpp.portfolio_daily_return) over (
            partition by wpp.portfolio_id
            order by wpp.valuation_date
            rows between 60 preceding and current row
        ) as rolling_60d_avg_return,
        stddev(wpp.portfolio_daily_return) over (
            partition by wpp.portfolio_id
            order by wpp.valuation_date
            rows between 20 preceding and current row
        ) as rolling_20d_volatility,
        stddev(wpp.portfolio_daily_return) over (
            partition by wpp.portfolio_id
            order by wpp.valuation_date
            rows between 60 preceding and current row
        ) as rolling_60d_volatility
    from with_prior_periods wpp
),

-- ISSUE: Complex derived metrics with repeated CASE statements
with_derived_metrics as (
    select
        wr.*,
        -- ISSUE: Repeated complex CASE for performance classification
        case
            when wr.portfolio_cumulative_return >= 0.50 then 'EXCEPTIONAL'
            when wr.portfolio_cumulative_return >= 0.30 then 'EXCELLENT'
            when wr.portfolio_cumulative_return >= 0.15 then 'VERY_GOOD'
            when wr.portfolio_cumulative_return >= 0.05 then 'GOOD'
            when wr.portfolio_cumulative_return >= 0.00 then 'NEUTRAL'
            when wr.portfolio_cumulative_return >= -0.05 then 'POOR'
            when wr.portfolio_cumulative_return >= -0.15 then 'VERY_POOR'
            else 'UNACCEPTABLE'
        end as performance_rating,
        -- ISSUE: Same CASE for risk classification
        case
            when wr.sharpe_ratio >= 3.0 then 'EXCEPTIONAL'
            when wr.sharpe_ratio >= 2.0 then 'EXCELLENT'
            when wr.sharpe_ratio >= 1.5 then 'VERY_GOOD'
            when wr.sharpe_ratio >= 1.0 then 'GOOD'
            when wr.sharpe_ratio >= 0.5 then 'NEUTRAL'
            when wr.sharpe_ratio >= 0.0 then 'POOR'
            else 'VERY_POOR'
        end as risk_adjusted_rating,
        -- ISSUE: Complex calculation repeated
        case
            when wr.nav_1m_ago is not null and wr.nav_1m_ago > 0
            then (wr.nav_usd - wr.nav_1m_ago) / wr.nav_1m_ago
            else null
        end as nav_change_1m_pct,
        case
            when wr.nav_3m_ago is not null and wr.nav_3m_ago > 0
            then (wr.nav_usd - wr.nav_3m_ago) / wr.nav_3m_ago
            else null
        end as nav_change_3m_pct,
        case
            when wr.nav_1y_ago is not null and wr.nav_1y_ago > 0
            then (wr.nav_usd - wr.nav_1y_ago) / wr.nav_1y_ago
            else null
        end as nav_change_1y_pct,
        -- ISSUE: Momentum indicators with repeated logic
        case
            when wr.rolling_20d_avg_return > wr.rolling_60d_avg_return then 'POSITIVE_MOMENTUM'
            when wr.rolling_20d_avg_return < wr.rolling_60d_avg_return then 'NEGATIVE_MOMENTUM'
            else 'NEUTRAL_MOMENTUM'
        end as momentum_signal,
        -- ISSUE: Volatility regime classification
        case
            when wr.rolling_20d_volatility > wr.rolling_60d_volatility * 1.5 then 'HIGH_VOLATILITY'
            when wr.rolling_20d_volatility > wr.rolling_60d_volatility * 1.2 then 'ELEVATED_VOLATILITY'
            when wr.rolling_20d_volatility < wr.rolling_60d_volatility * 0.8 then 'LOW_VOLATILITY'
            else 'NORMAL_VOLATILITY'
        end as volatility_regime
    from with_rankings wr
),

-- ISSUE: Separate peer aggregation CTEs (should use window functions with partition by portfolio_type)
peer_return_aggs as (
    select
        p2.portfolio_type,
        pvb2.valuation_date,
        avg(pvb2.portfolio_cumulative_return) as peer_avg_return
    from portfolio_vs_benchmark pvb2
    inner join portfolios p2
        on pvb2.portfolio_id = p2.portfolio_id
    group by 1, 2
),

peer_sharpe_aggs as (
    select
        p2.portfolio_type,
        rm2.valuation_date,
        percentile_cont(0.5) within group (order by rm2.sharpe_ratio) as peer_median_sharpe
    from risk_metrics rm2
    inner join portfolios p2
        on rm2.portfolio_id = p2.portfolio_id
    group by 1, 2
),

with_peer_comparison as (
    select
        wdm.*,
        pra.peer_avg_return,
        psa.peer_median_sharpe
    from with_derived_metrics wdm
    inner join portfolios p_self
        on wdm.portfolio_id = p_self.portfolio_id
    left join peer_return_aggs pra
        on p_self.portfolio_type = pra.portfolio_type
        and wdm.valuation_date = pra.valuation_date
    left join peer_sharpe_aggs psa
        on p_self.portfolio_type = psa.portfolio_type
        and wdm.valuation_date = psa.valuation_date
),

-- ISSUE: Portfolio attributes added last
final as (
    select
        md5(cast(coalesce(cast(wpc.portfolio_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wpc.valuation_date as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as performance_key,
        p.portfolio_name,
        p.portfolio_type,
        p.fund_id,
        wpc.*,
        -- ISSUE: Date dimensions recalculated (should be in dim table)
        extract(year from wpc.valuation_date) as valuation_year,
        extract(month from wpc.valuation_date) as valuation_month,
        extract(quarter from wpc.valuation_date) as valuation_quarter,
        extract(dayofweek from wpc.valuation_date) as valuation_day_of_week,
        extract(dayofyear from wpc.valuation_date) as valuation_day_of_year,
        date_trunc('month', wpc.valuation_date) as valuation_month_start,
        date_trunc('quarter', wpc.valuation_date) as valuation_quarter_start,
        date_trunc('year', wpc.valuation_date) as valuation_year_start,
        -- ISSUE: String concatenations (slow)
        concat(p.portfolio_name, ' - ', wpc.valuation_date::varchar) as display_name,
        concat('Q', extract(quarter from wpc.valuation_date), ' ', extract(year from wpc.valuation_date)) as quarter_label,
        -- ISSUE: Complex derived field
        case
            when wpc.portfolio_cumulative_return > wpc.peer_avg_return then 'OUTPERFORMING'
            when wpc.portfolio_cumulative_return < wpc.peer_avg_return then 'UNDERPERFORMING'
            else 'AT_PEER'
        end as peer_relative_performance
    from with_peer_comparison wpc
    inner join portfolios p
        on wpc.portfolio_id = p.portfolio_id
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_portfolio_performance"} */;
[0m20:24:26.394498 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 2.335 seconds
[0m20:24:26.399003 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1137fcca0>]}
[0m20:24:26.400067 [info ] [Thread-2  ]: 30 of 32 OK created sql table model DEV_pipeline_b.report_trading_performance .. [[32mSUCCESS 1[0m in 2.38s]
[0m20:24:26.400737 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.report_trading_performance
[0m20:24:38.790686 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 14.725 seconds
[0m20:24:38.795573 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11383c1f0>]}
[0m20:24:38.796713 [info ] [Thread-1  ]: 29 of 32 OK created sql table model DEV_pipeline_c.fact_portfolio_performance .. [[32mSUCCESS 1[0m in 14.78s]
[0m20:24:38.797636 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.fact_portfolio_performance
[0m20:24:38.798568 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.report_ic_dashboard
[0m20:24:38.799026 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.report_lp_quarterly
[0m20:24:38.799598 [info ] [Thread-4  ]: 31 of 32 START sql table model DEV_pipeline_c.report_ic_dashboard .............. [RUN]
[0m20:24:38.800163 [info ] [Thread-3  ]: 32 of 32 START sql table model DEV_pipeline_c.report_lp_quarterly .............. [RUN]
[0m20:24:38.800703 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.fact_trade_summary, now model.bain_capital_portfolio_analytics.report_ic_dashboard)
[0m20:24:38.801117 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.fact_portfolio_positions, now model.bain_capital_portfolio_analytics.report_lp_quarterly)
[0m20:24:38.801502 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.report_ic_dashboard
[0m20:24:38.801851 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.report_lp_quarterly
[0m20:24:38.806943 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.report_ic_dashboard"
[0m20:24:38.812316 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.report_lp_quarterly"
[0m20:24:38.813431 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.report_lp_quarterly
[0m20:24:38.813842 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.report_ic_dashboard
[0m20:24:38.817185 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.report_lp_quarterly"
[0m20:24:38.820899 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.report_ic_dashboard"
[0m20:24:38.831581 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.report_lp_quarterly"
[0m20:24:38.835064 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.report_ic_dashboard"
[0m20:24:38.835860 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.report_lp_quarterly: create or replace transient table DBT_DEMO.DEV_pipeline_c.report_lp_quarterly
    
    
    
    as (-- Pipeline C: Complex Portfolio Analytics
-- Model: report_lp_quarterly
-- Description: Quarterly LP reporting with period comparisons
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-joins for multi-period comparisons (should use LAG)
-- 2. Repeated window functions with same partitions
-- 3. Correlated subqueries for fund-level aggregations
-- 4. Complex CASE statements repeated multiple times
-- 5. Late filtering after heavy computation

with portfolio_performance as (
    select * from DBT_DEMO.DEV_pipeline_c.fact_portfolio_performance
),

cashflow_summary as (
    select * from DBT_DEMO.DEV_pipeline_a.fact_cashflow_summary
),

-- ISSUE: Filter to quarter-end dates only (filtering late)
quarter_end_perf as (
    select *
    from portfolio_performance
    where valuation_date = last_day(valuation_date, 'quarter')
),

-- ISSUE: Aggregate cashflows to quarterly
quarterly_cashflows as (
    select
        portfolio_id,
        date_trunc('quarter', cashflow_month) as quarter_start,
        sum(case when cashflow_type = 'CONTRIBUTION' then total_amount else 0 end) as quarterly_contributions,
        sum(case when cashflow_type = 'DISTRIBUTION' then total_amount else 0 end) as quarterly_distributions,
        sum(case when cashflow_type = 'DIVIDEND' then total_amount else 0 end) as quarterly_dividends,
        sum(case when cashflow_type = 'FEE' then abs(total_amount) else 0 end) as quarterly_fees,
        sum(total_amount) as quarterly_net_cashflow,
        count(distinct transaction_count) as total_transactions
    from cashflow_summary
    group by 1, 2
),

-- ISSUE: Join performance and cashflows
combined as (
    select
        qep.portfolio_id,
        qep.portfolio_name,
        qep.portfolio_type,
        qep.fund_id,
        qep.valuation_date as quarter_end,
        qep.valuation_quarter_start as quarter_start,
        qep.valuation_year,
        qep.valuation_quarter,
        qep.nav_usd,
        qep.portfolio_return_3m as quarterly_return,
        qep.portfolio_return_1y,
        qep.portfolio_cumulative_return,
        qep.benchmark_return_3m as benchmark_quarterly_return,
        qep.excess_return_3m as quarterly_excess_return,
        qep.sharpe_ratio,
        qep.sortino_ratio,
        qep.max_drawdown,
        qep.performance_rating,
        qep.risk_adjusted_rating,
        qcf.quarterly_contributions,
        qcf.quarterly_distributions,
        qcf.quarterly_dividends,
        qcf.quarterly_fees,
        qcf.quarterly_net_cashflow,
        qcf.total_transactions
    from quarter_end_perf qep
    left join quarterly_cashflows qcf
        on qep.portfolio_id = qcf.portfolio_id
        and qep.valuation_quarter_start = qcf.quarter_start
),

-- ISSUE: Multiple self-joins for historical comparisons (should use LAG)
with_self_joins as (
    select
        c.*,
        -- ISSUE: Self-join for prior quarter
        c_q1.nav_usd as prior_1q_nav,
        c_q1.quarterly_return as prior_1q_return,
        c_q1.sharpe_ratio as prior_1q_sharpe,
        -- ISSUE: Self-join for 2 quarters ago
        c_q2.nav_usd as prior_2q_nav,
        c_q2.quarterly_return as prior_2q_return,
        -- ISSUE: Self-join for 3 quarters ago
        c_q3.nav_usd as prior_3q_nav,
        c_q3.quarterly_return as prior_3q_return,
        -- ISSUE: Self-join for 4 quarters ago (1 year)
        c_q4.nav_usd as prior_4q_nav,
        c_q4.quarterly_return as prior_4q_return,
        c_q4.sharpe_ratio as prior_4q_sharpe,
        -- ISSUE: Self-join for 8 quarters ago (2 years)
        c_q8.nav_usd as prior_8q_nav,
        c_q8.quarterly_return as prior_8q_return
    from combined c
    left join combined c_q1
        on c.portfolio_id = c_q1.portfolio_id
        and c_q1.quarter_end = dateadd(quarter, -1, c.quarter_end)
    left join combined c_q2
        on c.portfolio_id = c_q2.portfolio_id
        and c_q2.quarter_end = dateadd(quarter, -2, c.quarter_end)
    left join combined c_q3
        on c.portfolio_id = c_q3.portfolio_id
        and c_q3.quarter_end = dateadd(quarter, -3, c.quarter_end)
    left join combined c_q4
        on c.portfolio_id = c_q4.portfolio_id
        and c_q4.quarter_end = dateadd(quarter, -4, c.quarter_end)
    left join combined c_q8
        on c.portfolio_id = c_q8.portfolio_id
        and c_q8.quarter_end = dateadd(quarter, -8, c.quarter_end)
),

-- ISSUE: Multiple window functions with repeated partitions
with_window_calcs as (
    select
        wsj.*,
        -- ISSUE: Running totals (repeated partition)
        sum(quarterly_contributions) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between unbounded preceding and current row
        ) as cumulative_contributions,
        sum(quarterly_distributions) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between unbounded preceding and current row
        ) as cumulative_distributions,
        sum(quarterly_dividends) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between unbounded preceding and current row
        ) as cumulative_dividends,
        sum(quarterly_fees) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between unbounded preceding and current row
        ) as cumulative_fees,
        -- ISSUE: Moving averages (same partition repeated)
        avg(quarterly_return) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between 3 preceding and current row
        ) as rolling_4q_avg_return,
        avg(quarterly_return) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between 7 preceding and current row
        ) as rolling_8q_avg_return,
        -- ISSUE: More window calculations
        stddev(quarterly_return) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between 3 preceding and current row
        ) as rolling_4q_volatility,
        min(quarterly_return) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between 3 preceding and current row
        ) as rolling_4q_min_return,
        max(quarterly_return) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between 3 preceding and current row
        ) as rolling_4q_max_return,
        -- ISSUE: Ranking (same partition again)
        row_number() over (
            partition by wsj.portfolio_id
            order by wsj.quarterly_return desc
        ) as best_quarter_rank,
        row_number() over (
            partition by wsj.portfolio_id
            order by wsj.quarterly_return asc
        ) as worst_quarter_rank
    from with_self_joins wsj
),

-- ISSUE: Separate fund-level aggregation (should use window functions)
fund_quarterly_aggs as (
    select
        fund_id,
        valuation_date as quarter_end,
        sum(nav_usd) as fund_total_nav,
        avg(portfolio_return_3m) as fund_avg_quarterly_return,
        count(distinct portfolio_id) as fund_portfolio_count
    from quarter_end_perf
    group by 1, 2
),

with_fund_context as (
    select
        wwc.*,
        fqa.fund_total_nav,
        fqa.fund_avg_quarterly_return,
        fqa.fund_portfolio_count
    from with_window_calcs wwc
    left join fund_quarterly_aggs fqa
        on wwc.fund_id = fqa.fund_id
        and wwc.quarter_end = fqa.quarter_end
),

-- ISSUE: Complex derived metrics with repeated CASE statements
with_derived_metrics as (
    select
        wfc.*,
        -- ISSUE: Portfolio weight in fund
        case
            when wfc.fund_total_nav > 0
            then (wfc.nav_usd / wfc.fund_total_nav) * 100
            else null
        end as portfolio_weight_in_fund,
        -- ISSUE: Complex QoQ calculations (repeated division logic)
        case
            when wfc.prior_1q_nav is not null and wfc.prior_1q_nav > 0
            then ((wfc.nav_usd - wfc.prior_1q_nav) / wfc.prior_1q_nav) * 100
            else null
        end as qoq_nav_growth_pct,
        case
            when wfc.prior_1q_return is not null
            then (wfc.quarterly_return - wfc.prior_1q_return)
            else null
        end as qoq_return_change,
        -- ISSUE: Complex YoY calculations
        case
            when wfc.prior_4q_nav is not null and wfc.prior_4q_nav > 0
            then ((wfc.nav_usd - wfc.prior_4q_nav) / wfc.prior_4q_nav) * 100
            else null
        end as yoy_nav_growth_pct,
        case
            when wfc.prior_4q_return is not null
            then (wfc.quarterly_return - wfc.prior_4q_return)
            else null
        end as yoy_return_change,
        -- ISSUE: 2-year growth
        case
            when wfc.prior_8q_nav is not null and wfc.prior_8q_nav > 0
            then ((wfc.nav_usd - wfc.prior_8q_nav) / wfc.prior_8q_nav) * 100
            else null
        end as two_year_nav_growth_pct,
        -- ISSUE: TVPI and DPI calculations (repeated division)
        case
            when wfc.cumulative_contributions > 0
            then (wfc.nav_usd + wfc.cumulative_distributions) / wfc.cumulative_contributions
            else null
        end as tvpi,
        case
            when wfc.cumulative_contributions > 0
            then wfc.cumulative_distributions / wfc.cumulative_contributions
            else null
        end as dpi,
        case
            when wfc.cumulative_contributions > 0
            then wfc.nav_usd / wfc.cumulative_contributions
            else null
        end as rvpi,
        -- ISSUE: Performance trend classification (complex nested CASE)
        case
            when wfc.rolling_4q_avg_return > wfc.rolling_8q_avg_return * 1.2 then 'ACCELERATING'
            when wfc.rolling_4q_avg_return > wfc.rolling_8q_avg_return * 1.05 then 'IMPROVING'
            when wfc.rolling_4q_avg_return < wfc.rolling_8q_avg_return * 0.8 then 'DECELERATING'
            when wfc.rolling_4q_avg_return < wfc.rolling_8q_avg_return * 0.95 then 'DECLINING'
            else 'STABLE'
        end as performance_trend,
        -- ISSUE: Consistency rating (complex nested CASE)
        case
            when wfc.rolling_4q_volatility < 0.02 then 'VERY_CONSISTENT'
            when wfc.rolling_4q_volatility < 0.05 then 'CONSISTENT'
            when wfc.rolling_4q_volatility < 0.10 then 'MODERATE'
            when wfc.rolling_4q_volatility < 0.15 then 'VARIABLE'
            else 'HIGHLY_VARIABLE'
        end as consistency_rating,
        -- ISSUE: Relative to fund performance (nested CASE)
        case
            when wfc.quarterly_return > wfc.fund_avg_quarterly_return + 0.05 then 'SIGNIFICANT_OUTPERFORM'
            when wfc.quarterly_return > wfc.fund_avg_quarterly_return + 0.02 then 'OUTPERFORM'
            when wfc.quarterly_return < wfc.fund_avg_quarterly_return - 0.05 then 'SIGNIFICANT_UNDERPERFORM'
            when wfc.quarterly_return < wfc.fund_avg_quarterly_return - 0.02 then 'UNDERPERFORM'
            else 'IN_LINE'
        end as relative_to_fund
    from with_fund_context wfc
),

-- ISSUE: More complex calculations and string operations
final as (
    select
        wdm.portfolio_id,
        wdm.portfolio_name,
        wdm.portfolio_type,
        wdm.fund_id,
        wdm.quarter_end,
        wdm.quarter_start,
        wdm.valuation_year,
        wdm.valuation_quarter,
        -- ISSUE: String concatenations (slow)
        concat('Q', wdm.valuation_quarter, ' ', wdm.valuation_year) as quarter_label,
        concat(wdm.valuation_year, '-Q', wdm.valuation_quarter) as quarter_code,
        concat(wdm.portfolio_name, ' (', wdm.portfolio_type, ')') as portfolio_display,
        -- Core metrics
        wdm.nav_usd,
        wdm.quarterly_return,
        wdm.benchmark_quarterly_return,
        wdm.quarterly_excess_return,
        wdm.portfolio_return_1y as trailing_1y_return,
        wdm.portfolio_cumulative_return as since_inception_return,
        wdm.sharpe_ratio,
        wdm.sortino_ratio,
        wdm.max_drawdown,
        wdm.performance_rating,
        wdm.risk_adjusted_rating,
        -- Cashflow metrics
        wdm.quarterly_contributions,
        wdm.quarterly_distributions,
        wdm.quarterly_dividends,
        wdm.quarterly_fees,
        wdm.quarterly_net_cashflow,
        wdm.cumulative_contributions,
        wdm.cumulative_distributions,
        wdm.total_transactions,
        -- Period comparisons
        wdm.qoq_nav_growth_pct,
        wdm.qoq_return_change,
        wdm.yoy_nav_growth_pct,
        wdm.yoy_return_change,
        wdm.two_year_nav_growth_pct,
        -- Performance ratios
        wdm.tvpi,
        wdm.dpi,
        wdm.rvpi,
        -- Rolling metrics
        wdm.rolling_4q_avg_return,
        wdm.rolling_8q_avg_return,
        wdm.rolling_4q_volatility,
        wdm.rolling_4q_min_return,
        wdm.rolling_4q_max_return,
        -- Fund context
        wdm.fund_total_nav,
        wdm.fund_avg_quarterly_return,
        wdm.fund_portfolio_count,
        wdm.portfolio_weight_in_fund,
        -- Classifications
        wdm.performance_trend,
        wdm.consistency_rating,
        wdm.relative_to_fund,
        wdm.best_quarter_rank,
        wdm.worst_quarter_rank,
        -- ISSUE: Additional derived fields (repeated calculations)
        case
            when wdm.cumulative_contributions > 0
            then wdm.cumulative_distributions / wdm.cumulative_contributions * 100
            else null
        end as distribution_yield_pct,
        case
            when wdm.cumulative_contributions > 0
            then wdm.cumulative_fees / wdm.cumulative_contributions * 100
            else null
        end as fee_burden_pct
    from with_derived_metrics wdm
)

select * from final
order by portfolio_id, quarter_end
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.report_lp_quarterly"} */;
[0m20:24:38.836776 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.report_ic_dashboard: create or replace transient table DBT_DEMO.DEV_pipeline_c.report_ic_dashboard
    
    
    
    as (-- Pipeline C: Complex Portfolio Analytics
-- Model: report_ic_dashboard
-- Description: Investment Committee dashboard report
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Combines multiple fact tables
-- 2. Re-aggregates aggregated data
-- 3. Complex pivoting logic
-- 4. Multiple CTEs that could be simplified

with portfolio_performance as (
    select * from DBT_DEMO.DEV_pipeline_c.fact_portfolio_performance
),

fund_summary as (
    select * from DBT_DEMO.DEV_pipeline_c.fact_fund_summary
),

position_snapshot as (
    select * from DBT_DEMO.DEV_pipeline_c.fact_position_snapshot
),

-- ISSUE: Get latest performance per portfolio
latest_portfolio_perf as (
    select *
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id
                order by valuation_date desc
            ) as rn
        from portfolio_performance
    )
    where rn = 1  -- ISSUE: Should use QUALIFY
),

-- ISSUE: Get latest positions per portfolio
latest_positions as (
    select *
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id, security_id
                order by position_date desc
            ) as rn
        from position_snapshot
    )
    where rn = 1
),

-- ISSUE: Re-aggregate positions for portfolio summary
position_summary as (
    select
        portfolio_id,
        count(distinct security_id) as total_positions,
        count(distinct sector) as sector_count,
        sum(market_value_usd) as total_market_value,
        max(weight_pct) as max_position_weight,
        -- Concentration metrics
        sum(case when weight_pct >= 0.05 then 1 else 0 end) as positions_over_5pct
    from latest_positions
    group by 1
),

-- ISSUE: Sector concentration
sector_concentration as (
    select
        portfolio_id,
        listagg(sector, ', ') within group (order by sector_weight desc) as top_sectors
    from (
        select
            portfolio_id,
            sector,
            sum(weight_pct) as sector_weight,
            row_number() over (partition by portfolio_id order by sum(weight_pct) desc) as sector_rank
        from latest_positions
        group by 1, 2
    )
    where sector_rank <= 3
    group by 1
),

-- ISSUE: Combine all metrics
dashboard_data as (
    select
        lpp.portfolio_id,
        lpp.portfolio_name,
        lpp.portfolio_type,
        lpp.fund_id,
        fs.fund_name,
        lpp.valuation_date as as_of_date,
        lpp.nav_usd,
        -- Performance
        lpp.portfolio_return_1m,
        lpp.portfolio_return_3m,
        lpp.portfolio_return_1y,
        lpp.portfolio_cumulative_return as inception_return,
        -- Benchmark comparison
        lpp.benchmark_id,
        lpp.excess_return_1m,
        lpp.excess_return_1y,
        lpp.information_ratio,
        -- Risk
        lpp.portfolio_volatility,
        lpp.sharpe_ratio,
        lpp.sortino_ratio,
        lpp.max_drawdown,
        lpp.var_95_1d,
        -- Positions
        ps.total_positions,
        ps.sector_count,
        ps.max_position_weight,
        ps.positions_over_5pct,
        sc.top_sectors,
        -- Fund level
        fs.total_nav_usd as fund_total_nav,
        fs.portfolio_count as fund_portfolio_count,
        fs.weighted_sharpe_ratio as fund_sharpe,
        -- Portfolio share of fund
        case
            when fs.total_nav_usd > 0
            then lpp.nav_usd / fs.total_nav_usd * 100
            else null
        end as pct_of_fund
    from latest_portfolio_perf lpp
    left join fund_summary fs
        on lpp.fund_id = fs.fund_id
        and lpp.valuation_date = fs.valuation_date
    left join position_summary ps
        on lpp.portfolio_id = ps.portfolio_id
    left join sector_concentration sc
        on lpp.portfolio_id = sc.portfolio_id
),

-- ISSUE: Final scoring/ranking
final as (
    select
        *,
        -- Performance score (simplified)
        (coalesce(portfolio_return_1y, 0) * 0.4 +
         coalesce(sharpe_ratio, 0) * 0.3 +
         coalesce(information_ratio, 0) * 0.3) as composite_score,
        rank() over (order by portfolio_return_1y desc nulls last) as return_rank,
        rank() over (order by sharpe_ratio desc nulls last) as sharpe_rank
    from dashboard_data
)

select * from final
order by composite_score desc
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.report_ic_dashboard"} */;
[0m20:24:40.299128 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 1.461 seconds
[0m20:24:40.301777 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113934490>]}
[0m20:24:40.302476 [info ] [Thread-4  ]: 31 of 32 OK created sql table model DEV_pipeline_c.report_ic_dashboard ......... [[32mSUCCESS 1[0m in 1.50s]
[0m20:24:40.303001 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.report_ic_dashboard
[0m20:24:40.728081 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 1.891 seconds
[0m20:24:40.731787 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113938580>]}
[0m20:24:40.732859 [info ] [Thread-3  ]: 32 of 32 OK created sql table model DEV_pipeline_c.report_lp_quarterly ......... [[32mSUCCESS 1[0m in 1.93s]
[0m20:24:40.733834 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.report_lp_quarterly
[0m20:24:40.735554 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:24:40.736050 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.fact_portfolio_performance' was left open.
[0m20:24:40.736536 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.fact_portfolio_performance: Close
[0m20:24:41.041893 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.report_trading_performance' was left open.
[0m20:24:41.042489 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.report_trading_performance: Close
[0m20:24:41.328491 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.report_lp_quarterly' was left open.
[0m20:24:41.328973 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.report_lp_quarterly: Close
[0m20:24:41.638322 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.report_ic_dashboard' was left open.
[0m20:24:41.638842 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.report_ic_dashboard: Close
[0m20:24:41.924641 [info ] [MainThread]: 
[0m20:24:41.925492 [info ] [MainThread]: Finished running 11 table models, 21 view models in 0 hours 1 minutes and 0.64 seconds (60.64s).
[0m20:24:41.931905 [debug] [MainThread]: Command end result
[0m20:24:41.992681 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/manifest.json
[0m20:24:41.994818 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/semantic_manifest.json
[0m20:24:42.002251 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/nedazarei/Documents/turintech/dbtproject/target/run_results.json
[0m20:24:42.002572 [info ] [MainThread]: 
[0m20:24:42.002907 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:24:42.003202 [info ] [MainThread]: 
[0m20:24:42.003492 [info ] [MainThread]: Done. PASS=32 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=32
[0m20:24:42.004012 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 10 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m20:24:42.006317 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 64.04781, "process_in_blocks": "0", "process_kernel_time": 0.617423, "process_mem_max_rss": "204423168", "process_out_blocks": "0", "process_user_time": 5.830906}
[0m20:24:42.006655 [debug] [MainThread]: Command `dbt run` succeeded at 20:24:42.006589 after 64.05 seconds
[0m20:24:42.006969 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ad5100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bfccc40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d63fa30>]}
[0m20:24:42.007291 [debug] [MainThread]: Flushing usage events
[0m20:24:43.068137 [debug] [MainThread]: An error was encountered while trying to flush usage events
