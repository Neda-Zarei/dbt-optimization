[0m20:23:26.757727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a9811c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba30490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba302b0>]}


============================== 20:23:26.763559 | 21849da2-fa47-4a88-b001-b2e68de14cce ==============================
[0m20:23:26.763559 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:23:26.764066 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'fail_fast': 'False', 'no_print': 'None', 'introspect': 'True', 'log_path': '/Users/nedazarei/Documents/turintech/dbtproject/logs', 'use_colors': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'debug': 'False', 'version_check': 'True', 'warn_error': 'None', 'log_cache_events': 'False', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'log_format': 'default', 'use_experimental_parser': 'False', 'profiles_dir': '/Users/nedazarei/Documents/turintech/dbtproject', 'target_path': 'None', 'empty': 'None', 'invocation_command': 'dbt deps', 'cache_selected_only': 'False'}
[0m20:23:26.922730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '21849da2-fa47-4a88-b001-b2e68de14cce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b9882b0>]}
[0m20:23:26.943452 [debug] [MainThread]: Set downloads directory='/var/folders/9d/rc17811j07v56khq2t9t7h5m0000gn/T/dbt-downloads-wkw3g488'
[0m20:23:26.943863 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m20:23:27.365625 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m20:23:27.368731 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m20:23:27.772770 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m20:23:27.782202 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m20:23:28.787488 [info ] [MainThread]: Installed from version 1.3.3
[0m20:23:28.787894 [info ] [MainThread]: Up to date!
[0m20:23:28.788227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '21849da2-fa47-4a88-b001-b2e68de14cce', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bb5b340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba09b20>]}
[0m20:23:28.790870 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 2.1120458, "process_in_blocks": "0", "process_kernel_time": 0.261893, "process_mem_max_rss": "110166016", "process_out_blocks": "0", "process_user_time": 1.488211}
[0m20:23:28.791261 [debug] [MainThread]: Command `dbt deps` succeeded at 20:23:28.791177 after 2.11 seconds
[0m20:23:28.791564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a9811c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bb551c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10acd1ca0>]}
[0m20:23:28.791874 [debug] [MainThread]: Flushing usage events
[0m20:23:29.685044 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:23:38.034666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ad5100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ab834f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ab83310>]}


============================== 20:23:38.039859 | 662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3 ==============================
[0m20:23:38.039859 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m20:23:38.040385 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'partial_parse': 'True', 'write_json': 'True', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'static_parser': 'True', 'profiles_dir': '/Users/nedazarei/Documents/turintech/dbtproject', 'use_experimental_parser': 'False', 'log_path': '/Users/nedazarei/Documents/turintech/dbtproject/logs', 'cache_selected_only': 'False', 'introspect': 'True', 'invocation_command': 'dbt run --full-refresh', 'version_check': 'True', 'printer_width': '80', 'debug': 'False', 'no_print': 'None', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'target_path': 'None', 'fail_fast': 'False', 'use_colors': 'True'}
[0m20:23:38.589211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108389820>]}
[0m20:23:38.661724 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ddc1f0>]}
[0m20:23:38.662928 [info ] [MainThread]: Registered adapter: snowflake=1.10.2
[0m20:23:38.795936 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m20:23:38.796884 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m20:23:38.797243 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cb7f3d0>]}
[0m20:23:40.760563 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `accepted_values` defined on 'stg_trades' in
package 'bain_capital_portfolio_analytics' (models/pipeline_b/schema.yml).
Arguments to generic tests should be nested under the `arguments` property.
[0m20:23:40.761200 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cf6ebb0>]}
[0m20:23:41.103054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d5fa130>]}
[0m20:23:41.256207 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/manifest.json
[0m20:23:41.258817 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/semantic_manifest.json
[0m20:23:41.282050 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d63fac0>]}
[0m20:23:41.282557 [info ] [MainThread]: Found 32 models, 90 data tests, 5 seeds, 12 sources, 632 macros
[0m20:23:41.282873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d712070>]}
[0m20:23:41.286280 [info ] [MainThread]: 
[0m20:23:41.286590 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m20:23:41.286853 [info ] [MainThread]: 
[0m20:23:41.287316 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m20:23:41.293886 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO'
[0m20:23:41.294388 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO'
[0m20:23:41.294804 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO'
[0m20:23:41.397188 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO"
[0m20:23:41.397698 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO"
[0m20:23:41.398121 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO"
[0m20:23:41.398434 [debug] [ThreadPool]: On list_DBT_DEMO: show terse schemas in database DBT_DEMO
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO"} */
[0m20:23:41.398729 [debug] [ThreadPool]: On list_DBT_DEMO: show terse schemas in database DBT_DEMO
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO"} */
[0m20:23:41.399024 [debug] [ThreadPool]: On list_DBT_DEMO: show terse schemas in database DBT_DEMO
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO"} */
[0m20:23:41.399295 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:23:41.399575 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:23:41.399826 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:23:44.956656 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 3.556 seconds
[0m20:23:45.187243 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 3.788 seconds
[0m20:23:45.246439 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 3.847 seconds
[0m20:23:45.258108 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DBT_DEMO, now list_DBT_DEMO_DEV)
[0m20:23:45.258775 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DBT_DEMO, now list_DBT_DEMO_DEV_pipeline_b)
[0m20:23:45.266198 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DBT_DEMO, now list_DBT_DEMO_DEV_pipeline_a)
[0m20:23:45.272215 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV"
[0m20:23:45.274635 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV_pipeline_b"
[0m20:23:45.275094 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO_DEV_pipeline_c'
[0m20:23:45.278748 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV_pipeline_a"
[0m20:23:45.279142 [debug] [ThreadPool]: On list_DBT_DEMO_DEV: show objects in DBT_DEMO.DEV
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV"} */;
[0m20:23:45.279463 [debug] [ThreadPool]: On list_DBT_DEMO_DEV_pipeline_b: show objects in DBT_DEMO.DEV_pipeline_b
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV_pipeline_b"} */;
[0m20:23:45.281781 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV_pipeline_c"
[0m20:23:45.282081 [debug] [ThreadPool]: On list_DBT_DEMO_DEV_pipeline_a: show objects in DBT_DEMO.DEV_pipeline_a
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV_pipeline_a"} */;
[0m20:23:45.282831 [debug] [ThreadPool]: On list_DBT_DEMO_DEV_pipeline_c: show objects in DBT_DEMO.DEV_pipeline_c
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV_pipeline_c"} */;
[0m20:23:45.283308 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:23:45.516831 [debug] [ThreadPool]: SQL status: SUCCESS 17 in 0.234 seconds
[0m20:23:45.527641 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.244 seconds
[0m20:23:45.530503 [debug] [ThreadPool]: SQL status: SUCCESS 9 in 0.248 seconds
[0m20:23:48.068144 [debug] [ThreadPool]: SQL status: SUCCESS 19 in 2.784 seconds
[0m20:23:48.075788 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d7be280>]}
[0m20:23:48.079753 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.stg_benchmark_returns
[0m20:23:48.080150 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.stg_benchmarks
[0m20:23:48.080499 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.stg_brokers
[0m20:23:48.081533 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.stg_cashflows
[0m20:23:48.080988 [info ] [Thread-1  ]: 1 of 32 START sql view model DEV_pipeline_c.stg_benchmark_returns .............. [RUN]
[0m20:23:48.082045 [info ] [Thread-2  ]: 2 of 32 START sql view model DEV_pipeline_c.stg_benchmarks ..................... [RUN]
[0m20:23:48.082462 [info ] [Thread-3  ]: 3 of 32 START sql view model DEV_pipeline_b.stg_brokers ........................ [RUN]
[0m20:23:48.082866 [info ] [Thread-4  ]: 4 of 32 START sql view model DEV_pipeline_a.stg_cashflows ...................... [RUN]
[0m20:23:48.083271 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV, now model.bain_capital_portfolio_analytics.stg_benchmark_returns)
[0m20:23:48.083608 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV_pipeline_b, now model.bain_capital_portfolio_analytics.stg_benchmarks)
[0m20:23:48.083925 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV_pipeline_a, now model.bain_capital_portfolio_analytics.stg_brokers)
[0m20:23:48.084231 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV_pipeline_c, now model.bain_capital_portfolio_analytics.stg_cashflows)
[0m20:23:48.084548 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_benchmark_returns
[0m20:23:48.084839 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_benchmarks
[0m20:23:48.085119 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_brokers
[0m20:23:48.085409 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_cashflows
[0m20:23:48.094431 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_benchmark_returns"
[0m20:23:48.098132 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_benchmarks"
[0m20:23:48.100874 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_brokers"
[0m20:23:48.103959 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_cashflows"
[0m20:23:48.105113 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.stg_benchmark_returns
[0m20:23:48.117754 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.stg_benchmarks
[0m20:23:48.118169 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.stg_brokers
[0m20:23:48.129432 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.stg_cashflows
[0m20:23:48.137663 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_benchmark_returns"
[0m20:23:48.140646 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_benchmarks"
[0m20:23:48.143386 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_brokers"
[0m20:23:48.146028 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_cashflows"
[0m20:23:48.148043 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_benchmarks"
[0m20:23:48.149706 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_benchmark_returns"
[0m20:23:48.150063 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.stg_benchmarks: create or replace   view DBT_DEMO.DEV_pipeline_c.stg_benchmarks
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: stg_benchmarks
-- Description: Benchmark index data for performance comparison

with source as (
    select
        benchmark_id,
        benchmark_name,
        benchmark_ticker,
        asset_class,
        region,
        is_active,
        created_at,
        updated_at
    from DBT_DEMO.DEV.benchmarks
),

-- ISSUE: Subquery for deduplication
deduplicated as (
    select *
    from (
        select
            *,
            row_number() over (partition by benchmark_id order by updated_at desc) as rn
        from source
    )
    where rn = 1
)

select
    benchmark_id,
    trim(benchmark_name) as benchmark_name,
    upper(trim(benchmark_ticker)) as benchmark_ticker,
    upper(asset_class) as asset_class,
    upper(region) as region,
    is_active,
    created_at,
    updated_at
from deduplicated
where is_active = true
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_benchmarks"} */;
[0m20:23:48.150537 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.stg_benchmark_returns: create or replace   view DBT_DEMO.DEV_pipeline_c.stg_benchmark_returns
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: stg_benchmark_returns
-- Description: Daily benchmark return data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-join for cumulative returns (inefficient)
-- 2. Multiple window functions

with source as (
    select
        benchmark_id,
        return_date,
        daily_return,
        index_level,
        created_at
    from DBT_DEMO.DEV.benchmark_returns
    where return_date >= '2020-01-01'
),

-- ISSUE: Multiple window functions that could be consolidated
with_cumulative as (
    select
        benchmark_id,
        return_date,
        daily_return,
        index_level,
        -- ISSUE: Separate window functions for each period
        exp(sum(ln(1 + daily_return)) over (
            partition by benchmark_id
            order by return_date
            rows between unbounded preceding and current row
        )) - 1 as cumulative_return,
        exp(sum(ln(1 + daily_return)) over (
            partition by benchmark_id
            order by return_date
            rows between 29 preceding and current row
        )) - 1 as return_30d,
        exp(sum(ln(1 + daily_return)) over (
            partition by benchmark_id
            order by return_date
            rows between 89 preceding and current row
        )) - 1 as return_90d,
        exp(sum(ln(1 + daily_return)) over (
            partition by benchmark_id
            order by return_date
            rows between 364 preceding and current row
        )) - 1 as return_1y,
        stddev(daily_return) over (
            partition by benchmark_id
            order by return_date
            rows between 251 preceding and current row
        ) * sqrt(252) as annualized_volatility,
        created_at
    from source
)

select * from with_cumulative
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_benchmark_returns"} */;
[0m20:23:48.151689 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_cashflows"
[0m20:23:48.152717 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_brokers"
[0m20:23:48.153488 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.stg_cashflows: create or replace   view DBT_DEMO.DEV_pipeline_a.stg_cashflows
  
  
  
  
  as (
    -- Pipeline A: Simple Cashflow Pipeline
-- Model: stg_cashflows
-- Description: Staging model for raw cashflow data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Unnecessary DISTINCT (source already unique)
-- 2. Late filtering (should push date filter upstream)
-- 3. Non-optimal date casting

with source as (
    select distinct  -- ISSUE: Unnecessary DISTINCT, source has unique constraint
        cashflow_id,
        portfolio_id,
        cashflow_type,
        cashflow_date,
        amount,
        currency,
        created_at,
        updated_at
    from DBT_DEMO.DEV.cashflows
),

-- ISSUE: Heavy transformation before filtering
converted as (
    select
        cashflow_id,
        portfolio_id,
        upper(cashflow_type) as cashflow_type,
        cast(cashflow_date as date) as cashflow_date,
        cast(amount as decimal(18,2)) as amount,
        upper(currency) as currency,
        cast(created_at as timestamp) as created_at,
        cast(updated_at as timestamp) as updated_at
    from source
),

-- ISSUE: Filter applied after transformation, should be earlier
filtered as (
    select *
    from converted
    where cashflow_date >= '2020-01-01'
      and cashflow_date <= '2024-12-31'
)

select * from filtered
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_cashflows"} */;
[0m20:23:48.153941 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.stg_brokers: create or replace   view DBT_DEMO.DEV_pipeline_b.stg_brokers
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: stg_brokers
-- Description: Staging model for broker information

with source as (
    select
        broker_id,
        broker_name,
        broker_type,
        region,
        is_active,
        commission_rate,
        created_at,
        updated_at
    from DBT_DEMO.DEV.brokers
),

deduplicated as (
    select *
    from (
        select
            *,
            row_number() over (partition by broker_id order by updated_at desc) as rn
        from source
    )
    where rn = 1
)

select
    broker_id,
    trim(broker_name) as broker_name,
    upper(broker_type) as broker_type,
    upper(region) as region,
    is_active,
    commission_rate,
    created_at,
    updated_at
from deduplicated
where is_active = true
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_brokers"} */;
[0m20:23:48.408434 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.255 seconds
[0m20:23:48.437740 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 0.283 seconds
[0m20:23:48.438453 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.285 seconds
[0m20:23:48.442585 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d40c0d0>]}
[0m20:23:48.442915 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d5f7610>]}
[0m20:23:48.443758 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d34d610>]}
[0m20:23:48.443517 [info ] [Thread-2  ]: 2 of 32 OK created sql view model DEV_pipeline_c.stg_benchmarks ................ [[32mSUCCESS 1[0m in 0.35s]
[0m20:23:48.444350 [info ] [Thread-3  ]: 3 of 32 OK created sql view model DEV_pipeline_b.stg_brokers ................... [[32mSUCCESS 1[0m in 0.36s]
[0m20:23:48.444845 [info ] [Thread-1  ]: 1 of 32 OK created sql view model DEV_pipeline_c.stg_benchmark_returns ......... [[32mSUCCESS 1[0m in 0.36s]
[0m20:23:48.445337 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.stg_benchmarks
[0m20:23:48.445757 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.stg_brokers
[0m20:23:48.446187 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.stg_benchmark_returns
[0m20:23:48.446539 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.stg_fund_hierarchy
[0m20:23:48.446949 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.stg_market_prices
[0m20:23:48.447354 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks
[0m20:23:48.447804 [info ] [Thread-2  ]: 5 of 32 START sql view model DEV_pipeline_c.stg_fund_hierarchy ................. [RUN]
[0m20:23:48.448228 [info ] [Thread-3  ]: 6 of 32 START sql view model DEV_pipeline_b.stg_market_prices .................. [RUN]
[0m20:23:48.448637 [info ] [Thread-1  ]: 7 of 32 START sql view model DEV_pipeline_c.stg_portfolio_benchmarks ........... [RUN]
[0m20:23:48.449075 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_benchmarks, now model.bain_capital_portfolio_analytics.stg_fund_hierarchy)
[0m20:23:48.449401 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_brokers, now model.bain_capital_portfolio_analytics.stg_market_prices)
[0m20:23:48.449724 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_benchmark_returns, now model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks)
[0m20:23:48.450040 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_fund_hierarchy
[0m20:23:48.450388 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_market_prices
[0m20:23:48.450692 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks
[0m20:23:48.455139 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_fund_hierarchy"
[0m20:23:48.459602 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_market_prices"
[0m20:23:48.462315 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks"
[0m20:23:48.462998 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.stg_fund_hierarchy
[0m20:23:48.466075 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_fund_hierarchy"
[0m20:23:48.466518 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.stg_market_prices
[0m20:23:48.469396 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_market_prices"
[0m20:23:48.469801 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks
[0m20:23:48.472537 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks"
[0m20:23:48.473513 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_fund_hierarchy"
[0m20:23:48.473873 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.stg_fund_hierarchy: create or replace   view DBT_DEMO.DEV_pipeline_c.stg_fund_hierarchy
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: stg_fund_hierarchy
-- Description: Fund and portfolio hierarchy for roll-up reporting

with source as (
    select
        entity_id,
        entity_name,
        entity_type,
        parent_entity_id,
        hierarchy_level,
        is_active,
        created_at,
        updated_at
    from DBT_DEMO.DEV.fund_hierarchy
)

select
    entity_id,
    trim(entity_name) as entity_name,
    upper(entity_type) as entity_type,
    parent_entity_id,
    hierarchy_level,
    is_active,
    created_at,
    updated_at
from source
where is_active = true
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_fund_hierarchy"} */;
[0m20:23:48.476213 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_market_prices"
[0m20:23:48.476753 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.stg_market_prices: create or replace   view DBT_DEMO.DEV_pipeline_b.stg_market_prices
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: stg_market_prices
-- Description: Staging model for daily market prices
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-join for prior day prices (inefficient)
-- 2. Late aggregation
-- 3. Multiple window functions that could be consolidated

with source as (
    select
        security_id,
        price_date,
        open_price,
        high_price,
        low_price,
        close_price,
        volume,
        created_at
    from DBT_DEMO.DEV.market_prices
    where price_date >= '2020-01-01'
),

-- ISSUE: Self-join to get prior day price (should use LAG)
with_prior_day as (
    select
        curr.security_id,
        curr.price_date,
        curr.open_price,
        curr.high_price,
        curr.low_price,
        curr.close_price,
        curr.volume,
        prev.close_price as prior_close,
        prev.volume as prior_volume
    from source curr
    left join source prev
        on curr.security_id = prev.security_id
        and curr.price_date = dateadd('day', 1, prev.price_date)  -- ISSUE: Doesn't handle weekends
),

-- ISSUE: Multiple separate window functions
with_returns as (
    select
        *,
        -- Daily return
        case
            when prior_close > 0
            then (close_price - prior_close) / prior_close
            else null
        end as daily_return,
        -- ISSUE: These could be computed together
        avg(close_price) over (
            partition by security_id
            order by price_date
            rows between 19 preceding and current row
        ) as ma_20,
        avg(close_price) over (
            partition by security_id
            order by price_date
            rows between 49 preceding and current row
        ) as ma_50,
        avg(close_price) over (
            partition by security_id
            order by price_date
            rows between 199 preceding and current row
        ) as ma_200,
        stddev(close_price) over (
            partition by security_id
            order by price_date
            rows between 19 preceding and current row
        ) as volatility_20d,
        avg(volume) over (
            partition by security_id
            order by price_date
            rows between 19 preceding and current row
        ) as avg_volume_20d
    from with_prior_day
),

-- ISSUE: Another pass for more calculations
final as (
    select
        *,
        case
            when ma_20 > ma_50 and ma_50 > ma_200 then 'BULLISH'
            when ma_20 < ma_50 and ma_50 < ma_200 then 'BEARISH'
            else 'NEUTRAL'
        end as trend_signal,
        case
            when volume > avg_volume_20d * 2 then 'HIGH'
            when volume < avg_volume_20d * 0.5 then 'LOW'
            else 'NORMAL'
        end as volume_signal
    from with_returns
)

select * from final
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_market_prices"} */;
[0m20:23:48.477718 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks"
[0m20:23:48.478442 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks: create or replace   view DBT_DEMO.DEV_pipeline_c.stg_portfolio_benchmarks
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: stg_portfolio_benchmarks
-- Description: Mapping of portfolios to their benchmarks

with source as (
    select
        portfolio_id,
        benchmark_id,
        is_primary,
        start_date,
        end_date,
        created_at,
        updated_at
    from DBT_DEMO.DEV.portfolio_benchmarks
)

select
    portfolio_id,
    benchmark_id,
    is_primary,
    cast(start_date as date) as start_date,
    cast(end_date as date) as end_date,
    created_at,
    updated_at
from source
where end_date is null or end_date >= current_date()
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks"} */;
[0m20:23:48.496989 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 0.342 seconds
[0m20:23:48.498655 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d1fdbb0>]}
[0m20:23:48.499180 [info ] [Thread-4  ]: 4 of 32 OK created sql view model DEV_pipeline_a.stg_cashflows ................. [[32mSUCCESS 1[0m in 0.41s]
[0m20:23:48.499653 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.stg_cashflows
[0m20:23:48.499978 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.stg_portfolios
[0m20:23:48.500416 [info ] [Thread-4  ]: 8 of 32 START sql view model DEV_pipeline_a.stg_portfolios ..................... [RUN]
[0m20:23:48.500805 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_cashflows, now model.bain_capital_portfolio_analytics.stg_portfolios)
[0m20:23:48.501112 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_portfolios
[0m20:23:48.559302 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_portfolios"
[0m20:23:48.560128 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.stg_portfolios
[0m20:23:48.563189 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_portfolios"
[0m20:23:48.564558 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_portfolios"
[0m20:23:48.564924 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.stg_portfolios: create or replace   view DBT_DEMO.DEV_pipeline_a.stg_portfolios
  
  
  
  
  as (
    -- Pipeline A: Simple Cashflow Pipeline
-- Model: stg_portfolios
-- Description: Staging model for portfolio master data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Subquery for deduplication instead of QUALIFY
-- 2. Multiple passes over data

with source as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        inception_date,
        status,
        currency,
        created_at,
        updated_at,
        row_number() over (
            partition by portfolio_id
            order by updated_at desc
        ) as rn
    from DBT_DEMO.DEV.portfolios
),

-- ISSUE: Using subquery filter instead of QUALIFY
deduplicated as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        inception_date,
        status,
        currency,
        created_at,
        updated_at
    from source
    where rn = 1  -- ISSUE: Should use QUALIFY in Snowflake
),

-- ISSUE: Another pass just for active filter
active_only as (
    select *
    from deduplicated
    where status = 'ACTIVE'
)

select * from active_only
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_portfolios"} */;
[0m20:23:48.705140 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.226 seconds
[0m20:23:48.707334 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1137ccdf0>]}
[0m20:23:48.707966 [info ] [Thread-1  ]: 7 of 32 OK created sql view model DEV_pipeline_c.stg_portfolio_benchmarks ...... [[32mSUCCESS 1[0m in 0.26s]
[0m20:23:48.708533 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks
[0m20:23:48.708914 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.stg_positions_daily
[0m20:23:48.709447 [info ] [Thread-1  ]: 9 of 32 START sql view model DEV_pipeline_c.stg_positions_daily ................ [RUN]
[0m20:23:48.709940 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks, now model.bain_capital_portfolio_analytics.stg_positions_daily)
[0m20:23:48.710295 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_positions_daily
[0m20:23:48.714115 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_positions_daily"
[0m20:23:48.714938 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.stg_positions_daily
[0m20:23:48.718339 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_positions_daily"
[0m20:23:48.720597 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_positions_daily"
[0m20:23:48.721039 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.stg_positions_daily: create or replace   view DBT_DEMO.DEV_pipeline_c.stg_positions_daily
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: stg_positions_daily
-- Description: Daily position snapshots from source system
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Heavy transformations before filtering
-- 2. Unnecessary type conversions
-- 3. Could push filters upstream

with source as (
    select
        position_id,
        portfolio_id,
        security_id,
        position_date,
        quantity,
        cost_basis_price,
        cost_basis_value,
        market_price,
        market_value,
        market_value_usd,
        unrealized_pnl,
        unrealized_pnl_pct,
        weight_pct,
        created_at,
        updated_at
    from DBT_DEMO.DEV.positions_daily
),

-- ISSUE: Transformations applied to all rows before filter
transformed as (
    select
        position_id,
        portfolio_id,
        security_id,
        cast(position_date as date) as position_date,
        cast(quantity as decimal(18,6)) as quantity,
        cast(cost_basis_price as decimal(18,4)) as cost_basis_price,
        cast(cost_basis_value as decimal(18,2)) as cost_basis_value,
        cast(market_price as decimal(18,4)) as market_price,
        cast(market_value as decimal(18,2)) as market_value,
        cast(market_value_usd as decimal(18,2)) as market_value_usd,
        cast(unrealized_pnl as decimal(18,2)) as unrealized_pnl,
        cast(unrealized_pnl_pct as decimal(10,4)) as unrealized_pnl_pct,
        cast(weight_pct as decimal(10,6)) as weight_pct,
        created_at,
        updated_at
    from source
),

-- ISSUE: Filter applied last
filtered as (
    select *
    from transformed
    where position_date >= '2020-01-01'
)

select * from filtered
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_positions_daily"} */;
[0m20:23:48.738403 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 0.260 seconds
[0m20:23:48.740268 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11376eb80>]}
[0m20:23:48.740838 [info ] [Thread-3  ]: 6 of 32 OK created sql view model DEV_pipeline_b.stg_market_prices ............. [[32mSUCCESS 1[0m in 0.29s]
[0m20:23:48.741327 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.stg_market_prices
[0m20:23:48.741673 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.stg_securities
[0m20:23:48.742130 [info ] [Thread-3  ]: 10 of 32 START sql view model DEV_pipeline_b.stg_securities .................... [RUN]
[0m20:23:48.742542 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_market_prices, now model.bain_capital_portfolio_analytics.stg_securities)
[0m20:23:48.742861 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_securities
[0m20:23:48.746076 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_securities"
[0m20:23:48.746690 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.stg_securities
[0m20:23:48.751391 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_securities"
[0m20:23:48.753502 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_securities"
[0m20:23:48.753918 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.stg_securities: create or replace   view DBT_DEMO.DEV_pipeline_b.stg_securities
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: stg_securities
-- Description: Staging model for security master data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Nested subqueries instead of QUALIFY
-- 2. Multiple deduplication passes

with source as (
    select
        security_id,
        ticker,
        security_name,
        security_type,
        asset_class,
        sector,
        industry,
        currency,
        exchange,
        is_active,
        created_at,
        updated_at
    from DBT_DEMO.DEV.securities
),

-- ISSUE: Complex deduplication using subquery
deduplicated as (
    select *
    from (
        select
            *,
            row_number() over (
                partition by security_id
                order by updated_at desc
            ) as rn
        from source
    ) sub
    where rn = 1  -- ISSUE: Should use QUALIFY
),

-- ISSUE: Separate CTE for type standardization
standardized as (
    select
        security_id,
        upper(trim(ticker)) as ticker,
        trim(security_name) as security_name,
        -- ISSUE: Repeated CASE logic found in other models
        case
            when security_type in ('STOCK', 'EQUITY', 'COMMON') then 'EQUITY'
            when security_type in ('BOND', 'NOTE', 'DEBENTURE') then 'FIXED_INCOME'
            when security_type in ('OPTION', 'FUTURE', 'SWAP') then 'DERIVATIVE'
            when security_type in ('ETF', 'MUTUAL_FUND') then 'FUND'
            else 'OTHER'
        end as security_type_standardized,
        security_type as security_type_original,
        upper(asset_class) as asset_class,
        sector,
        industry,
        upper(currency) as currency,
        exchange,
        is_active,
        created_at,
        updated_at
    from deduplicated
)

select * from standardized
where is_active = true
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_securities"} */;
[0m20:23:48.797153 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.321 seconds
[0m20:23:48.798899 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109db4eb0>]}
[0m20:23:48.799485 [info ] [Thread-2  ]: 5 of 32 OK created sql view model DEV_pipeline_c.stg_fund_hierarchy ............ [[32mSUCCESS 1[0m in 0.35s]
[0m20:23:48.800019 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.stg_fund_hierarchy
[0m20:23:48.800364 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.stg_trades
[0m20:23:48.800851 [info ] [Thread-2  ]: 11 of 32 START sql view model DEV_pipeline_b.stg_trades ........................ [RUN]
[0m20:23:48.801319 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_fund_hierarchy, now model.bain_capital_portfolio_analytics.stg_trades)
[0m20:23:48.801664 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_trades
[0m20:23:48.805230 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_trades"
[0m20:23:48.805885 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.stg_trades
[0m20:23:48.809186 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_trades"
[0m20:23:48.811241 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_trades"
[0m20:23:48.811788 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.stg_trades: create or replace   view DBT_DEMO.DEV_pipeline_b.stg_trades
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: stg_trades
-- Description: Staging model for trade transactions
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Complex CASE statements that repeat
-- 2. Multiple CTEs doing similar transformations
-- 3. Unnecessary string operations

with source as (
    select
        trade_id,
        portfolio_id,
        security_id,
        broker_id,
        trade_date,
        settlement_date,
        trade_type,
        quantity,
        price,
        gross_amount,
        commission,
        fees,
        net_amount,
        currency,
        created_at,
        updated_at
    from DBT_DEMO.DEV.trades
),

-- ISSUE: Repeated CASE statements for trade categorization
categorized as (
    select
        *,
        -- ISSUE: This logic is repeated in multiple models
        case
            when trade_type in ('BUY', 'COVER') then 'PURCHASE'
            when trade_type in ('SELL', 'SHORT') then 'SALE'
            when trade_type in ('DIVIDEND', 'INTEREST') then 'INCOME'
            else 'OTHER'
        end as trade_category,
        case
            when abs(net_amount) >= 10000000 then 'LARGE'
            when abs(net_amount) >= 1000000 then 'MEDIUM'
            when abs(net_amount) >= 100000 then 'SMALL'
            else 'MICRO'
        end as trade_size_bucket,
        -- ISSUE: Redundant string manipulation
        upper(trim(trade_type)) as trade_type_clean,
        upper(trim(currency)) as currency_clean
    from source
),

-- ISSUE: Another pass just for date calculations
with_dates as (
    select
        *,
        datediff('day', trade_date, settlement_date) as settlement_days,
        date_trunc('month', trade_date) as trade_month,
        date_trunc('quarter', trade_date) as trade_quarter,
        extract(year from trade_date) as trade_year,
        extract(month from trade_date) as trade_month_num,
        dayofweek(trade_date) as trade_day_of_week
    from categorized
),

-- ISSUE: Late filtering
filtered as (
    select *
    from with_dates
    where trade_date >= '2020-01-01'
)

select * from filtered
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_trades"} */;
[0m20:23:48.813137 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 0.248 seconds
[0m20:23:48.815132 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d1fdbb0>]}
[0m20:23:48.815732 [info ] [Thread-4  ]: 8 of 32 OK created sql view model DEV_pipeline_a.stg_portfolios ................ [[32mSUCCESS 1[0m in 0.31s]
[0m20:23:48.816606 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.stg_portfolios
[0m20:23:48.817063 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.stg_valuations
[0m20:23:48.817617 [info ] [Thread-4  ]: 12 of 32 START sql view model DEV_pipeline_c.stg_valuations .................... [RUN]
[0m20:23:48.818201 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_portfolios, now model.bain_capital_portfolio_analytics.stg_valuations)
[0m20:23:48.818550 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_valuations
[0m20:23:48.822053 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_valuations"
[0m20:23:48.822701 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.stg_valuations
[0m20:23:48.826618 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_valuations"
[0m20:23:48.828548 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_valuations"
[0m20:23:48.828931 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.stg_valuations: create or replace   view DBT_DEMO.DEV_pipeline_c.stg_valuations
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: stg_valuations
-- Description: Portfolio valuation data (NAV, etc.)
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Deduplication via subquery
-- 2. Heavy calculations before filtering

with source as (
    select
        valuation_id,
        portfolio_id,
        valuation_date,
        nav,
        nav_per_share,
        shares_outstanding,
        gross_assets,
        total_liabilities,
        net_assets,
        currency,
        fx_rate_to_usd,
        nav_usd,
        created_at,
        updated_at
    from DBT_DEMO.DEV.valuations
),

-- ISSUE: Deduplication using subquery
deduplicated as (
    select *
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id, valuation_date
                order by updated_at desc
            ) as rn
        from source
    )
    where rn = 1
),

-- ISSUE: Filter applied after deduplication
filtered as (
    select
        valuation_id,
        portfolio_id,
        cast(valuation_date as date) as valuation_date,
        cast(nav as decimal(18,2)) as nav,
        cast(nav_per_share as decimal(18,6)) as nav_per_share,
        cast(shares_outstanding as decimal(18,6)) as shares_outstanding,
        cast(gross_assets as decimal(18,2)) as gross_assets,
        cast(total_liabilities as decimal(18,2)) as total_liabilities,
        cast(net_assets as decimal(18,2)) as net_assets,
        upper(currency) as currency,
        cast(fx_rate_to_usd as decimal(18,8)) as fx_rate_to_usd,
        cast(nav_usd as decimal(18,2)) as nav_usd,
        created_at,
        updated_at
    from deduplicated
    where valuation_date >= '2020-01-01'
)

select * from filtered
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_valuations"} */;
[0m20:23:49.054829 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.333 seconds
[0m20:23:49.058513 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c110fa0>]}
[0m20:23:49.059486 [info ] [Thread-1  ]: 9 of 32 OK created sql view model DEV_pipeline_c.stg_positions_daily ........... [[32mSUCCESS 1[0m in 0.35s]
[0m20:23:49.060058 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.stg_positions_daily
[0m20:23:49.060532 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m20:23:49.061194 [info ] [Thread-1  ]: 13 of 32 START sql table model DEV_pipeline_a.fact_cashflow_summary ............ [RUN]
[0m20:23:49.061758 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_positions_daily, now model.bain_capital_portfolio_analytics.fact_cashflow_summary)
[0m20:23:49.062200 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m20:23:49.071114 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_cashflow_summary"
[0m20:23:49.073879 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 0.319 seconds
[0m20:23:49.076778 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11376eb80>]}
[0m20:23:49.077525 [info ] [Thread-3  ]: 10 of 32 OK created sql view model DEV_pipeline_b.stg_securities ............... [[32mSUCCESS 1[0m in 0.33s]
[0m20:23:49.078000 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m20:23:49.078566 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.stg_securities
[0m20:23:49.102663 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_cashflow_summary"
[0m20:23:49.103642 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.int_position_attribution
[0m20:23:49.104770 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 0.275 seconds
[0m20:23:49.104112 [info ] [Thread-3  ]: 14 of 32 START sql view model DEV_pipeline_c.int_position_attribution .......... [RUN]
[0m20:23:49.106628 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1137e6eb0>]}
[0m20:23:49.107023 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_securities, now model.bain_capital_portfolio_analytics.int_position_attribution)
[0m20:23:49.107578 [info ] [Thread-4  ]: 12 of 32 OK created sql view model DEV_pipeline_c.stg_valuations ............... [[32mSUCCESS 1[0m in 0.29s]
[0m20:23:49.107937 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.int_position_attribution
[0m20:23:49.108411 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.stg_valuations
[0m20:23:49.112052 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_position_attribution"
[0m20:23:49.119814 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_cashflow_summary"
[0m20:23:49.120747 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.fact_cashflow_summary: create or replace transient table DBT_DEMO.DEV_pipeline_a.fact_cashflow_summary
    
    
    
    as (-- Pipeline A: Simple Cashflow Pipeline
-- Model: fact_cashflow_summary
-- Description: Fact table summarizing cashflows by portfolio and month
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-joins for prior period comparisons (should use LAG)
-- 2. Late aggregation (aggregates after full join)
-- 3. Repeated window functions with same partitions
-- 4. Redundant date calculations per row
-- 5. Correlated subqueries for fund-level totals

with cashflows as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_cashflows
),

portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

-- ISSUE: Full join before aggregation (scans all rows)
joined as (
    select
        c.cashflow_id,
        c.portfolio_id,
        p.portfolio_name,
        p.portfolio_type,
        p.fund_id,
        c.cashflow_type,
        c.cashflow_date,
        c.amount,
        c.currency,
        -- ISSUE: Redundant date calculations done per row
        date_trunc('month', c.cashflow_date) as cashflow_month,
        date_trunc('quarter', c.cashflow_date) as cashflow_quarter,
        date_trunc('year', c.cashflow_date) as cashflow_year,
        extract(year from c.cashflow_date) as year_num,
        extract(month from c.cashflow_date) as month_num,
        extract(quarter from c.cashflow_date) as quarter_num,
        extract(dayofmonth from c.cashflow_date) as day_num
    from cashflows c
    inner join portfolios p
        on c.portfolio_id = p.portfolio_id
),

-- ISSUE: Aggregation happens after full row-level join
aggregated as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        cashflow_month,
        cashflow_quarter,
        cashflow_year,
        year_num,
        month_num,
        quarter_num,
        cashflow_type,
        currency,
        count(*) as transaction_count,
        count(distinct cashflow_id) as unique_transactions,
        sum(amount) as total_amount,
        avg(amount) as avg_amount,
        min(amount) as min_amount,
        max(amount) as max_amount,
        stddev(amount) as stddev_amount,
        -- ISSUE: Percentile calculations (slow)
        percentile_cont(0.25) within group (order by amount) as p25_amount,
        percentile_cont(0.50) within group (order by amount) as median_amount,
        percentile_cont(0.75) within group (order by amount) as p75_amount
    from joined
    group by 1,2,3,4,5,6,7,8,9,10,11,12  -- ISSUE: Non-descriptive GROUP BY
),

-- ISSUE: Self-join for prior month comparisons (should use LAG)
with_prior_months as (
    select
        agg.*,
        -- ISSUE: Self-join for prior month
        agg_m1.total_amount as prior_1m_total,
        agg_m1.transaction_count as prior_1m_count,
        -- ISSUE: Self-join for 3 months ago
        agg_m3.total_amount as prior_3m_total,
        -- ISSUE: Self-join for 6 months ago
        agg_m6.total_amount as prior_6m_total,
        -- ISSUE: Self-join for 12 months ago
        agg_m12.total_amount as prior_12m_total
    from aggregated agg
    left join aggregated agg_m1
        on agg.portfolio_id = agg_m1.portfolio_id
        and agg.cashflow_type = agg_m1.cashflow_type
        and agg.currency = agg_m1.currency
        and agg_m1.cashflow_month = dateadd(month, -1, agg.cashflow_month)
    left join aggregated agg_m3
        on agg.portfolio_id = agg_m3.portfolio_id
        and agg.cashflow_type = agg_m3.cashflow_type
        and agg.currency = agg_m3.currency
        and agg_m3.cashflow_month = dateadd(month, -3, agg.cashflow_month)
    left join aggregated agg_m6
        on agg.portfolio_id = agg_m6.portfolio_id
        and agg.cashflow_type = agg_m6.cashflow_type
        and agg.currency = agg_m6.currency
        and agg_m6.cashflow_month = dateadd(month, -6, agg.cashflow_month)
    left join aggregated agg_m12
        on agg.portfolio_id = agg_m12.portfolio_id
        and agg.cashflow_type = agg_m12.cashflow_type
        and agg.currency = agg_m12.currency
        and agg_m12.cashflow_month = dateadd(month, -12, agg.cashflow_month)
),

-- ISSUE: Multiple window functions with repeated partitions
with_window_calcs as (
    select
        wpm.*,
        -- ISSUE: Running totals (repeated partition)
        sum(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_total,
        sum(transaction_count) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_count,
        -- ISSUE: Moving averages (same partition repeated)
        avg(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 2 preceding and current row
        ) as rolling_3m_avg,
        avg(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 5 preceding and current row
        ) as rolling_6m_avg,
        avg(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_avg,
        -- ISSUE: More window calculations
        stddev(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_stddev,
        min(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_min,
        max(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_max
    from with_prior_months wpm
),

-- ISSUE: Correlated subqueries for fund-level context (very slow)
with_fund_context as (
    select
        wwc.*,
        -- ISSUE: Correlated subquery for fund total
        (
            select sum(total_amount)
            from aggregated agg2
            inner join portfolios p2
                on agg2.portfolio_id = p2.portfolio_id
            where p2.fund_id = wwc.fund_id
            and agg2.cashflow_month = wwc.cashflow_month
            and agg2.cashflow_type = wwc.cashflow_type
        ) as fund_total_amount,
        -- ISSUE: Another correlated subquery for portfolio count
        (
            select count(distinct agg2.portfolio_id)
            from aggregated agg2
            inner join portfolios p2
                on agg2.portfolio_id = p2.portfolio_id
            where p2.fund_id = wwc.fund_id
            and agg2.cashflow_month = wwc.cashflow_month
            and agg2.cashflow_type = wwc.cashflow_type
        ) as fund_portfolio_count
    from with_window_calcs wwc
),

-- ISSUE: Complex derived metrics with repeated CASE statements
final as (
    select
        md5(cast(coalesce(cast(wfc.portfolio_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wfc.cashflow_month as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wfc.cashflow_type as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wfc.currency as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as cashflow_summary_key,
        wfc.*,
        -- ISSUE: Portfolio share of fund (repeated division)
        case
            when wfc.fund_total_amount > 0
            then (wfc.total_amount / wfc.fund_total_amount) * 100
            else null
        end as portfolio_share_of_fund_pct,
        -- ISSUE: Month-over-month growth calculations
        case
            when wfc.prior_1m_total is not null and wfc.prior_1m_total != 0
            then ((wfc.total_amount - wfc.prior_1m_total) / abs(wfc.prior_1m_total)) * 100
            else null
        end as mom_growth_pct,
        case
            when wfc.prior_3m_total is not null and wfc.prior_3m_total != 0
            then ((wfc.total_amount - wfc.prior_3m_total) / abs(wfc.prior_3m_total)) * 100
            else null
        end as growth_3m_pct,
        case
            when wfc.prior_12m_total is not null and wfc.prior_12m_total != 0
            then ((wfc.total_amount - wfc.prior_12m_total) / abs(wfc.prior_12m_total)) * 100
            else null
        end as yoy_growth_pct,
        -- ISSUE: Trend classification (complex nested CASE)
        case
            when wfc.rolling_3m_avg > wfc.rolling_12m_avg * 1.3 then 'ACCELERATING'
            when wfc.rolling_3m_avg > wfc.rolling_12m_avg * 1.1 then 'GROWING'
            when wfc.rolling_3m_avg < wfc.rolling_12m_avg * 0.7 then 'DECLINING_FAST'
            when wfc.rolling_3m_avg < wfc.rolling_12m_avg * 0.9 then 'DECLINING'
            else 'STABLE'
        end as trend_classification,
        -- ISSUE: Volatility classification
        case
            when wfc.rolling_12m_stddev < wfc.rolling_12m_avg * 0.1 then 'LOW_VOLATILITY'
            when wfc.rolling_12m_stddev < wfc.rolling_12m_avg * 0.3 then 'MODERATE_VOLATILITY'
            when wfc.rolling_12m_stddev < wfc.rolling_12m_avg * 0.5 then 'HIGH_VOLATILITY'
            else 'VERY_HIGH_VOLATILITY'
        end as volatility_classification,
        -- ISSUE: Size classification (repeated CASE)
        case
            when abs(wfc.total_amount) >= 10000000 then 'MEGA'
            when abs(wfc.total_amount) >= 5000000 then 'LARGE'
            when abs(wfc.total_amount) >= 1000000 then 'MEDIUM'
            when abs(wfc.total_amount) >= 100000 then 'SMALL'
            else 'MICRO'
        end as transaction_size_category
    from with_fund_context wfc
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_cashflow_summary"} */;
[0m20:23:49.121475 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.int_benchmark_aligned
[0m20:23:49.122095 [info ] [Thread-4  ]: 15 of 32 START sql view model DEV_pipeline_c.int_benchmark_aligned ............. [RUN]
[0m20:23:49.122808 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_valuations, now model.bain_capital_portfolio_analytics.int_benchmark_aligned)
[0m20:23:49.123508 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.int_benchmark_aligned
[0m20:23:49.128065 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_benchmark_aligned"
[0m20:23:49.129247 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.317 seconds
[0m20:23:49.130937 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109db4eb0>]}
[0m20:23:49.131387 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.int_position_attribution
[0m20:23:49.135162 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_position_attribution"
[0m20:23:49.131830 [info ] [Thread-2  ]: 11 of 32 OK created sql view model DEV_pipeline_b.stg_trades ................... [[32mSUCCESS 1[0m in 0.33s]
[0m20:23:49.135789 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.int_benchmark_aligned
[0m20:23:49.136354 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.stg_trades
[0m20:23:49.139370 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_benchmark_aligned"
[0m20:23:49.140051 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.int_portfolio_returns_daily
[0m20:23:49.142370 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_position_attribution"
[0m20:23:49.142801 [info ] [Thread-2  ]: 16 of 32 START sql view model DEV_pipeline_c.int_portfolio_returns_daily ....... [RUN]
[0m20:23:49.144352 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_benchmark_aligned"
[0m20:23:49.144787 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.int_position_attribution: create or replace   view DBT_DEMO.DEV_pipeline_c.int_position_attribution
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: int_position_attribution
-- Description: Attribution analysis at position level
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Heavy multi-way join
-- 2. Complex attribution calculations
-- 3. Multiple window functions

with positions as (
    select * from DBT_DEMO.DEV_pipeline_c.stg_positions_daily
),

securities as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_securities
),

market_prices as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_market_prices
),

-- ISSUE: Heavy 3-way join
enriched_positions as (
    select
        p.portfolio_id,
        p.security_id,
        p.position_date,
        p.quantity,
        p.market_value_usd,
        p.weight_pct,
        s.ticker,
        s.security_type_standardized as security_type,
        s.asset_class,
        s.sector,
        s.industry,
        mp.daily_return as security_return,
        mp.close_price,
        mp.ma_20,
        mp.ma_50,
        mp.volatility_20d
    from positions p
    inner join securities s
        on p.security_id = s.security_id
    left join market_prices mp
        on p.security_id = mp.security_id
        and p.position_date = mp.price_date
),

-- ISSUE: Window functions for prior day weight
with_prior_weight as (
    select
        *,
        lag(weight_pct, 1) over (
            partition by portfolio_id, security_id
            order by position_date
        ) as prior_weight_pct,
        lag(market_value_usd, 1) over (
            partition by portfolio_id, security_id
            order by position_date
        ) as prior_market_value
    from enriched_positions
),

-- ISSUE: Attribution calculations
with_attribution as (
    select
        *,
        -- Contribution to return
        coalesce(prior_weight_pct, weight_pct) * coalesce(security_return, 0) as contribution_to_return,
        -- Allocation effect (simplified Brinson)
        (weight_pct - coalesce(prior_weight_pct, weight_pct)) * coalesce(security_return, 0) as allocation_effect,
        -- Position P&L
        market_value_usd - coalesce(prior_market_value, market_value_usd) as position_pnl
    from with_prior_weight
)

select * from with_attribution
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_position_attribution"} */;
[0m20:23:49.145272 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_trades, now model.bain_capital_portfolio_analytics.int_portfolio_returns_daily)
[0m20:23:49.145654 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.int_benchmark_aligned: create or replace   view DBT_DEMO.DEV_pipeline_c.int_benchmark_aligned
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: int_benchmark_aligned
-- Description: Align benchmark returns with portfolio dates for comparison
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Heavy join between portfolios and benchmarks
-- 2. Could pre-filter benchmarks

with portfolio_dates as (
    select distinct
        portfolio_id,
        valuation_date
    from DBT_DEMO.DEV_pipeline_c.stg_valuations
),

portfolio_benchmarks as (
    select * from DBT_DEMO.DEV_pipeline_c.stg_portfolio_benchmarks
),

benchmark_returns as (
    select * from DBT_DEMO.DEV_pipeline_c.stg_benchmark_returns
),

-- ISSUE: Cross-join like pattern (portfolio dates x benchmarks)
aligned as (
    select
        pd.portfolio_id,
        pd.valuation_date,
        pb.benchmark_id,
        1.0 as benchmark_weight,  -- Default weight since not in source
        pb.is_primary,
        br.daily_return as benchmark_daily_return,
        br.cumulative_return as benchmark_cumulative_return,
        br.return_30d as benchmark_return_30d,
        br.return_90d as benchmark_return_90d,
        br.return_1y as benchmark_return_1y,
        br.annualized_volatility as benchmark_volatility
    from portfolio_dates pd
    inner join portfolio_benchmarks pb
        on pd.portfolio_id = pb.portfolio_id
        and pd.valuation_date >= pb.start_date
        and (pb.end_date is null or pd.valuation_date <= pb.end_date)
    left join benchmark_returns br
        on pb.benchmark_id = br.benchmark_id
        and pd.valuation_date = br.return_date
)

select * from aligned
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_benchmark_aligned"} */;
[0m20:23:49.146140 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.int_portfolio_returns_daily
[0m20:23:49.150367 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_portfolio_returns_daily"
[0m20:23:49.151699 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.int_portfolio_returns_daily
[0m20:23:49.155134 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_portfolio_returns_daily"
[0m20:23:49.159502 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_portfolio_returns_daily"
[0m20:23:49.160121 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.int_portfolio_returns_daily: create or replace   view DBT_DEMO.DEV_pipeline_c.int_portfolio_returns_daily
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: int_portfolio_returns_daily
-- Description: Calculate daily portfolio returns from NAV
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-join for prior day NAV (should use LAG)
-- 2. Multiple passes for return calculations
-- 3. Complex window functions

with valuations as (
    select * from DBT_DEMO.DEV_pipeline_c.stg_valuations
),

cashflows as (
    select
        portfolio_id,
        cashflow_date,
        sum(case when cashflow_type = 'CONTRIBUTION' then amount else 0 end) as contributions,
        sum(case when cashflow_type = 'DISTRIBUTION' then amount else 0 end) as distributions
    from DBT_DEMO.DEV_pipeline_a.stg_cashflows
    group by 1, 2
),

-- ISSUE: Self-join instead of LAG for prior NAV
with_prior_nav as (
    select
        curr.portfolio_id,
        curr.valuation_date,
        curr.nav,
        curr.nav_usd,
        prev.nav as prior_nav,
        prev.nav_usd as prior_nav_usd,
        coalesce(cf.contributions, 0) as contributions,
        coalesce(cf.distributions, 0) as distributions
    from valuations curr
    left join valuations prev
        on curr.portfolio_id = prev.portfolio_id
        and curr.valuation_date = dateadd('day', 1, prev.valuation_date)
    left join cashflows cf
        on curr.portfolio_id = cf.portfolio_id
        and curr.valuation_date = cf.cashflow_date
),

-- ISSUE: Modified Dietz calculation done inefficiently
with_daily_return as (
    select
        portfolio_id,
        valuation_date,
        nav,
        nav_usd,
        prior_nav,
        prior_nav_usd,
        contributions,
        distributions,
        -- Simple return
        case
            when prior_nav > 0
            then (nav - prior_nav - contributions + distributions) / prior_nav
            else null
        end as daily_return_simple,
        -- Modified Dietz (approximation)
        case
            when (prior_nav + contributions * 0.5) > 0
            then (nav - prior_nav - contributions + distributions) / (prior_nav + contributions * 0.5 - distributions * 0.5)
            else null
        end as daily_return_mod_dietz
    from with_prior_nav
),

-- ISSUE: Multiple window functions for different periods
with_rolling_returns as (
    select
        *,
        -- Cumulative return using log returns
        exp(sum(ln(1 + coalesce(daily_return_mod_dietz, 0))) over (
            partition by portfolio_id
            order by valuation_date
            rows between unbounded preceding and current row
        )) - 1 as cumulative_return,
        -- Rolling period returns
        exp(sum(ln(1 + coalesce(daily_return_mod_dietz, 0))) over (
            partition by portfolio_id
            order by valuation_date
            rows between 6 preceding and current row
        )) - 1 as return_1w,
        exp(sum(ln(1 + coalesce(daily_return_mod_dietz, 0))) over (
            partition by portfolio_id
            order by valuation_date
            rows between 29 preceding and current row
        )) - 1 as return_1m,
        exp(sum(ln(1 + coalesce(daily_return_mod_dietz, 0))) over (
            partition by portfolio_id
            order by valuation_date
            rows between 89 preceding and current row
        )) - 1 as return_3m,
        exp(sum(ln(1 + coalesce(daily_return_mod_dietz, 0))) over (
            partition by portfolio_id
            order by valuation_date
            rows between 179 preceding and current row
        )) - 1 as return_6m,
        exp(sum(ln(1 + coalesce(daily_return_mod_dietz, 0))) over (
            partition by portfolio_id
            order by valuation_date
            rows between 364 preceding and current row
        )) - 1 as return_1y,
        -- Rolling volatility
        stddev(daily_return_mod_dietz) over (
            partition by portfolio_id
            order by valuation_date
            rows between 29 preceding and current row
        ) * sqrt(252) as volatility_1m,
        stddev(daily_return_mod_dietz) over (
            partition by portfolio_id
            order by valuation_date
            rows between 251 preceding and current row
        ) * sqrt(252) as volatility_1y
    from with_daily_return
)

select * from with_rolling_returns
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_portfolio_returns_daily"} */;
[0m20:23:49.540507 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.379 seconds
[0m20:23:49.545406 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109db4eb0>]}
[0m20:23:49.546460 [info ] [Thread-2  ]: 16 of 32 OK created sql view model DEV_pipeline_c.int_portfolio_returns_daily .. [[32mSUCCESS 1[0m in 0.40s]
[0m20:23:49.547256 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.int_portfolio_returns_daily
[0m20:23:49.547937 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.int_trades_enriched
[0m20:23:49.548703 [info ] [Thread-2  ]: 17 of 32 START sql view model DEV_pipeline_b.int_trades_enriched ............... [RUN]
[0m20:23:49.549467 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_portfolio_returns_daily, now model.bain_capital_portfolio_analytics.int_trades_enriched)
[0m20:23:49.549934 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.int_trades_enriched
[0m20:23:49.557160 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_trades_enriched"
[0m20:23:49.558361 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.int_trades_enriched
[0m20:23:49.561372 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_trades_enriched"
[0m20:23:49.563927 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_trades_enriched"
[0m20:23:49.564353 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.int_trades_enriched: create or replace   view DBT_DEMO.DEV_pipeline_b.int_trades_enriched
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: int_trades_enriched
-- Description: Intermediate model enriching trades with security and price data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Multiple heavy joins done row-by-row
-- 2. Price lookup repeated for every trade
-- 3. Could pre-aggregate before joining

with trades as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_trades
),

securities as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_securities
),

market_prices as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_market_prices
),

brokers as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_brokers
),

-- ISSUE: Heavy multi-way join before any aggregation
enriched as (
    select
        t.trade_id,
        t.portfolio_id,
        t.security_id,
        t.trade_date,
        t.settlement_date,
        t.trade_type,
        t.trade_category,
        t.trade_size_bucket,
        t.quantity,
        t.price as execution_price,
        t.gross_amount,
        t.commission,
        t.fees,
        t.net_amount,
        t.currency,
        t.settlement_days,
        t.trade_month,
        t.trade_quarter,
        t.trade_year,
        -- Security attributes
        s.ticker,
        s.security_name,
        s.security_type_standardized as security_type,
        s.asset_class,
        s.sector,
        s.industry,
        -- Broker attributes
        b.broker_name,
        b.broker_type,
        b.region as broker_region,
        b.commission_rate as standard_commission_rate,
        -- Market price on trade date
        mp.close_price as market_close_price,
        mp.ma_20,
        mp.ma_50,
        mp.volatility_20d,
        mp.trend_signal,
        mp.volume_signal,
        -- ISSUE: These calculations done per row
        case
            when mp.close_price > 0
            then (t.price - mp.close_price) / mp.close_price * 100
            else null
        end as execution_vs_close_pct,
        case
            when t.price > mp.close_price then 'ABOVE_MARKET'
            when t.price < mp.close_price then 'BELOW_MARKET'
            else 'AT_MARKET'
        end as execution_quality,
        -- Cost analysis
        t.commission + t.fees as total_costs,
        case
            when t.gross_amount > 0
            then (t.commission + t.fees) / t.gross_amount * 10000
            else null
        end as cost_bps
    from trades t
    inner join securities s
        on t.security_id = s.security_id
    left join brokers b
        on t.broker_id = b.broker_id
    left join market_prices mp
        on t.security_id = mp.security_id
        and t.trade_date = mp.price_date
)

select * from enriched
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_trades_enriched"} */;
[0m20:23:49.597353 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 0.451 seconds
[0m20:23:49.599551 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11370beb0>]}
[0m20:23:49.600131 [info ] [Thread-3  ]: 14 of 32 OK created sql view model DEV_pipeline_c.int_position_attribution ..... [[32mSUCCESS 1[0m in 0.49s]
[0m20:23:49.600617 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.int_position_attribution
[0m20:23:49.600956 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.int_risk_metrics
[0m20:23:49.601341 [info ] [Thread-3  ]: 18 of 32 START sql view model DEV_pipeline_c.int_risk_metrics .................. [RUN]
[0m20:23:49.601852 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_position_attribution, now model.bain_capital_portfolio_analytics.int_risk_metrics)
[0m20:23:49.602197 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.int_risk_metrics
[0m20:23:49.605766 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_risk_metrics"
[0m20:23:49.606615 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.int_risk_metrics
[0m20:23:49.609703 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_risk_metrics"
[0m20:23:49.612366 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_risk_metrics"
[0m20:23:49.612821 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.int_risk_metrics: create or replace   view DBT_DEMO.DEV_pipeline_c.int_risk_metrics
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: int_risk_metrics
-- Description: Calculate risk metrics for portfolios
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Multiple window functions with same partition
-- 2. VaR calculation inefficiencies
-- 3. Could pre-compute some metrics

with portfolio_returns as (
    select * from DBT_DEMO.DEV_pipeline_c.int_portfolio_returns_daily
),

-- ISSUE: Multiple passes for different risk calculations
with_risk_metrics as (
    select
        portfolio_id,
        valuation_date,
        daily_return_mod_dietz as daily_return,
        nav_usd,
        -- ISSUE: Repeated window frame definitions
        -- Max drawdown components
        max(nav_usd) over (
            partition by portfolio_id
            order by valuation_date
            rows between unbounded preceding and current row
        ) as running_max_nav,
        -- Downside deviation
        sqrt(avg(
            case when daily_return_mod_dietz < 0 then power(daily_return_mod_dietz, 2) else 0 end
        ) over (
            partition by portfolio_id
            order by valuation_date
            rows between 251 preceding and current row
        )) * sqrt(252) as downside_deviation_1y,
        -- Sortino components
        avg(daily_return_mod_dietz) over (
            partition by portfolio_id
            order by valuation_date
            rows between 251 preceding and current row
        ) * 252 as annualized_return_1y,
        volatility_1y
    from portfolio_returns
),

-- ISSUE: Another CTE for derived metrics
with_derived as (
    select
        *,
        -- Drawdown
        (nav_usd - running_max_nav) / nullif(running_max_nav, 0) as drawdown,
        -- Sortino ratio (assuming 0% risk-free)
        case
            when downside_deviation_1y > 0
            then annualized_return_1y / downside_deviation_1y
            else null
        end as sortino_ratio,
        -- Sharpe ratio (assuming 0% risk-free)
        case
            when volatility_1y > 0
            then annualized_return_1y / volatility_1y
            else null
        end as sharpe_ratio
    from with_risk_metrics
),

-- ISSUE: Max drawdown calculation
with_max_drawdown as (
    select
        *,
        min(drawdown) over (
            partition by portfolio_id
            order by valuation_date
            rows between unbounded preceding and current row
        ) as max_drawdown
    from with_derived
),

-- ISSUE: VaR calculation (simplified parametric)
final as (
    select
        *,
        -- Parametric VaR (95%)
        nav_usd * volatility_1y / sqrt(252) * 1.645 as var_95_1d,
        -- Parametric VaR (99%)
        nav_usd * volatility_1y / sqrt(252) * 2.326 as var_99_1d
    from with_max_drawdown
)

select * from final
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_risk_metrics"} */;
[0m20:23:49.659803 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 0.513 seconds
[0m20:23:49.661987 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113705430>]}
[0m20:23:49.662621 [info ] [Thread-4  ]: 15 of 32 OK created sql view model DEV_pipeline_c.int_benchmark_aligned ........ [[32mSUCCESS 1[0m in 0.54s]
[0m20:23:49.663182 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.int_benchmark_aligned
[0m20:23:49.663575 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.fact_position_snapshot
[0m20:23:49.664032 [info ] [Thread-4  ]: 19 of 32 START sql table model DEV_pipeline_c.fact_position_snapshot ........... [RUN]
[0m20:23:49.664545 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_benchmark_aligned, now model.bain_capital_portfolio_analytics.fact_position_snapshot)
[0m20:23:49.664964 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_position_snapshot
[0m20:23:49.670241 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_position_snapshot"
[0m20:23:49.671069 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.fact_position_snapshot
[0m20:23:49.674400 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_position_snapshot"
[0m20:23:49.676393 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_position_snapshot"
[0m20:23:49.676778 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.fact_position_snapshot: create or replace transient table DBT_DEMO.DEV_pipeline_c.fact_position_snapshot
    
    
    
    as (-- Pipeline C: Complex Portfolio Analytics
-- Model: fact_position_snapshot
-- Description: Position-level fact table with attribution
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Heavy joins that duplicate upstream work
-- 2. Could be more selective in columns

with position_attribution as (
    select * from DBT_DEMO.DEV_pipeline_c.int_position_attribution
),

portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

-- ISSUE: Re-joining portfolio data
final as (
    select
        md5(cast(coalesce(cast(pa.portfolio_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(pa.security_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(pa.position_date as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as position_snapshot_key,
        p.portfolio_name,
        p.portfolio_type,
        p.fund_id,
        pa.portfolio_id,
        pa.security_id,
        pa.position_date,
        pa.ticker,
        pa.security_type,
        pa.asset_class,
        pa.sector,
        pa.industry,
        pa.quantity,
        pa.market_value_usd,
        pa.weight_pct,
        pa.close_price,
        pa.ma_20,
        pa.ma_50,
        pa.volatility_20d,
        pa.security_return,
        pa.contribution_to_return,
        pa.allocation_effect,
        pa.position_pnl,
        -- ISSUE: More date extractions
        extract(year from pa.position_date) as position_year,
        extract(month from pa.position_date) as position_month,
        date_trunc('month', pa.position_date) as position_month_start
    from position_attribution pa
    inner join portfolios p
        on pa.portfolio_id = p.portfolio_id
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_position_snapshot"} */;
[0m20:23:50.007714 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 0.394 seconds
[0m20:23:50.011485 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11370beb0>]}
[0m20:23:50.012615 [info ] [Thread-3  ]: 18 of 32 OK created sql view model DEV_pipeline_c.int_risk_metrics ............. [[32mSUCCESS 1[0m in 0.41s]
[0m20:23:50.013395 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.int_risk_metrics
[0m20:23:50.013925 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.int_sector_attribution
[0m20:23:50.014699 [info ] [Thread-3  ]: 20 of 32 START sql view model DEV_pipeline_c.int_sector_attribution ............ [RUN]
[0m20:23:50.015460 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_risk_metrics, now model.bain_capital_portfolio_analytics.int_sector_attribution)
[0m20:23:50.015892 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.int_sector_attribution
[0m20:23:50.021636 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_sector_attribution"
[0m20:23:50.022573 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.int_sector_attribution
[0m20:23:50.026934 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_sector_attribution"
[0m20:23:50.029293 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_sector_attribution"
[0m20:23:50.029745 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.int_sector_attribution: create or replace   view DBT_DEMO.DEV_pipeline_c.int_sector_attribution
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: int_sector_attribution
-- Description: Aggregate attribution to sector level
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Re-aggregation of position data
-- 2. Complex grouping logic
-- 3. Could be combined with position attribution

with position_attribution as (
    select * from DBT_DEMO.DEV_pipeline_c.int_position_attribution
),

-- ISSUE: Aggregation that could be pushed upstream
sector_daily as (
    select
        portfolio_id,
        position_date,
        sector,
        count(distinct security_id) as position_count,
        sum(market_value_usd) as sector_market_value,
        sum(weight_pct) as sector_weight,
        sum(contribution_to_return) as sector_contribution,
        sum(allocation_effect) as sector_allocation_effect,
        sum(position_pnl) as sector_pnl,
        avg(security_return) as avg_security_return
    from position_attribution
    group by 1, 2, 3
),

-- ISSUE: Window functions for rolling metrics
with_rolling as (
    select
        *,
        sum(sector_contribution) over (
            partition by portfolio_id, sector
            order by position_date
            rows between 29 preceding and current row
        ) as sector_contribution_30d,
        avg(sector_weight) over (
            partition by portfolio_id, sector
            order by position_date
            rows between 29 preceding and current row
        ) as avg_sector_weight_30d,
        lag(sector_weight, 1) over (
            partition by portfolio_id, sector
            order by position_date
        ) as prior_sector_weight
    from sector_daily
)

select * from with_rolling
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_sector_attribution"} */;
[0m20:23:50.181081 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.616 seconds
[0m20:23:50.186379 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1133be9a0>]}
[0m20:23:50.187029 [info ] [Thread-2  ]: 17 of 32 OK created sql view model DEV_pipeline_b.int_trades_enriched .......... [[32mSUCCESS 1[0m in 0.64s]
[0m20:23:50.187534 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.int_trades_enriched
[0m20:23:50.187889 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark
[0m20:23:50.188383 [info ] [Thread-2  ]: 21 of 32 START sql view model DEV_pipeline_c.int_portfolio_vs_benchmark ........ [RUN]
[0m20:23:50.188815 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_trades_enriched, now model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark)
[0m20:23:50.189211 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark
[0m20:23:50.193214 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark"
[0m20:23:50.193891 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark
[0m20:23:50.198310 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark"
[0m20:23:50.201199 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark"
[0m20:23:50.201674 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark: create or replace   view DBT_DEMO.DEV_pipeline_c.int_portfolio_vs_benchmark
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: int_portfolio_vs_benchmark
-- Description: Compare portfolio returns to benchmark
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Re-joins data that could be joined once upstream
-- 2. Excess return calculation repeated
-- 3. Complex rolling calculations

with portfolio_returns as (
    select * from DBT_DEMO.DEV_pipeline_c.int_portfolio_returns_daily
),

benchmark_aligned as (
    select * from DBT_DEMO.DEV_pipeline_c.int_benchmark_aligned
    where is_primary = true
),

-- ISSUE: Another join that combines already-processed data
combined as (
    select
        pr.portfolio_id,
        pr.valuation_date,
        pr.nav,
        pr.nav_usd,
        pr.daily_return_mod_dietz as portfolio_daily_return,
        pr.cumulative_return as portfolio_cumulative_return,
        pr.return_1m as portfolio_return_1m,
        pr.return_3m as portfolio_return_3m,
        pr.return_1y as portfolio_return_1y,
        pr.volatility_1y as portfolio_volatility,
        ba.benchmark_id,
        ba.benchmark_daily_return,
        ba.benchmark_cumulative_return,
        ba.benchmark_return_30d as benchmark_return_1m,
        ba.benchmark_return_90d as benchmark_return_3m,
        ba.benchmark_return_1y,
        ba.benchmark_volatility
    from portfolio_returns pr
    left join benchmark_aligned ba
        on pr.portfolio_id = ba.portfolio_id
        and pr.valuation_date = ba.valuation_date
),

-- ISSUE: Excess return calculations
with_excess as (
    select
        *,
        portfolio_daily_return - coalesce(benchmark_daily_return, 0) as daily_excess_return,
        portfolio_cumulative_return - coalesce(benchmark_cumulative_return, 0) as cumulative_excess_return,
        portfolio_return_1m - coalesce(benchmark_return_1m, 0) as excess_return_1m,
        portfolio_return_3m - coalesce(benchmark_return_3m, 0) as excess_return_3m,
        portfolio_return_1y - coalesce(benchmark_return_1y, 0) as excess_return_1y
    from combined
),

-- ISSUE: Rolling tracking error calculation
with_tracking_error as (
    select
        *,
        stddev(daily_excess_return) over (
            partition by portfolio_id
            order by valuation_date
            rows between 251 preceding and current row
        ) * sqrt(252) as tracking_error_1y,
        avg(daily_excess_return) over (
            partition by portfolio_id
            order by valuation_date
            rows between 251 preceding and current row
        ) * 252 as annualized_alpha
    from with_excess
),

-- ISSUE: Information ratio
final as (
    select
        *,
        case
            when tracking_error_1y > 0
            then annualized_alpha / tracking_error_1y
            else null
        end as information_ratio
    from with_tracking_error
)

select * from final
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark"} */;
[0m20:23:50.777398 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 0.747 seconds
[0m20:23:50.781880 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1133d4c10>]}
[0m20:23:50.783040 [info ] [Thread-3  ]: 20 of 32 OK created sql view model DEV_pipeline_c.int_sector_attribution ....... [[32mSUCCESS 1[0m in 0.77s]
[0m20:23:50.783793 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.int_sector_attribution
[0m20:23:50.784336 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.int_fund_rollup
[0m20:23:50.785116 [info ] [Thread-3  ]: 22 of 32 START sql view model DEV_pipeline_c.int_fund_rollup ................... [RUN]
[0m20:23:50.785907 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_sector_attribution, now model.bain_capital_portfolio_analytics.int_fund_rollup)
[0m20:23:50.786391 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.int_fund_rollup
[0m20:23:50.793262 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_fund_rollup"
[0m20:23:50.794125 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.int_fund_rollup
[0m20:23:50.798158 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_fund_rollup"
[0m20:23:50.801284 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_fund_rollup"
[0m20:23:50.801766 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.int_fund_rollup: create or replace   view DBT_DEMO.DEV_pipeline_c.int_fund_rollup
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: int_fund_rollup
-- Description: Roll up portfolio metrics to fund level
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Recursive-like hierarchy traversal
-- 2. Multiple aggregation levels
-- 3. Heavy joins

with fund_hierarchy as (
    select * from DBT_DEMO.DEV_pipeline_c.stg_fund_hierarchy
),

portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

risk_metrics as (
    select * from DBT_DEMO.DEV_pipeline_c.int_risk_metrics
),

-- Get portfolio to fund mapping
portfolio_fund_map as (
    select
        p.portfolio_id,
        p.portfolio_name,
        p.fund_id,
        fh.entity_name as fund_name,
        fh.parent_entity_id,
        fh.hierarchy_level
    from portfolios p
    left join fund_hierarchy fh
        on p.fund_id = fh.entity_id
),

-- Get latest risk metrics per portfolio
latest_metrics as (
    select *
    from (
        select
            *,
            row_number() over (partition by portfolio_id order by valuation_date desc) as rn
        from risk_metrics
    )
    where rn = 1  -- ISSUE: Should use QUALIFY
),

-- ISSUE: Join and aggregate
fund_aggregated as (
    select
        pfm.fund_id,
        pfm.fund_name,
        pfm.parent_entity_id,
        pfm.hierarchy_level,
        lm.valuation_date,
        count(distinct pfm.portfolio_id) as portfolio_count,
        sum(lm.nav_usd) as total_nav_usd,
        -- Weighted average metrics
        sum(lm.nav_usd * lm.annualized_return_1y) / nullif(sum(lm.nav_usd), 0) as weighted_return_1y,
        sum(lm.nav_usd * lm.volatility_1y) / nullif(sum(lm.nav_usd), 0) as weighted_volatility_1y,
        sum(lm.nav_usd * lm.sharpe_ratio) / nullif(sum(lm.nav_usd), 0) as weighted_sharpe_ratio,
        min(lm.max_drawdown) as worst_drawdown,
        sum(lm.var_95_1d) as total_var_95
    from portfolio_fund_map pfm
    inner join latest_metrics lm
        on pfm.portfolio_id = lm.portfolio_id
    group by 1, 2, 3, 4, 5
)

select * from fund_aggregated
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_fund_rollup"} */;
[0m20:23:50.820625 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.618 seconds
[0m20:23:50.823027 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113676f40>]}
[0m20:23:50.823704 [info ] [Thread-2  ]: 21 of 32 OK created sql view model DEV_pipeline_c.int_portfolio_vs_benchmark ... [[32mSUCCESS 1[0m in 0.63s]
[0m20:23:50.824231 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark
[0m20:23:50.824576 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.int_trade_pnl
[0m20:23:50.825039 [info ] [Thread-2  ]: 23 of 32 START sql view model DEV_pipeline_b.int_trade_pnl ..................... [RUN]
[0m20:23:50.825435 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark, now model.bain_capital_portfolio_analytics.int_trade_pnl)
[0m20:23:50.825773 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.int_trade_pnl
[0m20:23:50.829575 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_trade_pnl"
[0m20:23:50.830230 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.int_trade_pnl
[0m20:23:50.833321 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_trade_pnl"
[0m20:23:50.836242 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_trade_pnl"
[0m20:23:50.836666 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.int_trade_pnl: create or replace   view DBT_DEMO.DEV_pipeline_b.int_trade_pnl
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: int_trade_pnl
-- Description: Calculate P&L for each trade
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Complex position tracking logic that could be simplified
-- 2. Multiple self-joins for cost basis calculation
-- 3. Window functions recalculated multiple times

with trades as (
    select * from DBT_DEMO.DEV_pipeline_b.int_trades_enriched
),

-- ISSUE: Running position calculation done inefficiently
positions as (
    select
        trade_id,
        portfolio_id,
        security_id,
        ticker,
        security_name,
        security_type,
        asset_class,
        sector,
        industry,
        trade_date,
        trade_type,
        trade_category,
        quantity,
        execution_price,
        net_amount,
        commission,
        -- ISSUE: Multiple window functions with same partition
        sum(case
            when trade_category = 'PURCHASE' then quantity
            when trade_category = 'SALE' then -quantity
            else 0
        end) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as running_position,
        sum(case
            when trade_category = 'PURCHASE' then net_amount
            when trade_category = 'SALE' then -net_amount
            else 0
        end) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as cumulative_cost,
        -- ISSUE: Another separate window for purchase-only
        sum(case when trade_category = 'PURCHASE' then quantity else 0 end) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as cumulative_purchased_qty,
        sum(case when trade_category = 'PURCHASE' then net_amount else 0 end) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as cumulative_purchase_cost
    from trades
),

-- ISSUE: Separate CTE for cost basis
with_cost_basis as (
    select
        *,
        case
            when cumulative_purchased_qty > 0
            then cumulative_purchase_cost / cumulative_purchased_qty
            else null
        end as avg_cost_basis
    from positions
),

-- ISSUE: Another pass for realized P&L
with_pnl as (
    select
        *,
        case
            when trade_category = 'SALE' and avg_cost_basis is not null
            then (execution_price - avg_cost_basis) * quantity
            else null
        end as realized_pnl,
        case
            when trade_category = 'SALE' and avg_cost_basis is not null and avg_cost_basis > 0
            then (execution_price - avg_cost_basis) / avg_cost_basis * 100
            else null
        end as realized_pnl_pct
    from with_cost_basis
)

select * from with_pnl
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_trade_pnl"} */;
[0m20:23:51.334011 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.497 seconds
[0m20:23:51.338532 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113837b50>]}
[0m20:23:51.339707 [info ] [Thread-2  ]: 23 of 32 OK created sql view model DEV_pipeline_b.int_trade_pnl ................ [[32mSUCCESS 1[0m in 0.51s]
[0m20:23:51.340354 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.int_trade_pnl
[0m20:23:51.340834 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.fact_sector_performance
[0m20:23:51.341545 [info ] [Thread-2  ]: 24 of 32 START sql table model DEV_pipeline_c.fact_sector_performance .......... [RUN]
[0m20:23:51.342296 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_trade_pnl, now model.bain_capital_portfolio_analytics.fact_sector_performance)
[0m20:23:51.342747 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_sector_performance
[0m20:23:51.351451 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_sector_performance"
[0m20:23:51.352723 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.fact_sector_performance
[0m20:23:51.356479 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_sector_performance"
[0m20:23:51.358401 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_sector_performance"
[0m20:23:51.358780 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.fact_sector_performance: create or replace transient table DBT_DEMO.DEV_pipeline_c.fact_sector_performance
    
    
    
    as (-- Pipeline C: Complex Portfolio Analytics
-- Model: fact_sector_performance
-- Description: Sector-level performance aggregation
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Re-aggregates data from upstream
-- 2. Complex window functions

with sector_attribution as (
    select * from DBT_DEMO.DEV_pipeline_c.int_sector_attribution
),

portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

-- ISSUE: Another portfolio join
with_portfolio_info as (
    select
        sa.*,
        p.portfolio_name,
        p.portfolio_type,
        p.fund_id
    from sector_attribution sa
    inner join portfolios p
        on sa.portfolio_id = p.portfolio_id
),

-- ISSUE: More window functions for sector ranking
with_rankings as (
    select
        *,
        rank() over (
            partition by portfolio_id, position_date
            order by sector_weight desc
        ) as sector_weight_rank,
        rank() over (
            partition by portfolio_id, position_date
            order by sector_contribution desc
        ) as sector_contribution_rank
    from with_portfolio_info
),

final as (
    select
        md5(cast(coalesce(cast(portfolio_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(sector as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(position_date as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as sector_performance_key,
        *,
        case
            when sector_weight_rank <= 3 then 'TOP_3'
            when sector_weight_rank <= 5 then 'TOP_5'
            else 'OTHER'
        end as sector_weight_tier
    from with_rankings
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_sector_performance"} */;
[0m20:23:51.642567 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 0.840 seconds
[0m20:23:51.648619 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1133d4c10>]}
[0m20:23:52.087280 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 2.410 seconds
[0m20:23:52.091453 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1133fd940>]}
[0m20:23:52.116643 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 2.995 seconds
[0m20:23:52.119767 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d1cc7f0>]}
[0m20:23:52.737931 [debug] [Thread-3  ]: An error was encountered while trying to send an event
[0m20:23:52.740185 [info ] [Thread-4  ]: 19 of 32 OK created sql table model DEV_pipeline_c.fact_position_snapshot ...... [[32mSUCCESS 1[0m in 2.43s]
[0m20:23:52.741268 [info ] [Thread-1  ]: 13 of 32 OK created sql table model DEV_pipeline_a.fact_cashflow_summary ....... [[32mSUCCESS 1[0m in 3.06s]
[0m20:23:52.742191 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.fact_position_snapshot
[0m20:23:52.744412 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m20:23:52.743938 [info ] [Thread-3  ]: 22 of 32 OK created sql view model DEV_pipeline_c.int_fund_rollup .............. [[32mSUCCESS 1[0m in 0.86s]
[0m20:23:52.745010 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.fact_trade_summary
[0m20:23:52.745726 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.int_fund_rollup
[0m20:23:52.746282 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.report_monthly_cashflows
[0m20:23:52.746979 [info ] [Thread-4  ]: 25 of 32 START sql table model DEV_pipeline_b.fact_trade_summary ............... [RUN]
[0m20:23:52.747802 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.fact_portfolio_positions
[0m20:23:52.748680 [info ] [Thread-1  ]: 26 of 32 START sql table model DEV_pipeline_a.report_monthly_cashflows ......... [RUN]
[0m20:23:52.749531 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.fact_position_snapshot, now model.bain_capital_portfolio_analytics.fact_trade_summary)
[0m20:23:52.750193 [info ] [Thread-3  ]: 27 of 32 START sql table model DEV_pipeline_b.fact_portfolio_positions ......... [RUN]
[0m20:23:52.750663 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.fact_cashflow_summary, now model.bain_capital_portfolio_analytics.report_monthly_cashflows)
[0m20:23:52.751056 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_trade_summary
[0m20:23:52.752032 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_fund_rollup, now model.bain_capital_portfolio_analytics.fact_portfolio_positions)
[0m20:23:52.752448 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.report_monthly_cashflows
[0m20:23:52.759006 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_trade_summary"
[0m20:23:52.759405 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_portfolio_positions
[0m20:23:52.762674 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.report_monthly_cashflows"
[0m20:23:52.768021 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_portfolio_positions"
[0m20:23:52.768911 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.report_monthly_cashflows
[0m20:23:52.772939 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.report_monthly_cashflows"
[0m20:23:52.773289 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.fact_trade_summary
[0m20:23:52.773620 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.fact_portfolio_positions
[0m20:23:52.776403 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_trade_summary"
[0m20:23:52.779249 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_portfolio_positions"
[0m20:23:52.782282 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.report_monthly_cashflows"
[0m20:23:52.782735 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.report_monthly_cashflows: create or replace transient table DBT_DEMO.DEV_pipeline_a.report_monthly_cashflows
    
    
    
    as (-- Pipeline A: Simple Cashflow Pipeline
-- Model: report_monthly_cashflows
-- Description: LP reporting view for monthly cashflow analysis
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Re-aggregates data that's already in fact table
-- 2. Repeated window functions
-- 3. Suboptimal pivot pattern

with fact_data as (
    select * from DBT_DEMO.DEV_pipeline_a.fact_cashflow_summary
),

-- ISSUE: Re-aggregating already aggregated data
monthly_totals as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        cashflow_month,
        year_num,
        month_num,
        sum(case when cashflow_type = 'CONTRIBUTION' then total_amount else 0 end) as contributions,
        sum(case when cashflow_type = 'DISTRIBUTION' then total_amount else 0 end) as distributions,
        sum(case when cashflow_type = 'DIVIDEND' then total_amount else 0 end) as dividends,
        sum(case when cashflow_type = 'FEE' then total_amount else 0 end) as fees,
        sum(total_amount) as total_cashflow,
        sum(transaction_count) as total_transactions
    from fact_data
    group by 1,2,3,4,5,6,7
),

-- ISSUE: Window functions recalculated multiple times
with_running_totals as (
    select
        *,
        -- Running totals (repeated pattern)
        sum(contributions) over (
            partition by portfolio_id
            order by cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_contributions,
        sum(distributions) over (
            partition by portfolio_id
            order by cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_distributions,
        sum(total_cashflow) over (
            partition by portfolio_id
            order by cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_net_cashflow,
        -- Prior period comparisons (another repeated pattern)
        lag(contributions, 1) over (partition by portfolio_id order by cashflow_month) as prior_month_contributions,
        lag(distributions, 1) over (partition by portfolio_id order by cashflow_month) as prior_month_distributions,
        lag(total_cashflow, 1) over (partition by portfolio_id order by cashflow_month) as prior_month_total,
        -- YoY comparison
        lag(contributions, 12) over (partition by portfolio_id order by cashflow_month) as prior_year_contributions,
        lag(distributions, 12) over (partition by portfolio_id order by cashflow_month) as prior_year_distributions
    from monthly_totals
),

-- ISSUE: Calculated columns that could be simplified
final as (
    select
        *,
        contributions - coalesce(prior_month_contributions, 0) as mom_contribution_change,
        distributions - coalesce(prior_month_distributions, 0) as mom_distribution_change,
        case
            when prior_year_contributions > 0
            then (contributions - prior_year_contributions) / prior_year_contributions * 100
            else null
        end as yoy_contribution_pct_change,
        contributions - distributions as net_inflow
    from with_running_totals
)

select * from final
order by portfolio_id, cashflow_month
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.report_monthly_cashflows"} */;
[0m20:23:52.797203 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_portfolio_positions"
[0m20:23:52.798614 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_trade_summary"
[0m20:23:52.799371 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.fact_portfolio_positions: create or replace transient table DBT_DEMO.DEV_pipeline_b.fact_portfolio_positions
    
    
    
    as (-- Pipeline B: Trade Analytics Pipeline
-- Model: fact_portfolio_positions
-- Description: Current position snapshot by portfolio and security
-- DEPENDENCY: Uses fact_cashflow_summary from Pipeline A for portfolio cash context
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Gets latest position via subquery (should use QUALIFY)
-- 2. Self-joins for historical position lookups
-- 3. Correlated subqueries for portfolio-level aggregations
-- 4. Repeated window functions

with trade_pnl as (
    select * from DBT_DEMO.DEV_pipeline_b.int_trade_pnl
),

-- DEPENDENCY ON PIPELINE A: Get cashflow context for each portfolio
cashflow_summary as (
    select * from DBT_DEMO.DEV_pipeline_a.fact_cashflow_summary
),

-- Aggregate cashflows by portfolio to get total contributions/distributions
portfolio_cashflows as (
    select
        portfolio_id,
        sum(case when cashflow_type = 'CONTRIBUTION' then cumulative_total else 0 end) as total_contributions,
        sum(case when cashflow_type = 'DISTRIBUTION' then abs(cumulative_total) else 0 end) as total_distributions,
        max(cashflow_month) as last_cashflow_date
    from cashflow_summary
    group by portfolio_id
),

latest_positions as (
    select *
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id, security_id
                order by trade_date desc, trade_id desc
            ) as rn
        from trade_pnl
    )
    where rn = 1
),

-- ISSUE: Self-join to get position 30 days ago
positions_30d_ago as (
    select
        portfolio_id,
        security_id,
        running_position as position_30d_ago,
        avg_cost_basis as cost_basis_30d_ago
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id, security_id
                order by trade_date desc, trade_id desc
            ) as rn
        from trade_pnl
        where trade_date <= dateadd(day, -30, current_date())
    )
    where rn = 1
),

-- ISSUE: Self-join to get position 90 days ago
positions_90d_ago as (
    select
        portfolio_id,
        security_id,
        running_position as position_90d_ago,
        avg_cost_basis as cost_basis_90d_ago
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id, security_id
                order by trade_date desc, trade_id desc
            ) as rn
        from trade_pnl
        where trade_date <= dateadd(day, -90, current_date())
    )
    where rn = 1
),

market_prices as (
    select
        security_id,
        close_price as current_price,
        price_date
    from (
        select
            security_id,
            close_price,
            price_date,
            row_number() over (partition by security_id order by price_date desc) as rn
        from DBT_DEMO.DEV_pipeline_b.stg_market_prices
    )
    where rn = 1  -- ISSUE: Again, should use QUALIFY
),

-- ISSUE: Get historical prices for comparison
market_prices_30d_ago as (
    select
        security_id,
        close_price as price_30d_ago
    from (
        select
            security_id,
            close_price,
            row_number() over (partition by security_id order by price_date desc) as rn
        from DBT_DEMO.DEV_pipeline_b.stg_market_prices
        where price_date <= dateadd(day, -30, current_date())
    )
    where rn = 1
),

-- ISSUE: Join all the position snapshots together
enriched_positions as (
    select
        lp.*,
        mp.current_price,
        mp.price_date as price_as_of_date,
        p30.position_30d_ago,
        p30.cost_basis_30d_ago,
        p90.position_90d_ago,
        p90.cost_basis_90d_ago,
        mp30.price_30d_ago,
        pcf.total_contributions,
        pcf.total_distributions,
        pcf.last_cashflow_date
    from latest_positions lp
    left join market_prices mp
        on lp.security_id = mp.security_id
    left join positions_30d_ago p30
        on lp.portfolio_id = p30.portfolio_id
        and lp.security_id = p30.security_id
    left join positions_90d_ago p90
        on lp.portfolio_id = p90.portfolio_id
        and lp.security_id = p90.security_id
    left join market_prices_30d_ago mp30
        on lp.security_id = mp30.security_id
    left join portfolio_cashflows pcf
        on lp.portfolio_id = pcf.portfolio_id
    where lp.running_position != 0
),

-- ISSUE: Window functions for portfolio-level context
with_portfolio_context as (
    select
        ep.*,
        -- ISSUE: Repeated partition by portfolio_id
        sum(ep.running_position * ep.current_price) over (
            partition by ep.portfolio_id
        ) as portfolio_total_market_value,
        sum(ep.running_position * ep.avg_cost_basis) over (
            partition by ep.portfolio_id
        ) as portfolio_total_cost_basis,
        count(*) over (
            partition by ep.portfolio_id
        ) as portfolio_position_count,
        -- ISSUE: Rankings
        row_number() over (
            partition by ep.portfolio_id
            order by (ep.running_position * ep.current_price) desc
        ) as position_size_rank,
        row_number() over (
            partition by ep.portfolio_id
            order by ((ep.current_price - ep.avg_cost_basis) / nullif(ep.avg_cost_basis, 0)) desc
        ) as position_return_rank
    from enriched_positions ep
),

-- ISSUE: Separate aggregation for sector context (should use window functions)
sector_aggs as (
    select
        portfolio_id,
        sector,
        sum(running_position * current_price) as sector_market_value,
        count(*) as sector_position_count
    from enriched_positions
    group by 1, 2
),

with_sector_context as (
    select
        wpc.*,
        sa.sector_market_value,
        sa.sector_position_count
    from with_portfolio_context wpc
    left join sector_aggs sa
        on wpc.portfolio_id = sa.portfolio_id
        and wpc.sector = sa.sector
),

-- ISSUE: Complex calculations in final select
final as (
    select
        md5(cast(coalesce(cast(wsc.portfolio_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wsc.security_id as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as position_key,
        wsc.portfolio_id,
        wsc.security_id,
        wsc.ticker,
        wsc.security_name,
        wsc.sector,
        wsc.asset_class,
        wsc.running_position as current_quantity,
        wsc.avg_cost_basis,
        wsc.current_price,
        wsc.price_as_of_date,
        -- Core calculations
        wsc.running_position * wsc.avg_cost_basis as cost_basis_value,
        wsc.running_position * wsc.current_price as market_value,
        (wsc.running_position * wsc.current_price) - (wsc.running_position * wsc.avg_cost_basis) as unrealized_pnl,
        -- ISSUE: Repeated division logic
        case
            when wsc.avg_cost_basis > 0
            then ((wsc.current_price - wsc.avg_cost_basis) / wsc.avg_cost_basis) * 100
            else null
        end as unrealized_pnl_pct,
        -- Portfolio context
        wsc.portfolio_total_market_value,
        wsc.portfolio_total_cost_basis,
        wsc.portfolio_position_count,
        -- ISSUE: Weight calculation (repeated division)
        case
            when wsc.portfolio_total_market_value > 0
            then ((wsc.running_position * wsc.current_price) / wsc.portfolio_total_market_value) * 100
            else null
        end as portfolio_weight_pct,
        -- Sector context
        wsc.sector_market_value,
        wsc.sector_position_count,
        case
            when wsc.sector_market_value > 0
            then ((wsc.running_position * wsc.current_price) / wsc.sector_market_value) * 100
            else null
        end as sector_weight_pct,
        -- Historical comparison
        wsc.position_30d_ago,
        wsc.position_90d_ago,
        wsc.position_30d_ago - wsc.running_position as position_change_30d,
        wsc.position_90d_ago - wsc.running_position as position_change_90d,
        -- Price momentum
        wsc.price_30d_ago,
        case
            when wsc.price_30d_ago > 0
            then ((wsc.current_price - wsc.price_30d_ago) / wsc.price_30d_ago) * 100
            else null
        end as price_change_30d_pct,
        -- Cashflow context from Pipeline A
        wsc.total_contributions,
        wsc.total_distributions,
        wsc.last_cashflow_date,
        -- Rankings
        wsc.position_size_rank,
        wsc.position_return_rank,
        -- ISSUE: Complex classification
        case
            when ((wsc.running_position * wsc.current_price) / nullif(wsc.portfolio_total_market_value, 0)) > 0.10 then 'CONCENTRATED'
            when ((wsc.running_position * wsc.current_price) / nullif(wsc.portfolio_total_market_value, 0)) > 0.05 then 'SIGNIFICANT'
            when ((wsc.running_position * wsc.current_price) / nullif(wsc.portfolio_total_market_value, 0)) > 0.02 then 'MODERATE'
            else 'SMALL'
        end as position_size_category,
        wsc.cumulative_purchase_cost as total_invested,
        current_timestamp() as snapshot_timestamp
    from with_sector_context wsc
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_portfolio_positions"} */;
[0m20:23:52.800447 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.fact_trade_summary: create or replace transient table DBT_DEMO.DEV_pipeline_b.fact_trade_summary
    
    
    
    as (-- Pipeline B: Trade Analytics Pipeline
-- Model: fact_trade_summary
-- Description: Fact table for trade-level analysis
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-joins for prior trade lookups (should use LAG)
-- 2. Repeated window functions with same partitions
-- 3. Correlated subqueries for security-level aggregations
-- 4. Complex CASE statements repeated multiple times

with trade_pnl as (
    select * from DBT_DEMO.DEV_pipeline_b.int_trade_pnl
),

-- ISSUE: Getting portfolio data again (already available through joins upstream)
portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

-- ISSUE: Join that adds overhead
enriched as (
    select
        t.trade_id,
        t.portfolio_id,
        p.portfolio_name,
        p.portfolio_type,
        p.fund_id,
        t.security_id,
        t.ticker,
        t.security_name,
        t.security_type,
        t.asset_class,
        t.sector,
        t.trade_date,
        t.trade_type,
        t.trade_category,
        t.quantity,
        t.execution_price,
        t.net_amount,
        t.commission,
        t.running_position,
        t.avg_cost_basis,
        t.realized_pnl,
        t.realized_pnl_pct,
        -- ISSUE: Redundant date extractions (already done upstream)
        extract(year from t.trade_date) as trade_year,
        extract(month from t.trade_date) as trade_month,
        extract(quarter from t.trade_date) as trade_quarter,
        extract(dayofweek from t.trade_date) as trade_day_of_week,
        date_trunc('week', t.trade_date) as trade_week,
        date_trunc('month', t.trade_date) as trade_month_start
    from trade_pnl t
    left join portfolios p
        on t.portfolio_id = p.portfolio_id
),

-- ISSUE: Self-joins for prior trade comparisons (should use LAG)
-- Pre-compute trade sequence for self-join lookups
trade_sequences as (
    select
        *,
        row_number() over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
        ) as trade_seq
    from enriched
),

with_prior_trades as (
    select
        ts.*,
        -- ISSUE: Self-join for prior trade same security (should use LAG)
        ts_prior.execution_price as prior_trade_price,
        ts_prior.trade_date as prior_trade_date,
        ts_prior.quantity as prior_trade_quantity,
        -- ISSUE: Self-join for 5 trades ago (should use LAG offset)
        ts_5.execution_price as price_5_trades_ago,
        -- ISSUE: Self-join for 10 trades ago (should use LAG offset)
        ts_10.execution_price as price_10_trades_ago
    from trade_sequences ts
    left join trade_sequences ts_prior
        on ts.portfolio_id = ts_prior.portfolio_id
        and ts.security_id = ts_prior.security_id
        and ts_prior.trade_seq = ts.trade_seq - 1
    left join trade_sequences ts_5
        on ts.portfolio_id = ts_5.portfolio_id
        and ts.security_id = ts_5.security_id
        and ts_5.trade_seq = ts.trade_seq - 5
    left join trade_sequences ts_10
        on ts.portfolio_id = ts_10.portfolio_id
        and ts.security_id = ts_10.security_id
        and ts_10.trade_seq = ts.trade_seq - 10
),

-- ISSUE: Multiple window functions with repeated partitions
with_window_calcs as (
    select
        wpt.*,
        -- ISSUE: Running aggregations (repeated partition by portfolio_id, security_id)
        sum(quantity) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between unbounded preceding and current row
        ) as cumulative_quantity,
        sum(abs(net_amount)) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between unbounded preceding and current row
        ) as cumulative_trade_value,
        sum(coalesce(realized_pnl, 0)) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between unbounded preceding and current row
        ) as cumulative_realized_pnl,
        -- ISSUE: Moving averages (same partition repeated)
        avg(execution_price) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between 4 preceding and current row
        ) as rolling_5_trade_avg_price,
        avg(execution_price) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between 9 preceding and current row
        ) as rolling_10_trade_avg_price,
        avg(execution_price) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between 19 preceding and current row
        ) as rolling_20_trade_avg_price,
        -- ISSUE: More window calculations
        stddev(execution_price) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between 9 preceding and current row
        ) as rolling_10_trade_price_stddev,
        count(*) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between unbounded preceding and current row
        ) as trade_sequence_number,
        -- ISSUE: Rankings (same partition again)
        row_number() over (
            partition by wpt.portfolio_id, wpt.security_id, wpt.trade_category
            order by abs(wpt.net_amount) desc
        ) as size_rank_within_category
    from with_prior_trades wpt
),

-- ISSUE: Separate CTE for running trade stats (should be combined with window calcs above)
security_trade_aggs as (
    select
        portfolio_id,
        security_id,
        trade_date,
        trade_id,
        -- ISSUE: These window functions duplicate the partition from with_window_calcs
        count(*) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as total_portfolio_trades_this_security,
        avg(execution_price) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as avg_portfolio_price_this_security
    from enriched
),

-- ISSUE: Separate aggregation for fund-level volume (should be combined upstream)
fund_daily_volume as (
    select
        fund_id,
        security_id,
        trade_date,
        sum(abs(net_amount)) as fund_total_volume_same_security_same_day
    from enriched
    group by 1, 2, 3
),

with_security_context as (
    select
        wwc.*,
        sta.total_portfolio_trades_this_security,
        sta.avg_portfolio_price_this_security,
        fdv.fund_total_volume_same_security_same_day
    from with_window_calcs wwc
    left join security_trade_aggs sta
        on wwc.portfolio_id = sta.portfolio_id
        and wwc.security_id = sta.security_id
        and wwc.trade_id = sta.trade_id
    left join fund_daily_volume fdv
        on wwc.fund_id = fdv.fund_id
        and wwc.security_id = fdv.security_id
        and wwc.trade_date = fdv.trade_date
),

-- ISSUE: Complex derived metrics with repeated CASE statements
final as (
    select
        md5(cast(coalesce(cast(wsc.trade_id as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as trade_key,
        wsc.*,
        -- ISSUE: Price change calculations (repeated division logic)
        case
            when wsc.prior_trade_price is not null and wsc.prior_trade_price > 0
            then ((wsc.execution_price - wsc.prior_trade_price) / wsc.prior_trade_price) * 100
            else null
        end as price_change_from_prior_pct,
        case
            when wsc.price_5_trades_ago is not null and wsc.price_5_trades_ago > 0
            then ((wsc.execution_price - wsc.price_5_trades_ago) / wsc.price_5_trades_ago) * 100
            else null
        end as price_change_from_5_trades_ago_pct,
        case
            when wsc.rolling_20_trade_avg_price is not null and wsc.rolling_20_trade_avg_price > 0
            then ((wsc.execution_price - wsc.rolling_20_trade_avg_price) / wsc.rolling_20_trade_avg_price) * 100
            else null
        end as deviation_from_20_trade_avg_pct,
        -- ISSUE: Trade size classification (repeated CASE)
        case
            when abs(wsc.net_amount) >= 10000000 then 'BLOCK_TRADE'
            when abs(wsc.net_amount) >= 1000000 then 'LARGE'
            when abs(wsc.net_amount) >= 100000 then 'MEDIUM'
            when abs(wsc.net_amount) >= 10000 then 'SMALL'
            else 'MICRO'
        end as trade_size_category,
        -- ISSUE: Trade timing classification (complex nested CASE)
        case
            when wsc.execution_price > wsc.rolling_10_trade_avg_price * 1.1 then 'BOUGHT_HIGH'
            when wsc.execution_price > wsc.rolling_10_trade_avg_price * 1.03 then 'ABOVE_AVERAGE'
            when wsc.execution_price < wsc.rolling_10_trade_avg_price * 0.9 then 'BOUGHT_LOW'
            when wsc.execution_price < wsc.rolling_10_trade_avg_price * 0.97 then 'BELOW_AVERAGE'
            else 'AVERAGE'
        end as execution_quality,
        -- ISSUE: Momentum signal (repeated logic)
        case
            when wsc.rolling_5_trade_avg_price > wsc.rolling_20_trade_avg_price then 'UPTREND'
            when wsc.rolling_5_trade_avg_price < wsc.rolling_20_trade_avg_price then 'DOWNTREND'
            else 'NEUTRAL'
        end as price_momentum,
        -- ISSUE: Volatility classification
        case
            when wsc.rolling_10_trade_price_stddev < wsc.rolling_10_trade_avg_price * 0.02 then 'LOW_VOLATILITY'
            when wsc.rolling_10_trade_price_stddev < wsc.rolling_10_trade_avg_price * 0.05 then 'MODERATE_VOLATILITY'
            when wsc.rolling_10_trade_price_stddev < wsc.rolling_10_trade_avg_price * 0.10 then 'HIGH_VOLATILITY'
            else 'VERY_HIGH_VOLATILITY'
        end as price_volatility_regime,
        -- ISSUE: Trade frequency indicator
        case
            when wsc.trade_sequence_number >= 100 then 'VERY_ACTIVE'
            when wsc.trade_sequence_number >= 50 then 'ACTIVE'
            when wsc.trade_sequence_number >= 20 then 'MODERATE'
            when wsc.trade_sequence_number >= 5 then 'LIGHT'
            else 'FIRST_FEW_TRADES'
        end as trading_activity_level
    from with_security_context wsc
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_trade_summary"} */;
[0m20:23:53.856212 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 2.497 seconds
[0m20:23:53.861701 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113837b50>]}
[0m20:23:53.862442 [info ] [Thread-2  ]: 24 of 32 OK created sql table model DEV_pipeline_c.fact_sector_performance ..... [[32mSUCCESS 1[0m in 2.52s]
[0m20:23:53.862968 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.fact_sector_performance
[0m20:23:53.863323 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.fact_fund_summary
[0m20:23:53.863826 [info ] [Thread-2  ]: 28 of 32 START sql table model DEV_pipeline_c.fact_fund_summary ................ [RUN]
[0m20:23:53.864299 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.fact_sector_performance, now model.bain_capital_portfolio_analytics.fact_fund_summary)
[0m20:23:53.864643 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_fund_summary
[0m20:23:53.870382 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_fund_summary"
[0m20:23:53.871274 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.fact_fund_summary
[0m20:23:53.874608 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_fund_summary"
[0m20:23:53.876635 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_fund_summary"
[0m20:23:53.877053 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.fact_fund_summary: create or replace transient table DBT_DEMO.DEV_pipeline_c.fact_fund_summary
    
    
    
    as (-- Pipeline C: Complex Portfolio Analytics
-- Model: fact_fund_summary
-- Description: Fund-level summary metrics
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Based on already-aggregated data
-- 2. Could push more logic upstream

with fund_rollup as (
    select * from DBT_DEMO.DEV_pipeline_c.int_fund_rollup
),

fund_hierarchy as (
    select * from DBT_DEMO.DEV_pipeline_c.stg_fund_hierarchy
),

-- Get parent fund info
with_parent as (
    select
        fr.*,
        parent.entity_name as parent_fund_name
    from fund_rollup fr
    left join fund_hierarchy parent
        on fr.parent_entity_id = parent.entity_id
),

final as (
    select
        md5(cast(coalesce(cast(fund_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(valuation_date as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as fund_summary_key,
        fund_id,
        fund_name,
        parent_entity_id as parent_fund_id,
        parent_fund_name,
        hierarchy_level,
        valuation_date,
        portfolio_count,
        total_nav_usd,
        weighted_return_1y,
        weighted_volatility_1y,
        weighted_sharpe_ratio,
        worst_drawdown,
        total_var_95,
        -- ISSUE: Calculated fields
        case
            when weighted_return_1y >= 0.15 then 'HIGH'
            when weighted_return_1y >= 0.08 then 'MEDIUM'
            when weighted_return_1y >= 0 then 'LOW'
            else 'NEGATIVE'
        end as return_tier,
        case
            when weighted_sharpe_ratio >= 1.5 then 'EXCELLENT'
            when weighted_sharpe_ratio >= 1.0 then 'GOOD'
            when weighted_sharpe_ratio >= 0.5 then 'FAIR'
            else 'POOR'
        end as risk_adjusted_tier
    from with_parent
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_fund_summary"} */;
[0m20:23:53.970335 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 1.187 seconds
[0m20:23:53.973684 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11358cfd0>]}
[0m20:23:53.974719 [info ] [Thread-1  ]: 26 of 32 OK created sql table model DEV_pipeline_a.report_monthly_cashflows .... [[32mSUCCESS 1[0m in 1.22s]
[0m20:23:53.975465 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.report_monthly_cashflows
[0m20:23:57.924179 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 4.046 seconds
[0m20:23:57.928125 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1138bde20>]}
[0m20:23:57.929271 [info ] [Thread-2  ]: 28 of 32 OK created sql table model DEV_pipeline_c.fact_fund_summary ........... [[32mSUCCESS 1[0m in 4.06s]
[0m20:23:57.929987 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.fact_fund_summary
[0m20:24:03.466129 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 10.664 seconds
[0m20:24:03.469118 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113835fa0>]}
[0m20:24:03.469893 [info ] [Thread-3  ]: 27 of 32 OK created sql table model DEV_pipeline_b.fact_portfolio_positions .... [[32mSUCCESS 1[0m in 10.72s]
[0m20:24:03.470499 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.fact_portfolio_positions
[0m20:24:24.010004 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 31.207 seconds
[0m20:24:24.013414 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1137fcca0>]}
[0m20:24:24.014882 [info ] [Thread-4  ]: 25 of 32 OK created sql table model DEV_pipeline_b.fact_trade_summary .......... [[32mSUCCESS 1[0m in 31.26s]
[0m20:24:24.016391 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.fact_trade_summary
[0m20:24:24.017587 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.fact_portfolio_performance
[0m20:24:24.018098 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.report_trading_performance
[0m20:24:24.018888 [info ] [Thread-1  ]: 29 of 32 START sql table model DEV_pipeline_c.fact_portfolio_performance ....... [RUN]
[0m20:24:24.019667 [info ] [Thread-2  ]: 30 of 32 START sql table model DEV_pipeline_b.report_trading_performance ....... [RUN]
[0m20:24:24.021051 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.fact_fund_summary, now model.bain_capital_portfolio_analytics.report_trading_performance)
[0m20:24:24.020323 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.report_monthly_cashflows, now model.bain_capital_portfolio_analytics.fact_portfolio_performance)
[0m20:24:24.022373 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_portfolio_performance
[0m20:24:24.021933 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.report_trading_performance
[0m20:24:24.032374 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_portfolio_performance"
[0m20:24:24.038698 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.report_trading_performance"
[0m20:24:24.039904 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.report_trading_performance
[0m20:24:24.040284 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.fact_portfolio_performance
[0m20:24:24.044600 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.report_trading_performance"
[0m20:24:24.048515 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_portfolio_performance"
[0m20:24:24.052068 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.report_trading_performance"
[0m20:24:24.052556 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.report_trading_performance: create or replace transient table DBT_DEMO.DEV_pipeline_b.report_trading_performance
    
    
    
    as (-- Pipeline B: Trade Analytics Pipeline
-- Model: report_trading_performance
-- Description: Trading performance report for IC dashboard
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Re-aggregates fact data that could be pre-computed
-- 2. Complex window functions repeated from other models
-- 3. Multiple CTEs that could be consolidated

with trades as (
    select * from DBT_DEMO.DEV_pipeline_b.fact_trade_summary
),

positions as (
    select * from DBT_DEMO.DEV_pipeline_b.fact_portfolio_positions
),

-- ISSUE: Re-aggregating trade data by portfolio/month
trade_metrics as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        trade_year,
        trade_month,
        count(distinct trade_id) as trade_count,
        count(distinct security_id) as securities_traded,
        sum(case when trade_category = 'PURCHASE' then 1 else 0 end) as buy_count,
        sum(case when trade_category = 'SALE' then 1 else 0 end) as sell_count,
        sum(case when trade_category = 'PURCHASE' then net_amount else 0 end) as total_purchases,
        sum(case when trade_category = 'SALE' then abs(net_amount) else 0 end) as total_sales,
        sum(coalesce(realized_pnl, 0)) as total_realized_pnl,
        avg(case when realized_pnl is not null then realized_pnl_pct else null end) as avg_realized_return_pct
    from trades
    group by 1,2,3,4,5,6
),

-- ISSUE: Aggregating positions separately
position_metrics as (
    select
        portfolio_id,
        count(distinct security_id) as position_count,
        sum(market_value) as total_market_value,
        sum(cost_basis_value) as total_cost_basis,
        sum(unrealized_pnl) as total_unrealized_pnl,
        avg(unrealized_pnl_pct) as avg_unrealized_return_pct
    from positions
    group by 1
),

-- ISSUE: Window functions for running totals (repeated pattern)
with_running_totals as (
    select
        tm.*,
        sum(total_realized_pnl) over (
            partition by tm.portfolio_id
            order by tm.trade_year, tm.trade_month
            rows between unbounded preceding and current row
        ) as cumulative_realized_pnl,
        sum(total_purchases) over (
            partition by tm.portfolio_id
            order by tm.trade_year, tm.trade_month
            rows between unbounded preceding and current row
        ) as cumulative_invested,
        -- ISSUE: Multiple LAG functions
        lag(total_realized_pnl, 1) over (
            partition by tm.portfolio_id
            order by tm.trade_year, tm.trade_month
        ) as prior_month_pnl,
        lag(trade_count, 1) over (
            partition by tm.portfolio_id
            order by tm.trade_year, tm.trade_month
        ) as prior_month_trades
    from trade_metrics tm
),

-- ISSUE: Final join adds more complexity
final as (
    select
        wrt.*,
        pm.position_count,
        pm.total_market_value,
        pm.total_cost_basis,
        pm.total_unrealized_pnl,
        pm.avg_unrealized_return_pct,
        -- Combined metrics
        wrt.total_realized_pnl + coalesce(pm.total_unrealized_pnl, 0) as total_pnl,
        case
            when pm.total_cost_basis > 0
            then ((pm.total_market_value + wrt.cumulative_realized_pnl) - pm.total_cost_basis) / pm.total_cost_basis * 100
            else null
        end as total_return_pct
    from with_running_totals wrt
    left join position_metrics pm
        on wrt.portfolio_id = pm.portfolio_id
)

select * from final
order by portfolio_id, trade_year, trade_month
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.report_trading_performance"} */;
[0m20:24:24.062488 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_portfolio_performance"
[0m20:24:24.063470 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.fact_portfolio_performance: create or replace transient table DBT_DEMO.DEV_pipeline_c.fact_portfolio_performance
    
    
    
    as (-- Pipeline C: Complex Portfolio Analytics
-- Model: fact_portfolio_performance
-- Description: Main performance fact table
-- DEPENDENCIES:
--   - Pipeline A: fact_cashflow_summary (for cashflow context)
--   - Pipeline B: fact_trade_summary, fact_portfolio_positions (for trading/position context)
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Multiple self-joins for period comparisons
-- 2. Repeated window functions with same partitions
-- 3. Complex CASE statements repeated multiple times
-- 4. Correlated subqueries
-- 5. Late filtering and unnecessary full table scans

with portfolio_vs_benchmark as (
    select * from DBT_DEMO.DEV_pipeline_c.int_portfolio_vs_benchmark
),

risk_metrics as (
    select * from DBT_DEMO.DEV_pipeline_c.int_risk_metrics
),

portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

-- DEPENDENCY ON PIPELINE A: Cashflow summary for capital deployment context
cashflow_summary as (
    select * from DBT_DEMO.DEV_pipeline_a.fact_cashflow_summary
),

-- Aggregate cashflows to portfolio level
portfolio_cashflow_totals as (
    select
        portfolio_id,
        sum(case when cashflow_type = 'CONTRIBUTION' then cumulative_total else 0 end) as total_contributions,
        sum(case when cashflow_type = 'DISTRIBUTION' then abs(cumulative_total) else 0 end) as total_distributions,
        sum(cumulative_total) as net_cashflow
    from cashflow_summary
    group by portfolio_id
),

-- DEPENDENCY ON PIPELINE B: Trade summary for trading activity context
trade_summary as (
    select * from DBT_DEMO.DEV_pipeline_b.fact_trade_summary
),

-- Aggregate trades to portfolio/date level
portfolio_trade_activity as (
    select
        portfolio_id,
        trade_month_start as activity_month,
        count(*) as trade_count,
        sum(case when trade_category = 'PURCHASE' then abs(net_amount) else 0 end) as total_purchases,
        sum(case when trade_category = 'SALE' then abs(net_amount) else 0 end) as total_sales,
        sum(coalesce(realized_pnl, 0)) as realized_pnl
    from trade_summary
    group by portfolio_id, trade_month_start
),

-- DEPENDENCY ON PIPELINE B: Position snapshot for current holdings context
portfolio_positions as (
    select * from DBT_DEMO.DEV_pipeline_b.fact_portfolio_positions
),

-- Aggregate positions to portfolio level
portfolio_position_totals as (
    select
        portfolio_id,
        sum(market_value) as total_market_value,
        sum(cost_basis_value) as total_cost_basis,
        sum(unrealized_pnl) as total_unrealized_pnl,
        count(*) as position_count
    from portfolio_positions
    group by portfolio_id
),

-- ISSUE: Another join when data could flow from upstream
combined as (
    select
        pvb.portfolio_id,
        pvb.valuation_date,
        pvb.nav,
        pvb.nav_usd,
        -- Portfolio returns
        pvb.portfolio_daily_return,
        pvb.portfolio_cumulative_return,
        pvb.portfolio_return_1m,
        pvb.portfolio_return_3m,
        pvb.portfolio_return_1y,
        pvb.portfolio_volatility,
        -- Benchmark comparison
        pvb.benchmark_id,
        pvb.benchmark_daily_return,
        pvb.benchmark_cumulative_return,
        pvb.benchmark_return_1m,
        pvb.benchmark_return_3m,
        pvb.benchmark_return_1y,
        pvb.benchmark_volatility,
        -- Relative performance
        pvb.daily_excess_return,
        pvb.cumulative_excess_return,
        pvb.excess_return_1m,
        pvb.excess_return_3m,
        pvb.excess_return_1y,
        pvb.tracking_error_1y,
        pvb.annualized_alpha,
        pvb.information_ratio,
        -- Risk metrics
        rm.drawdown,
        rm.max_drawdown,
        rm.downside_deviation_1y,
        rm.sharpe_ratio,
        rm.sortino_ratio,
        rm.var_95_1d,
        rm.var_99_1d
    from portfolio_vs_benchmark pvb
    inner join risk_metrics rm
        on pvb.portfolio_id = rm.portfolio_id
        and pvb.valuation_date = rm.valuation_date
),

-- ISSUE: Self-join for prior period comparisons (should use LAG)
with_prior_periods as (
    select
        c.*,
        -- ISSUE: Self-join for 1 day ago
        c1d.nav_usd as nav_1d_ago,
        c1d.portfolio_cumulative_return as return_1d_ago,
        -- ISSUE: Self-join for 1 week ago
        c1w.nav_usd as nav_1w_ago,
        c1w.portfolio_cumulative_return as return_1w_ago,
        -- ISSUE: Self-join for 1 month ago
        c1m.nav_usd as nav_1m_ago,
        c1m.portfolio_cumulative_return as return_1m_ago,
        -- ISSUE: Self-join for 3 months ago
        c3m.nav_usd as nav_3m_ago,
        c3m.portfolio_cumulative_return as return_3m_ago,
        -- ISSUE: Self-join for 1 year ago
        c1y.nav_usd as nav_1y_ago,
        c1y.portfolio_cumulative_return as return_1y_ago
    from combined c
    left join combined c1d
        on c.portfolio_id = c1d.portfolio_id
        and c1d.valuation_date = dateadd(day, -1, c.valuation_date)
    left join combined c1w
        on c.portfolio_id = c1w.portfolio_id
        and c1w.valuation_date = dateadd(day, -7, c.valuation_date)
    left join combined c1m
        on c.portfolio_id = c1m.portfolio_id
        and c1m.valuation_date = dateadd(month, -1, c.valuation_date)
    left join combined c3m
        on c.portfolio_id = c3m.portfolio_id
        and c3m.valuation_date = dateadd(month, -3, c.valuation_date)
    left join combined c1y
        on c.portfolio_id = c1y.portfolio_id
        and c1y.valuation_date = dateadd(year, -1, c.valuation_date)
),

-- ISSUE: Multiple window functions with repeated partitions
with_rankings as (
    select
        wpp.*,
        -- ISSUE: Multiple ROW_NUMBER with same partition
        row_number() over (
            partition by wpp.portfolio_id
            order by wpp.portfolio_cumulative_return desc
        ) as best_performance_rank,
        row_number() over (
            partition by wpp.portfolio_id
            order by wpp.portfolio_cumulative_return asc
        ) as worst_performance_rank,
        row_number() over (
            partition by wpp.portfolio_id
            order by wpp.sharpe_ratio desc nulls last
        ) as best_sharpe_rank,
        -- ISSUE: DENSE_RANK with same partition
        dense_rank() over (
            partition by wpp.portfolio_id
            order by wpp.nav_usd desc
        ) as nav_size_rank,
        -- ISSUE: More window calculations
        avg(wpp.portfolio_daily_return) over (
            partition by wpp.portfolio_id
            order by wpp.valuation_date
            rows between 20 preceding and current row
        ) as rolling_20d_avg_return,
        avg(wpp.portfolio_daily_return) over (
            partition by wpp.portfolio_id
            order by wpp.valuation_date
            rows between 60 preceding and current row
        ) as rolling_60d_avg_return,
        stddev(wpp.portfolio_daily_return) over (
            partition by wpp.portfolio_id
            order by wpp.valuation_date
            rows between 20 preceding and current row
        ) as rolling_20d_volatility,
        stddev(wpp.portfolio_daily_return) over (
            partition by wpp.portfolio_id
            order by wpp.valuation_date
            rows between 60 preceding and current row
        ) as rolling_60d_volatility
    from with_prior_periods wpp
),

-- ISSUE: Complex derived metrics with repeated CASE statements
with_derived_metrics as (
    select
        wr.*,
        -- ISSUE: Repeated complex CASE for performance classification
        case
            when wr.portfolio_cumulative_return >= 0.50 then 'EXCEPTIONAL'
            when wr.portfolio_cumulative_return >= 0.30 then 'EXCELLENT'
            when wr.portfolio_cumulative_return >= 0.15 then 'VERY_GOOD'
            when wr.portfolio_cumulative_return >= 0.05 then 'GOOD'
            when wr.portfolio_cumulative_return >= 0.00 then 'NEUTRAL'
            when wr.portfolio_cumulative_return >= -0.05 then 'POOR'
            when wr.portfolio_cumulative_return >= -0.15 then 'VERY_POOR'
            else 'UNACCEPTABLE'
        end as performance_rating,
        -- ISSUE: Same CASE for risk classification
        case
            when wr.sharpe_ratio >= 3.0 then 'EXCEPTIONAL'
            when wr.sharpe_ratio >= 2.0 then 'EXCELLENT'
            when wr.sharpe_ratio >= 1.5 then 'VERY_GOOD'
            when wr.sharpe_ratio >= 1.0 then 'GOOD'
            when wr.sharpe_ratio >= 0.5 then 'NEUTRAL'
            when wr.sharpe_ratio >= 0.0 then 'POOR'
            else 'VERY_POOR'
        end as risk_adjusted_rating,
        -- ISSUE: Complex calculation repeated
        case
            when wr.nav_1m_ago is not null and wr.nav_1m_ago > 0
            then (wr.nav_usd - wr.nav_1m_ago) / wr.nav_1m_ago
            else null
        end as nav_change_1m_pct,
        case
            when wr.nav_3m_ago is not null and wr.nav_3m_ago > 0
            then (wr.nav_usd - wr.nav_3m_ago) / wr.nav_3m_ago
            else null
        end as nav_change_3m_pct,
        case
            when wr.nav_1y_ago is not null and wr.nav_1y_ago > 0
            then (wr.nav_usd - wr.nav_1y_ago) / wr.nav_1y_ago
            else null
        end as nav_change_1y_pct,
        -- ISSUE: Momentum indicators with repeated logic
        case
            when wr.rolling_20d_avg_return > wr.rolling_60d_avg_return then 'POSITIVE_MOMENTUM'
            when wr.rolling_20d_avg_return < wr.rolling_60d_avg_return then 'NEGATIVE_MOMENTUM'
            else 'NEUTRAL_MOMENTUM'
        end as momentum_signal,
        -- ISSUE: Volatility regime classification
        case
            when wr.rolling_20d_volatility > wr.rolling_60d_volatility * 1.5 then 'HIGH_VOLATILITY'
            when wr.rolling_20d_volatility > wr.rolling_60d_volatility * 1.2 then 'ELEVATED_VOLATILITY'
            when wr.rolling_20d_volatility < wr.rolling_60d_volatility * 0.8 then 'LOW_VOLATILITY'
            else 'NORMAL_VOLATILITY'
        end as volatility_regime
    from with_rankings wr
),

-- ISSUE: Separate peer aggregation CTEs (should use window functions with partition by portfolio_type)
peer_return_aggs as (
    select
        p2.portfolio_type,
        pvb2.valuation_date,
        avg(pvb2.portfolio_cumulative_return) as peer_avg_return
    from portfolio_vs_benchmark pvb2
    inner join portfolios p2
        on pvb2.portfolio_id = p2.portfolio_id
    group by 1, 2
),

peer_sharpe_aggs as (
    select
        p2.portfolio_type,
        rm2.valuation_date,
        percentile_cont(0.5) within group (order by rm2.sharpe_ratio) as peer_median_sharpe
    from risk_metrics rm2
    inner join portfolios p2
        on rm2.portfolio_id = p2.portfolio_id
    group by 1, 2
),

with_peer_comparison as (
    select
        wdm.*,
        pra.peer_avg_return,
        psa.peer_median_sharpe
    from with_derived_metrics wdm
    inner join portfolios p_self
        on wdm.portfolio_id = p_self.portfolio_id
    left join peer_return_aggs pra
        on p_self.portfolio_type = pra.portfolio_type
        and wdm.valuation_date = pra.valuation_date
    left join peer_sharpe_aggs psa
        on p_self.portfolio_type = psa.portfolio_type
        and wdm.valuation_date = psa.valuation_date
),

-- ISSUE: Portfolio attributes added last
final as (
    select
        md5(cast(coalesce(cast(wpc.portfolio_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wpc.valuation_date as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as performance_key,
        p.portfolio_name,
        p.portfolio_type,
        p.fund_id,
        wpc.*,
        -- ISSUE: Date dimensions recalculated (should be in dim table)
        extract(year from wpc.valuation_date) as valuation_year,
        extract(month from wpc.valuation_date) as valuation_month,
        extract(quarter from wpc.valuation_date) as valuation_quarter,
        extract(dayofweek from wpc.valuation_date) as valuation_day_of_week,
        extract(dayofyear from wpc.valuation_date) as valuation_day_of_year,
        date_trunc('month', wpc.valuation_date) as valuation_month_start,
        date_trunc('quarter', wpc.valuation_date) as valuation_quarter_start,
        date_trunc('year', wpc.valuation_date) as valuation_year_start,
        -- ISSUE: String concatenations (slow)
        concat(p.portfolio_name, ' - ', wpc.valuation_date::varchar) as display_name,
        concat('Q', extract(quarter from wpc.valuation_date), ' ', extract(year from wpc.valuation_date)) as quarter_label,
        -- ISSUE: Complex derived field
        case
            when wpc.portfolio_cumulative_return > wpc.peer_avg_return then 'OUTPERFORMING'
            when wpc.portfolio_cumulative_return < wpc.peer_avg_return then 'UNDERPERFORMING'
            else 'AT_PEER'
        end as peer_relative_performance
    from with_peer_comparison wpc
    inner join portfolios p
        on wpc.portfolio_id = p.portfolio_id
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_portfolio_performance"} */;
[0m20:24:26.394498 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 2.335 seconds
[0m20:24:26.399003 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1137fcca0>]}
[0m20:24:26.400067 [info ] [Thread-2  ]: 30 of 32 OK created sql table model DEV_pipeline_b.report_trading_performance .. [[32mSUCCESS 1[0m in 2.38s]
[0m20:24:26.400737 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.report_trading_performance
[0m20:24:38.790686 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 14.725 seconds
[0m20:24:38.795573 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11383c1f0>]}
[0m20:24:38.796713 [info ] [Thread-1  ]: 29 of 32 OK created sql table model DEV_pipeline_c.fact_portfolio_performance .. [[32mSUCCESS 1[0m in 14.78s]
[0m20:24:38.797636 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.fact_portfolio_performance
[0m20:24:38.798568 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.report_ic_dashboard
[0m20:24:38.799026 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.report_lp_quarterly
[0m20:24:38.799598 [info ] [Thread-4  ]: 31 of 32 START sql table model DEV_pipeline_c.report_ic_dashboard .............. [RUN]
[0m20:24:38.800163 [info ] [Thread-3  ]: 32 of 32 START sql table model DEV_pipeline_c.report_lp_quarterly .............. [RUN]
[0m20:24:38.800703 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.fact_trade_summary, now model.bain_capital_portfolio_analytics.report_ic_dashboard)
[0m20:24:38.801117 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.fact_portfolio_positions, now model.bain_capital_portfolio_analytics.report_lp_quarterly)
[0m20:24:38.801502 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.report_ic_dashboard
[0m20:24:38.801851 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.report_lp_quarterly
[0m20:24:38.806943 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.report_ic_dashboard"
[0m20:24:38.812316 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.report_lp_quarterly"
[0m20:24:38.813431 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.report_lp_quarterly
[0m20:24:38.813842 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.report_ic_dashboard
[0m20:24:38.817185 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.report_lp_quarterly"
[0m20:24:38.820899 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.report_ic_dashboard"
[0m20:24:38.831581 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.report_lp_quarterly"
[0m20:24:38.835064 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.report_ic_dashboard"
[0m20:24:38.835860 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.report_lp_quarterly: create or replace transient table DBT_DEMO.DEV_pipeline_c.report_lp_quarterly
    
    
    
    as (-- Pipeline C: Complex Portfolio Analytics
-- Model: report_lp_quarterly
-- Description: Quarterly LP reporting with period comparisons
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-joins for multi-period comparisons (should use LAG)
-- 2. Repeated window functions with same partitions
-- 3. Correlated subqueries for fund-level aggregations
-- 4. Complex CASE statements repeated multiple times
-- 5. Late filtering after heavy computation

with portfolio_performance as (
    select * from DBT_DEMO.DEV_pipeline_c.fact_portfolio_performance
),

cashflow_summary as (
    select * from DBT_DEMO.DEV_pipeline_a.fact_cashflow_summary
),

-- ISSUE: Filter to quarter-end dates only (filtering late)
quarter_end_perf as (
    select *
    from portfolio_performance
    where valuation_date = last_day(valuation_date, 'quarter')
),

-- ISSUE: Aggregate cashflows to quarterly
quarterly_cashflows as (
    select
        portfolio_id,
        date_trunc('quarter', cashflow_month) as quarter_start,
        sum(case when cashflow_type = 'CONTRIBUTION' then total_amount else 0 end) as quarterly_contributions,
        sum(case when cashflow_type = 'DISTRIBUTION' then total_amount else 0 end) as quarterly_distributions,
        sum(case when cashflow_type = 'DIVIDEND' then total_amount else 0 end) as quarterly_dividends,
        sum(case when cashflow_type = 'FEE' then abs(total_amount) else 0 end) as quarterly_fees,
        sum(total_amount) as quarterly_net_cashflow,
        count(distinct transaction_count) as total_transactions
    from cashflow_summary
    group by 1, 2
),

-- ISSUE: Join performance and cashflows
combined as (
    select
        qep.portfolio_id,
        qep.portfolio_name,
        qep.portfolio_type,
        qep.fund_id,
        qep.valuation_date as quarter_end,
        qep.valuation_quarter_start as quarter_start,
        qep.valuation_year,
        qep.valuation_quarter,
        qep.nav_usd,
        qep.portfolio_return_3m as quarterly_return,
        qep.portfolio_return_1y,
        qep.portfolio_cumulative_return,
        qep.benchmark_return_3m as benchmark_quarterly_return,
        qep.excess_return_3m as quarterly_excess_return,
        qep.sharpe_ratio,
        qep.sortino_ratio,
        qep.max_drawdown,
        qep.performance_rating,
        qep.risk_adjusted_rating,
        qcf.quarterly_contributions,
        qcf.quarterly_distributions,
        qcf.quarterly_dividends,
        qcf.quarterly_fees,
        qcf.quarterly_net_cashflow,
        qcf.total_transactions
    from quarter_end_perf qep
    left join quarterly_cashflows qcf
        on qep.portfolio_id = qcf.portfolio_id
        and qep.valuation_quarter_start = qcf.quarter_start
),

-- ISSUE: Multiple self-joins for historical comparisons (should use LAG)
with_self_joins as (
    select
        c.*,
        -- ISSUE: Self-join for prior quarter
        c_q1.nav_usd as prior_1q_nav,
        c_q1.quarterly_return as prior_1q_return,
        c_q1.sharpe_ratio as prior_1q_sharpe,
        -- ISSUE: Self-join for 2 quarters ago
        c_q2.nav_usd as prior_2q_nav,
        c_q2.quarterly_return as prior_2q_return,
        -- ISSUE: Self-join for 3 quarters ago
        c_q3.nav_usd as prior_3q_nav,
        c_q3.quarterly_return as prior_3q_return,
        -- ISSUE: Self-join for 4 quarters ago (1 year)
        c_q4.nav_usd as prior_4q_nav,
        c_q4.quarterly_return as prior_4q_return,
        c_q4.sharpe_ratio as prior_4q_sharpe,
        -- ISSUE: Self-join for 8 quarters ago (2 years)
        c_q8.nav_usd as prior_8q_nav,
        c_q8.quarterly_return as prior_8q_return
    from combined c
    left join combined c_q1
        on c.portfolio_id = c_q1.portfolio_id
        and c_q1.quarter_end = dateadd(quarter, -1, c.quarter_end)
    left join combined c_q2
        on c.portfolio_id = c_q2.portfolio_id
        and c_q2.quarter_end = dateadd(quarter, -2, c.quarter_end)
    left join combined c_q3
        on c.portfolio_id = c_q3.portfolio_id
        and c_q3.quarter_end = dateadd(quarter, -3, c.quarter_end)
    left join combined c_q4
        on c.portfolio_id = c_q4.portfolio_id
        and c_q4.quarter_end = dateadd(quarter, -4, c.quarter_end)
    left join combined c_q8
        on c.portfolio_id = c_q8.portfolio_id
        and c_q8.quarter_end = dateadd(quarter, -8, c.quarter_end)
),

-- ISSUE: Multiple window functions with repeated partitions
with_window_calcs as (
    select
        wsj.*,
        -- ISSUE: Running totals (repeated partition)
        sum(quarterly_contributions) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between unbounded preceding and current row
        ) as cumulative_contributions,
        sum(quarterly_distributions) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between unbounded preceding and current row
        ) as cumulative_distributions,
        sum(quarterly_dividends) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between unbounded preceding and current row
        ) as cumulative_dividends,
        sum(quarterly_fees) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between unbounded preceding and current row
        ) as cumulative_fees,
        -- ISSUE: Moving averages (same partition repeated)
        avg(quarterly_return) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between 3 preceding and current row
        ) as rolling_4q_avg_return,
        avg(quarterly_return) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between 7 preceding and current row
        ) as rolling_8q_avg_return,
        -- ISSUE: More window calculations
        stddev(quarterly_return) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between 3 preceding and current row
        ) as rolling_4q_volatility,
        min(quarterly_return) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between 3 preceding and current row
        ) as rolling_4q_min_return,
        max(quarterly_return) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between 3 preceding and current row
        ) as rolling_4q_max_return,
        -- ISSUE: Ranking (same partition again)
        row_number() over (
            partition by wsj.portfolio_id
            order by wsj.quarterly_return desc
        ) as best_quarter_rank,
        row_number() over (
            partition by wsj.portfolio_id
            order by wsj.quarterly_return asc
        ) as worst_quarter_rank
    from with_self_joins wsj
),

-- ISSUE: Separate fund-level aggregation (should use window functions)
fund_quarterly_aggs as (
    select
        fund_id,
        valuation_date as quarter_end,
        sum(nav_usd) as fund_total_nav,
        avg(portfolio_return_3m) as fund_avg_quarterly_return,
        count(distinct portfolio_id) as fund_portfolio_count
    from quarter_end_perf
    group by 1, 2
),

with_fund_context as (
    select
        wwc.*,
        fqa.fund_total_nav,
        fqa.fund_avg_quarterly_return,
        fqa.fund_portfolio_count
    from with_window_calcs wwc
    left join fund_quarterly_aggs fqa
        on wwc.fund_id = fqa.fund_id
        and wwc.quarter_end = fqa.quarter_end
),

-- ISSUE: Complex derived metrics with repeated CASE statements
with_derived_metrics as (
    select
        wfc.*,
        -- ISSUE: Portfolio weight in fund
        case
            when wfc.fund_total_nav > 0
            then (wfc.nav_usd / wfc.fund_total_nav) * 100
            else null
        end as portfolio_weight_in_fund,
        -- ISSUE: Complex QoQ calculations (repeated division logic)
        case
            when wfc.prior_1q_nav is not null and wfc.prior_1q_nav > 0
            then ((wfc.nav_usd - wfc.prior_1q_nav) / wfc.prior_1q_nav) * 100
            else null
        end as qoq_nav_growth_pct,
        case
            when wfc.prior_1q_return is not null
            then (wfc.quarterly_return - wfc.prior_1q_return)
            else null
        end as qoq_return_change,
        -- ISSUE: Complex YoY calculations
        case
            when wfc.prior_4q_nav is not null and wfc.prior_4q_nav > 0
            then ((wfc.nav_usd - wfc.prior_4q_nav) / wfc.prior_4q_nav) * 100
            else null
        end as yoy_nav_growth_pct,
        case
            when wfc.prior_4q_return is not null
            then (wfc.quarterly_return - wfc.prior_4q_return)
            else null
        end as yoy_return_change,
        -- ISSUE: 2-year growth
        case
            when wfc.prior_8q_nav is not null and wfc.prior_8q_nav > 0
            then ((wfc.nav_usd - wfc.prior_8q_nav) / wfc.prior_8q_nav) * 100
            else null
        end as two_year_nav_growth_pct,
        -- ISSUE: TVPI and DPI calculations (repeated division)
        case
            when wfc.cumulative_contributions > 0
            then (wfc.nav_usd + wfc.cumulative_distributions) / wfc.cumulative_contributions
            else null
        end as tvpi,
        case
            when wfc.cumulative_contributions > 0
            then wfc.cumulative_distributions / wfc.cumulative_contributions
            else null
        end as dpi,
        case
            when wfc.cumulative_contributions > 0
            then wfc.nav_usd / wfc.cumulative_contributions
            else null
        end as rvpi,
        -- ISSUE: Performance trend classification (complex nested CASE)
        case
            when wfc.rolling_4q_avg_return > wfc.rolling_8q_avg_return * 1.2 then 'ACCELERATING'
            when wfc.rolling_4q_avg_return > wfc.rolling_8q_avg_return * 1.05 then 'IMPROVING'
            when wfc.rolling_4q_avg_return < wfc.rolling_8q_avg_return * 0.8 then 'DECELERATING'
            when wfc.rolling_4q_avg_return < wfc.rolling_8q_avg_return * 0.95 then 'DECLINING'
            else 'STABLE'
        end as performance_trend,
        -- ISSUE: Consistency rating (complex nested CASE)
        case
            when wfc.rolling_4q_volatility < 0.02 then 'VERY_CONSISTENT'
            when wfc.rolling_4q_volatility < 0.05 then 'CONSISTENT'
            when wfc.rolling_4q_volatility < 0.10 then 'MODERATE'
            when wfc.rolling_4q_volatility < 0.15 then 'VARIABLE'
            else 'HIGHLY_VARIABLE'
        end as consistency_rating,
        -- ISSUE: Relative to fund performance (nested CASE)
        case
            when wfc.quarterly_return > wfc.fund_avg_quarterly_return + 0.05 then 'SIGNIFICANT_OUTPERFORM'
            when wfc.quarterly_return > wfc.fund_avg_quarterly_return + 0.02 then 'OUTPERFORM'
            when wfc.quarterly_return < wfc.fund_avg_quarterly_return - 0.05 then 'SIGNIFICANT_UNDERPERFORM'
            when wfc.quarterly_return < wfc.fund_avg_quarterly_return - 0.02 then 'UNDERPERFORM'
            else 'IN_LINE'
        end as relative_to_fund
    from with_fund_context wfc
),

-- ISSUE: More complex calculations and string operations
final as (
    select
        wdm.portfolio_id,
        wdm.portfolio_name,
        wdm.portfolio_type,
        wdm.fund_id,
        wdm.quarter_end,
        wdm.quarter_start,
        wdm.valuation_year,
        wdm.valuation_quarter,
        -- ISSUE: String concatenations (slow)
        concat('Q', wdm.valuation_quarter, ' ', wdm.valuation_year) as quarter_label,
        concat(wdm.valuation_year, '-Q', wdm.valuation_quarter) as quarter_code,
        concat(wdm.portfolio_name, ' (', wdm.portfolio_type, ')') as portfolio_display,
        -- Core metrics
        wdm.nav_usd,
        wdm.quarterly_return,
        wdm.benchmark_quarterly_return,
        wdm.quarterly_excess_return,
        wdm.portfolio_return_1y as trailing_1y_return,
        wdm.portfolio_cumulative_return as since_inception_return,
        wdm.sharpe_ratio,
        wdm.sortino_ratio,
        wdm.max_drawdown,
        wdm.performance_rating,
        wdm.risk_adjusted_rating,
        -- Cashflow metrics
        wdm.quarterly_contributions,
        wdm.quarterly_distributions,
        wdm.quarterly_dividends,
        wdm.quarterly_fees,
        wdm.quarterly_net_cashflow,
        wdm.cumulative_contributions,
        wdm.cumulative_distributions,
        wdm.total_transactions,
        -- Period comparisons
        wdm.qoq_nav_growth_pct,
        wdm.qoq_return_change,
        wdm.yoy_nav_growth_pct,
        wdm.yoy_return_change,
        wdm.two_year_nav_growth_pct,
        -- Performance ratios
        wdm.tvpi,
        wdm.dpi,
        wdm.rvpi,
        -- Rolling metrics
        wdm.rolling_4q_avg_return,
        wdm.rolling_8q_avg_return,
        wdm.rolling_4q_volatility,
        wdm.rolling_4q_min_return,
        wdm.rolling_4q_max_return,
        -- Fund context
        wdm.fund_total_nav,
        wdm.fund_avg_quarterly_return,
        wdm.fund_portfolio_count,
        wdm.portfolio_weight_in_fund,
        -- Classifications
        wdm.performance_trend,
        wdm.consistency_rating,
        wdm.relative_to_fund,
        wdm.best_quarter_rank,
        wdm.worst_quarter_rank,
        -- ISSUE: Additional derived fields (repeated calculations)
        case
            when wdm.cumulative_contributions > 0
            then wdm.cumulative_distributions / wdm.cumulative_contributions * 100
            else null
        end as distribution_yield_pct,
        case
            when wdm.cumulative_contributions > 0
            then wdm.cumulative_fees / wdm.cumulative_contributions * 100
            else null
        end as fee_burden_pct
    from with_derived_metrics wdm
)

select * from final
order by portfolio_id, quarter_end
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.report_lp_quarterly"} */;
[0m20:24:38.836776 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.report_ic_dashboard: create or replace transient table DBT_DEMO.DEV_pipeline_c.report_ic_dashboard
    
    
    
    as (-- Pipeline C: Complex Portfolio Analytics
-- Model: report_ic_dashboard
-- Description: Investment Committee dashboard report
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Combines multiple fact tables
-- 2. Re-aggregates aggregated data
-- 3. Complex pivoting logic
-- 4. Multiple CTEs that could be simplified

with portfolio_performance as (
    select * from DBT_DEMO.DEV_pipeline_c.fact_portfolio_performance
),

fund_summary as (
    select * from DBT_DEMO.DEV_pipeline_c.fact_fund_summary
),

position_snapshot as (
    select * from DBT_DEMO.DEV_pipeline_c.fact_position_snapshot
),

-- ISSUE: Get latest performance per portfolio
latest_portfolio_perf as (
    select *
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id
                order by valuation_date desc
            ) as rn
        from portfolio_performance
    )
    where rn = 1  -- ISSUE: Should use QUALIFY
),

-- ISSUE: Get latest positions per portfolio
latest_positions as (
    select *
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id, security_id
                order by position_date desc
            ) as rn
        from position_snapshot
    )
    where rn = 1
),

-- ISSUE: Re-aggregate positions for portfolio summary
position_summary as (
    select
        portfolio_id,
        count(distinct security_id) as total_positions,
        count(distinct sector) as sector_count,
        sum(market_value_usd) as total_market_value,
        max(weight_pct) as max_position_weight,
        -- Concentration metrics
        sum(case when weight_pct >= 0.05 then 1 else 0 end) as positions_over_5pct
    from latest_positions
    group by 1
),

-- ISSUE: Sector concentration
sector_concentration as (
    select
        portfolio_id,
        listagg(sector, ', ') within group (order by sector_weight desc) as top_sectors
    from (
        select
            portfolio_id,
            sector,
            sum(weight_pct) as sector_weight,
            row_number() over (partition by portfolio_id order by sum(weight_pct) desc) as sector_rank
        from latest_positions
        group by 1, 2
    )
    where sector_rank <= 3
    group by 1
),

-- ISSUE: Combine all metrics
dashboard_data as (
    select
        lpp.portfolio_id,
        lpp.portfolio_name,
        lpp.portfolio_type,
        lpp.fund_id,
        fs.fund_name,
        lpp.valuation_date as as_of_date,
        lpp.nav_usd,
        -- Performance
        lpp.portfolio_return_1m,
        lpp.portfolio_return_3m,
        lpp.portfolio_return_1y,
        lpp.portfolio_cumulative_return as inception_return,
        -- Benchmark comparison
        lpp.benchmark_id,
        lpp.excess_return_1m,
        lpp.excess_return_1y,
        lpp.information_ratio,
        -- Risk
        lpp.portfolio_volatility,
        lpp.sharpe_ratio,
        lpp.sortino_ratio,
        lpp.max_drawdown,
        lpp.var_95_1d,
        -- Positions
        ps.total_positions,
        ps.sector_count,
        ps.max_position_weight,
        ps.positions_over_5pct,
        sc.top_sectors,
        -- Fund level
        fs.total_nav_usd as fund_total_nav,
        fs.portfolio_count as fund_portfolio_count,
        fs.weighted_sharpe_ratio as fund_sharpe,
        -- Portfolio share of fund
        case
            when fs.total_nav_usd > 0
            then lpp.nav_usd / fs.total_nav_usd * 100
            else null
        end as pct_of_fund
    from latest_portfolio_perf lpp
    left join fund_summary fs
        on lpp.fund_id = fs.fund_id
        and lpp.valuation_date = fs.valuation_date
    left join position_summary ps
        on lpp.portfolio_id = ps.portfolio_id
    left join sector_concentration sc
        on lpp.portfolio_id = sc.portfolio_id
),

-- ISSUE: Final scoring/ranking
final as (
    select
        *,
        -- Performance score (simplified)
        (coalesce(portfolio_return_1y, 0) * 0.4 +
         coalesce(sharpe_ratio, 0) * 0.3 +
         coalesce(information_ratio, 0) * 0.3) as composite_score,
        rank() over (order by portfolio_return_1y desc nulls last) as return_rank,
        rank() over (order by sharpe_ratio desc nulls last) as sharpe_rank
    from dashboard_data
)

select * from final
order by composite_score desc
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.report_ic_dashboard"} */;
[0m20:24:40.299128 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 1.461 seconds
[0m20:24:40.301777 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113934490>]}
[0m20:24:40.302476 [info ] [Thread-4  ]: 31 of 32 OK created sql table model DEV_pipeline_c.report_ic_dashboard ......... [[32mSUCCESS 1[0m in 1.50s]
[0m20:24:40.303001 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.report_ic_dashboard
[0m20:24:40.728081 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 1.891 seconds
[0m20:24:40.731787 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '662b4662-28f9-4fb5-9a5e-5ea9d7b8e3c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113938580>]}
[0m20:24:40.732859 [info ] [Thread-3  ]: 32 of 32 OK created sql table model DEV_pipeline_c.report_lp_quarterly ......... [[32mSUCCESS 1[0m in 1.93s]
[0m20:24:40.733834 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.report_lp_quarterly
[0m20:24:40.735554 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:24:40.736050 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.fact_portfolio_performance' was left open.
[0m20:24:40.736536 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.fact_portfolio_performance: Close
[0m20:24:41.041893 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.report_trading_performance' was left open.
[0m20:24:41.042489 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.report_trading_performance: Close
[0m20:24:41.328491 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.report_lp_quarterly' was left open.
[0m20:24:41.328973 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.report_lp_quarterly: Close
[0m20:24:41.638322 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.report_ic_dashboard' was left open.
[0m20:24:41.638842 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.report_ic_dashboard: Close
[0m20:24:41.924641 [info ] [MainThread]: 
[0m20:24:41.925492 [info ] [MainThread]: Finished running 11 table models, 21 view models in 0 hours 1 minutes and 0.64 seconds (60.64s).
[0m20:24:41.931905 [debug] [MainThread]: Command end result
[0m20:24:41.992681 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/manifest.json
[0m20:24:41.994818 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/semantic_manifest.json
[0m20:24:42.002251 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/nedazarei/Documents/turintech/dbtproject/target/run_results.json
[0m20:24:42.002572 [info ] [MainThread]: 
[0m20:24:42.002907 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:24:42.003202 [info ] [MainThread]: 
[0m20:24:42.003492 [info ] [MainThread]: Done. PASS=32 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=32
[0m20:24:42.004012 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 10 occurrences
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m20:24:42.006317 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 64.04781, "process_in_blocks": "0", "process_kernel_time": 0.617423, "process_mem_max_rss": "204423168", "process_out_blocks": "0", "process_user_time": 5.830906}
[0m20:24:42.006655 [debug] [MainThread]: Command `dbt run` succeeded at 20:24:42.006589 after 64.05 seconds
[0m20:24:42.006969 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ad5100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bfccc40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d63fa30>]}
[0m20:24:42.007291 [debug] [MainThread]: Flushing usage events
[0m20:24:43.068137 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:48:15.124696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c32a160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d3d7940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d3d7760>]}


============================== 21:48:15.131408 | 7bd73edd-937b-4295-9a09-5b6fadbcc337 ==============================
[0m21:48:15.131408 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:48:15.132029 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'warn_error': 'None', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'introspect': 'True', 'log_cache_events': 'False', 'write_json': 'True', 'debug': 'False', 'profiles_dir': '/Users/nedazarei/Documents/turintech/dbtproject', 'cache_selected_only': 'False', 'fail_fast': 'False', 'printer_width': '80', 'version_check': 'True', 'invocation_command': 'dbt run --select +pipeline_a.*', 'log_path': '/Users/nedazarei/Documents/turintech/dbtproject/logs', 'static_parser': 'True', 'empty': 'False', 'indirect_selection': 'eager', 'use_colors': 'True', 'target_path': 'None'}
[0m21:48:15.137252 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'SNOWFLAKE_ACCOUNT'
[0m21:48:15.140089 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.099966295, "process_in_blocks": "0", "process_kernel_time": 0.152621, "process_mem_max_rss": "101351424", "process_out_blocks": "0", "process_user_time": 1.324865}
[0m21:48:15.140548 [debug] [MainThread]: Command `dbt run` failed at 21:48:15.140400 after 0.10 seconds
[0m21:48:15.140961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c32a160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d375820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d429370>]}
[0m21:48:15.141461 [debug] [MainThread]: Flushing usage events
[0m21:48:16.026189 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:48:24.510932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109de2160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ae8f940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ae8f760>]}


============================== 21:48:24.516832 | 387918e1-c6d0-4155-8208-f2f323e06970 ==============================
[0m21:48:24.516832 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:48:24.517355 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'introspect': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'warn_error': 'None', 'log_path': '/Users/nedazarei/Documents/turintech/dbtproject/logs', 'profiles_dir': '/Users/nedazarei/Documents/turintech/dbtproject', 'no_print': 'None', 'static_parser': 'True', 'version_check': 'True', 'printer_width': '80', 'invocation_command': 'dbt run --select +pipeline_b.*', 'cache_selected_only': 'False'}
[0m21:48:24.522525 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'SNOWFLAKE_ACCOUNT'
[0m21:48:24.524954 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.09515929, "process_in_blocks": "0", "process_kernel_time": 0.161023, "process_mem_max_rss": "101367808", "process_out_blocks": "0", "process_user_time": 1.294269}
[0m21:48:24.525440 [debug] [MainThread]: Command `dbt run` failed at 21:48:24.525336 after 0.10 seconds
[0m21:48:24.525767 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109de2160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ae2e820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aee1370>]}
[0m21:48:24.526119 [debug] [MainThread]: Flushing usage events
[0m21:48:25.528515 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:48:42.132009 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bdca0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ce774f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ce77310>]}


============================== 21:48:42.137546 | 1ab3b69a-9f7c-446a-a21e-882562c320aa ==============================
[0m21:48:42.137546 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:48:42.138048 [debug] [MainThread]: running dbt with arguments {'log_path': '/Users/nedazarei/Documents/turintech/dbtproject/logs', 'cache_selected_only': 'False', 'debug': 'False', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'warn_error': 'None', 'log_cache_events': 'False', 'target_path': 'None', 'no_print': 'None', 'partial_parse': 'True', 'static_parser': 'True', 'quiet': 'False', 'version_check': 'True', 'write_json': 'True', 'fail_fast': 'False', 'invocation_command': 'dbt run --select +pipeline_a.*', 'empty': 'False', 'log_format': 'default', 'use_colors': 'True', 'profiles_dir': '/Users/nedazarei/Documents/turintech/dbtproject'}
[0m21:48:42.143006 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'SNOWFLAKE_ACCOUNT'
[0m21:48:42.145170 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.093269415, "process_in_blocks": "0", "process_kernel_time": 0.165671, "process_mem_max_rss": "101253120", "process_out_blocks": "0", "process_user_time": 1.302034}
[0m21:48:42.145580 [debug] [MainThread]: Command `dbt run` failed at 21:48:42.145488 after 0.09 seconds
[0m21:48:42.145887 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bdca0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ce59880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cec92e0>]}
[0m21:48:42.146207 [debug] [MainThread]: Flushing usage events
[0m21:48:43.338516 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:49:53.398605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108072130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10911cb20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10911cd00>]}


============================== 21:49:53.404326 | dc6452be-42be-49c9-90ff-06b761d23335 ==============================
[0m21:49:53.404326 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:49:53.404827 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'static_parser': 'True', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'write_json': 'True', 'log_path': '/Users/nedazarei/Documents/turintech/dbtproject/logs', 'cache_selected_only': 'False', 'fail_fast': 'False', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'quiet': 'False', 'profiles_dir': '/Users/nedazarei/Documents/turintech/dbtproject', 'introspect': 'True', 'target_path': 'None', 'debug': 'False', 'no_print': 'None', 'warn_error': 'None', 'invocation_command': 'dbt debug', 'printer_width': '80', 'indirect_selection': 'eager'}
[0m21:49:53.417635 [info ] [MainThread]: dbt version: 1.11.0-b3
[0m21:49:53.417986 [info ] [MainThread]: python version: 3.9.6
[0m21:49:53.418255 [info ] [MainThread]: python path: /Library/Developer/CommandLineTools/usr/bin/python3
[0m21:49:53.418506 [info ] [MainThread]: os info: macOS-26.1-arm64-arm-64bit
[0m21:49:53.815885 [info ] [MainThread]: Using profiles dir at /Users/nedazarei/Documents/turintech/dbtproject
[0m21:49:53.816458 [info ] [MainThread]: Using profiles.yml file at /Users/nedazarei/Documents/turintech/dbtproject/profiles.yml
[0m21:49:53.816750 [info ] [MainThread]: Using dbt_project.yml file at /Users/nedazarei/Documents/turintech/dbtproject/dbt_project.yml
[0m21:49:53.817406 [info ] [MainThread]: adapter type: snowflake
[0m21:49:53.817676 [info ] [MainThread]: adapter version: 1.10.2
[0m21:49:53.943253 [info ] [MainThread]: Configuration:
[0m21:49:53.943742 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m21:49:53.944027 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m21:49:53.944287 [info ] [MainThread]: Required dependencies:
[0m21:49:53.944635 [debug] [MainThread]: Executing "git --help"
[0m21:49:53.966549 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m21:49:53.967302 [debug] [MainThread]: STDERR: "b''"
[0m21:49:53.967630 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m21:49:53.967927 [info ] [MainThread]: Connection:
[0m21:49:53.968265 [info ] [MainThread]:   account: PZXTMSC-WP48482
[0m21:49:53.968530 [info ] [MainThread]:   user: NDAWZZ
[0m21:49:53.968794 [info ] [MainThread]:   database: DBT_DEMO
[0m21:49:53.969043 [info ] [MainThread]:   warehouse: COMPUTE_WH
[0m21:49:53.969286 [info ] [MainThread]:   role: ACCOUNTADMIN
[0m21:49:53.969529 [info ] [MainThread]:   schema: DEV
[0m21:49:53.969771 [info ] [MainThread]:   authenticator: None
[0m21:49:53.970014 [info ] [MainThread]:   oauth_client_id: None
[0m21:49:53.970252 [info ] [MainThread]:   query_tag: None
[0m21:49:53.970492 [info ] [MainThread]:   client_session_keep_alive: False
[0m21:49:53.970730 [info ] [MainThread]:   host: None
[0m21:49:53.970968 [info ] [MainThread]:   port: None
[0m21:49:53.971206 [info ] [MainThread]:   proxy_host: None
[0m21:49:53.971442 [info ] [MainThread]:   proxy_port: None
[0m21:49:53.971672 [info ] [MainThread]:   protocol: None
[0m21:49:53.971912 [info ] [MainThread]:   connect_retries: 1
[0m21:49:53.972147 [info ] [MainThread]:   connect_timeout: None
[0m21:49:53.972386 [info ] [MainThread]:   retry_on_database_errors: False
[0m21:49:53.972622 [info ] [MainThread]:   retry_all: False
[0m21:49:53.972854 [info ] [MainThread]:   insecure_mode: False
[0m21:49:53.973089 [info ] [MainThread]:   reuse_connections: True
[0m21:49:53.973329 [info ] [MainThread]:   s3_stage_vpce_dns_name: None
[0m21:49:53.973959 [info ] [MainThread]: Registered adapter: snowflake=1.10.2
[0m21:49:54.092925 [debug] [MainThread]: Acquiring new snowflake connection 'debug'
[0m21:49:54.138045 [debug] [MainThread]: Using snowflake connection "debug"
[0m21:49:54.138473 [debug] [MainThread]: On debug: select 1 as id
[0m21:49:54.138775 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:49:57.902047 [debug] [MainThread]: SQL status: SUCCESS 1 in 3.763 seconds
[0m21:49:57.904180 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m21:49:57.904733 [info ] [MainThread]: [32mAll checks passed![0m
[0m21:49:57.909303 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 4.5921254, "process_in_blocks": "0", "process_kernel_time": 0.382752, "process_mem_max_rss": "167493632", "process_out_blocks": "0", "process_user_time": 2.01037}
[0m21:49:57.910011 [debug] [MainThread]: Command `dbt debug` succeeded at 21:49:57.909891 after 4.59 seconds
[0m21:49:57.910499 [debug] [MainThread]: Connection 'debug' was left open.
[0m21:49:57.910909 [debug] [MainThread]: On debug: Close
[0m21:49:58.269885 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108072130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b6e42e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b6e4130>]}
[0m21:49:58.271019 [debug] [MainThread]: Flushing usage events
[0m21:49:59.158645 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:50:12.851757 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109df01c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ae9fa00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ae9f820>]}


============================== 21:50:12.857837 | 6c2848b3-059d-40ff-81f1-542874500440 ==============================
[0m21:50:12.857837 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:50:12.858375 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'printer_width': '80', 'static_parser': 'True', 'log_cache_events': 'False', 'version_check': 'True', 'write_json': 'True', 'introspect': 'True', 'log_format': 'default', 'cache_selected_only': 'False', 'indirect_selection': 'eager', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/Users/nedazarei/Documents/turintech/dbtproject', 'empty': 'False', 'invocation_command': 'dbt run --select +pipeline_a.*', 'debug': 'False', 'log_path': '/Users/nedazarei/Documents/turintech/dbtproject/logs', 'use_colors': 'True', 'warn_error': 'None', 'partial_parse': 'True', 'quiet': 'False', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False'}
[0m21:50:13.333884 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6c2848b3-059d-40ff-81f1-542874500440', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c429070>]}
[0m21:50:13.408321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6c2848b3-059d-40ff-81f1-542874500440', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c3c12b0>]}
[0m21:50:13.409162 [info ] [MainThread]: Registered adapter: snowflake=1.10.2
[0m21:50:13.541641 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m21:50:13.755502 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:50:13.755904 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:50:13.874134 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6c2848b3-059d-40ff-81f1-542874500440', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ae472e0>]}
[0m21:50:14.027123 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/manifest.json
[0m21:50:14.029578 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/semantic_manifest.json
[0m21:50:14.053728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6c2848b3-059d-40ff-81f1-542874500440', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c2e8040>]}
[0m21:50:14.054243 [info ] [MainThread]: Found 32 models, 90 data tests, 5 seeds, 12 sources, 632 macros
[0m21:50:14.054566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6c2848b3-059d-40ff-81f1-542874500440', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c2cfac0>]}
[0m21:50:14.057075 [info ] [MainThread]: 
[0m21:50:14.057390 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m21:50:14.057643 [info ] [MainThread]: 
[0m21:50:14.058052 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m21:50:14.062633 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO'
[0m21:50:14.119250 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO"
[0m21:50:14.119654 [debug] [ThreadPool]: On list_DBT_DEMO: show terse schemas in database DBT_DEMO
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO"} */
[0m21:50:14.119921 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:50:14.388120 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a24b280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b2fc8e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b2fc700>]}


============================== 21:50:14.392918 | 4ccd7fef-a1e2-400f-ba9b-44748b691e9c ==============================
[0m21:50:14.392918 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:50:14.393492 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'log_format': 'default', 'empty': 'False', 'write_json': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'debug': 'False', 'log_path': '/Users/nedazarei/Documents/turintech/dbtproject/logs', 'invocation_command': 'dbt run --select +pipeline_b.*', 'no_print': 'None', 'quiet': 'False', 'warn_error': 'None', 'use_colors': 'True', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'static_parser': 'True', 'profiles_dir': '/Users/nedazarei/Documents/turintech/dbtproject', 'target_path': 'None', 'version_check': 'True', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'indirect_selection': 'eager'}
[0m21:50:14.852749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4ccd7fef-a1e2-400f-ba9b-44748b691e9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c777280>]}
[0m21:50:14.926931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4ccd7fef-a1e2-400f-ba9b-44748b691e9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c777eb0>]}
[0m21:50:14.927908 [info ] [MainThread]: Registered adapter: snowflake=1.10.2
[0m21:50:15.059085 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m21:50:15.255668 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:50:15.256108 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:50:15.374705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4ccd7fef-a1e2-400f-ba9b-44748b691e9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b2cc190>]}
[0m21:50:15.531532 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/manifest.json
[0m21:50:15.533597 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/semantic_manifest.json
[0m21:50:15.552714 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4ccd7fef-a1e2-400f-ba9b-44748b691e9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d5e44f0>]}
[0m21:50:15.553217 [info ] [MainThread]: Found 32 models, 90 data tests, 5 seeds, 12 sources, 632 macros
[0m21:50:15.553522 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4ccd7fef-a1e2-400f-ba9b-44748b691e9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d5f1a60>]}
[0m21:50:15.556523 [info ] [MainThread]: 
[0m21:50:15.556824 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m21:50:15.557069 [info ] [MainThread]: 
[0m21:50:15.557483 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m21:50:15.562521 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO'
[0m21:50:15.571015 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO'
[0m21:50:15.611028 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO"
[0m21:50:15.611510 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO"
[0m21:50:15.611799 [debug] [ThreadPool]: On list_DBT_DEMO: show terse schemas in database DBT_DEMO
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO"} */
[0m21:50:15.612067 [debug] [ThreadPool]: On list_DBT_DEMO: show terse schemas in database DBT_DEMO
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO"} */
[0m21:50:15.612350 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:50:15.612634 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:50:17.661926 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 3.542 seconds
[0m21:50:17.670467 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DBT_DEMO, now list_DBT_DEMO_DEV_pipeline_a)
[0m21:50:17.671260 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO_DEV_pipeline_b'
[0m21:50:17.678823 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO_DEV_pipeline_c'
[0m21:50:17.691764 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO_DEV'
[0m21:50:17.694184 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV_pipeline_a"
[0m21:50:17.699886 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV_pipeline_b"
[0m21:50:17.702259 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV_pipeline_c"
[0m21:50:17.788780 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV"
[0m21:50:17.789185 [debug] [ThreadPool]: On list_DBT_DEMO_DEV_pipeline_a: show objects in DBT_DEMO.DEV_pipeline_a
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV_pipeline_a"} */;
[0m21:50:17.789532 [debug] [ThreadPool]: On list_DBT_DEMO_DEV_pipeline_b: show objects in DBT_DEMO.DEV_pipeline_b
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV_pipeline_b"} */;
[0m21:50:17.789830 [debug] [ThreadPool]: On list_DBT_DEMO_DEV_pipeline_c: show objects in DBT_DEMO.DEV_pipeline_c
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV_pipeline_c"} */;
[0m21:50:17.790108 [debug] [ThreadPool]: On list_DBT_DEMO_DEV: show objects in DBT_DEMO.DEV
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV"} */;
[0m21:50:17.790619 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:50:17.790972 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:50:17.791579 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:50:17.998732 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.208 seconds
[0m21:50:18.111890 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 2.499 seconds
[0m21:50:18.133017 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 2.521 seconds
[0m21:50:18.137000 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DBT_DEMO, now list_DBT_DEMO_DEV_pipeline_a)
[0m21:50:18.144239 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DBT_DEMO, now list_DBT_DEMO_DEV)
[0m21:50:18.149447 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV_pipeline_a"
[0m21:50:18.149901 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO_DEV_pipeline_b'
[0m21:50:18.152221 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV"
[0m21:50:18.152711 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO_DEV_pipeline_c'
[0m21:50:18.153097 [debug] [ThreadPool]: On list_DBT_DEMO_DEV_pipeline_a: show objects in DBT_DEMO.DEV_pipeline_a
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV_pipeline_a"} */;
[0m21:50:18.155437 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV_pipeline_b"
[0m21:50:18.155747 [debug] [ThreadPool]: On list_DBT_DEMO_DEV: show objects in DBT_DEMO.DEV
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV"} */;
[0m21:50:18.158722 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV_pipeline_c"
[0m21:50:18.159284 [debug] [ThreadPool]: On list_DBT_DEMO_DEV_pipeline_b: show objects in DBT_DEMO.DEV_pipeline_b
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV_pipeline_b"} */;
[0m21:50:18.159901 [debug] [ThreadPool]: On list_DBT_DEMO_DEV_pipeline_c: show objects in DBT_DEMO.DEV_pipeline_c
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV_pipeline_c"} */;
[0m21:50:18.160691 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:50:18.161783 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:50:18.345371 [debug] [ThreadPool]: SQL status: SUCCESS 17 in 0.186 seconds
[0m21:50:18.350167 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 0.191 seconds
[0m21:50:18.781291 [debug] [ThreadPool]: SQL status: SUCCESS 17 in 0.989 seconds
[0m21:50:19.155593 [debug] [ThreadPool]: SQL status: SUCCESS 19 in 1.364 seconds
[0m21:50:19.321417 [debug] [ThreadPool]: SQL status: SUCCESS 9 in 1.531 seconds
[0m21:50:19.329239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6c2848b3-059d-40ff-81f1-542874500440', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1085be9a0>]}
[0m21:50:19.333020 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.stg_cashflows
[0m21:50:19.333437 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.stg_portfolios
[0m21:50:19.333950 [info ] [Thread-1  ]: 1 of 4 START sql view model DEV_pipeline_a.stg_cashflows ....................... [RUN]
[0m21:50:19.334386 [info ] [Thread-2  ]: 2 of 4 START sql view model DEV_pipeline_a.stg_portfolios ...................... [RUN]
[0m21:50:19.334792 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV_pipeline_a, now model.bain_capital_portfolio_analytics.stg_cashflows)
[0m21:50:19.335130 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV_pipeline_b, now model.bain_capital_portfolio_analytics.stg_portfolios)
[0m21:50:19.335451 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_cashflows
[0m21:50:19.335760 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_portfolios
[0m21:50:19.344518 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_cashflows"
[0m21:50:19.347867 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_portfolios"
[0m21:50:19.349067 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.stg_cashflows
[0m21:50:19.349401 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.stg_portfolios
[0m21:50:19.387640 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_cashflows"
[0m21:50:19.390375 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_portfolios"
[0m21:50:19.391969 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_cashflows"
[0m21:50:19.393098 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_portfolios"
[0m21:50:19.393537 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.stg_cashflows: create or replace   view DBT_DEMO.DEV_pipeline_a.stg_cashflows
  
  
  
  
  as (
    -- Pipeline A: Simple Cashflow Pipeline
-- Model: stg_cashflows
-- Description: Staging model for raw cashflow data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Unnecessary DISTINCT (source already unique)
-- 2. Late filtering (should push date filter upstream)
-- 3. Non-optimal date casting

with source as (
    select distinct  -- ISSUE: Unnecessary DISTINCT, source has unique constraint
        cashflow_id,
        portfolio_id,
        cashflow_type,
        cashflow_date,
        amount,
        currency,
        created_at,
        updated_at
    from DBT_DEMO.DEV.cashflows
),

-- ISSUE: Heavy transformation before filtering
converted as (
    select
        cashflow_id,
        portfolio_id,
        upper(cashflow_type) as cashflow_type,
        cast(cashflow_date as date) as cashflow_date,
        cast(amount as decimal(18,2)) as amount,
        upper(currency) as currency,
        cast(created_at as timestamp) as created_at,
        cast(updated_at as timestamp) as updated_at
    from source
),

-- ISSUE: Filter applied after transformation, should be earlier
filtered as (
    select *
    from converted
    where cashflow_date >= '2020-01-01'
      and cashflow_date <= '2024-12-31'
)

select * from filtered
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_cashflows"} */;
[0m21:50:19.394048 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.stg_portfolios: create or replace   view DBT_DEMO.DEV_pipeline_a.stg_portfolios
  
  
  
  
  as (
    -- Pipeline A: Simple Cashflow Pipeline
-- Model: stg_portfolios
-- Description: Staging model for portfolio master data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Subquery for deduplication instead of QUALIFY
-- 2. Multiple passes over data

with source as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        inception_date,
        status,
        currency,
        created_at,
        updated_at,
        row_number() over (
            partition by portfolio_id
            order by updated_at desc
        ) as rn
    from DBT_DEMO.DEV.portfolios
),

-- ISSUE: Using subquery filter instead of QUALIFY
deduplicated as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        inception_date,
        status,
        currency,
        created_at,
        updated_at
    from source
    where rn = 1  -- ISSUE: Should use QUALIFY in Snowflake
),

-- ISSUE: Another pass just for active filter
active_only as (
    select *
    from deduplicated
    where status = 'ACTIVE'
)

select * from active_only
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_portfolios"} */;
[0m21:50:19.644406 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.249 seconds
[0m21:50:19.672549 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2848b3-059d-40ff-81f1-542874500440', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10867ae50>]}
[0m21:50:19.673320 [info ] [Thread-2  ]: 2 of 4 OK created sql view model DEV_pipeline_a.stg_portfolios ................. [[32mSUCCESS 1[0m in 0.33s]
[0m21:50:19.673846 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.stg_portfolios
[0m21:50:19.706869 [debug] [ThreadPool]: SQL status: SUCCESS 19 in 1.545 seconds
[0m21:50:19.707804 [debug] [ThreadPool]: SQL status: SUCCESS 9 in 1.547 seconds
[0m21:50:19.715118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4ccd7fef-a1e2-400f-ba9b-44748b691e9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c881e50>]}
[0m21:50:19.717572 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.stg_brokers
[0m21:50:19.717963 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.stg_cashflows
[0m21:50:19.718497 [info ] [Thread-1  ]: 1 of 12 START sql view model DEV_pipeline_b.stg_brokers ........................ [RUN]
[0m21:50:19.718919 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.stg_market_prices
[0m21:50:19.719363 [info ] [Thread-2  ]: 2 of 12 START sql view model DEV_pipeline_a.stg_cashflows ...................... [RUN]
[0m21:50:19.719731 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.stg_portfolios
[0m21:50:19.720103 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV_pipeline_a, now model.bain_capital_portfolio_analytics.stg_brokers)
[0m21:50:19.720518 [info ] [Thread-3  ]: 3 of 12 START sql view model DEV_pipeline_b.stg_market_prices .................. [RUN]
[0m21:50:19.721807 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.327 seconds
[0m21:50:19.720870 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV, now model.bain_capital_portfolio_analytics.stg_cashflows)
[0m21:50:19.721248 [info ] [Thread-4  ]: 4 of 12 START sql view model DEV_pipeline_a.stg_portfolios ..................... [RUN]
[0m21:50:19.721578 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_brokers
[0m21:50:19.723916 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2848b3-059d-40ff-81f1-542874500440', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1136c6130>]}
[0m21:50:19.724527 [info ] [Thread-1  ]: 1 of 4 OK created sql view model DEV_pipeline_a.stg_cashflows .................. [[32mSUCCESS 1[0m in 0.39s]
[0m21:50:19.725006 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.stg_cashflows
[0m21:50:19.725657 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m21:50:19.726145 [info ] [Thread-4  ]: 3 of 4 START sql table model DEV_pipeline_a.fact_cashflow_summary .............. [RUN]
[0m21:50:19.726598 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV, now model.bain_capital_portfolio_analytics.fact_cashflow_summary)
[0m21:50:19.726982 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m21:50:19.721884 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV_pipeline_b, now model.bain_capital_portfolio_analytics.stg_market_prices)
[0m21:50:19.722532 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_cashflows
[0m21:50:19.722846 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV_pipeline_c, now model.bain_capital_portfolio_analytics.stg_portfolios)
[0m21:50:19.731679 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_brokers"
[0m21:50:19.732031 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_market_prices
[0m21:50:19.735306 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_cashflows"
[0m21:50:19.735660 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_portfolios
[0m21:50:19.740868 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_cashflow_summary"
[0m21:50:19.741586 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m21:50:19.739043 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_market_prices"
[0m21:50:19.742006 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_portfolios"
[0m21:50:19.742710 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.stg_cashflows
[0m21:50:19.743043 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.stg_brokers
[0m21:50:19.743330 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.stg_market_prices
[0m21:50:19.764392 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_cashflow_summary"
[0m21:50:19.772256 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_cashflow_summary"
[0m21:50:19.772938 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.fact_cashflow_summary: create or replace transient table DBT_DEMO.DEV_pipeline_a.fact_cashflow_summary
    
    
    
    as (-- Pipeline A: Simple Cashflow Pipeline
-- Model: fact_cashflow_summary
-- Description: Fact table summarizing cashflows by portfolio and month
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-joins for prior period comparisons (should use LAG)
-- 2. Late aggregation (aggregates after full join)
-- 3. Repeated window functions with same partitions
-- 4. Redundant date calculations per row
-- 5. Correlated subqueries for fund-level totals

with cashflows as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_cashflows
),

portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

-- ISSUE: Full join before aggregation (scans all rows)
joined as (
    select
        c.cashflow_id,
        c.portfolio_id,
        p.portfolio_name,
        p.portfolio_type,
        p.fund_id,
        c.cashflow_type,
        c.cashflow_date,
        c.amount,
        c.currency,
        -- ISSUE: Redundant date calculations done per row
        date_trunc('month', c.cashflow_date) as cashflow_month,
        date_trunc('quarter', c.cashflow_date) as cashflow_quarter,
        date_trunc('year', c.cashflow_date) as cashflow_year,
        extract(year from c.cashflow_date) as year_num,
        extract(month from c.cashflow_date) as month_num,
        extract(quarter from c.cashflow_date) as quarter_num,
        extract(dayofmonth from c.cashflow_date) as day_num
    from cashflows c
    inner join portfolios p
        on c.portfolio_id = p.portfolio_id
),

-- ISSUE: Aggregation happens after full row-level join
aggregated as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        cashflow_month,
        cashflow_quarter,
        cashflow_year,
        year_num,
        month_num,
        quarter_num,
        cashflow_type,
        currency,
        count(*) as transaction_count,
        count(distinct cashflow_id) as unique_transactions,
        sum(amount) as total_amount,
        avg(amount) as avg_amount,
        min(amount) as min_amount,
        max(amount) as max_amount,
        stddev(amount) as stddev_amount,
        -- ISSUE: Percentile calculations (slow)
        percentile_cont(0.25) within group (order by amount) as p25_amount,
        percentile_cont(0.50) within group (order by amount) as median_amount,
        percentile_cont(0.75) within group (order by amount) as p75_amount
    from joined
    group by 1,2,3,4,5,6,7,8,9,10,11,12  -- ISSUE: Non-descriptive GROUP BY
),

-- ISSUE: Self-join for prior month comparisons (should use LAG)
with_prior_months as (
    select
        agg.*,
        -- ISSUE: Self-join for prior month
        agg_m1.total_amount as prior_1m_total,
        agg_m1.transaction_count as prior_1m_count,
        -- ISSUE: Self-join for 3 months ago
        agg_m3.total_amount as prior_3m_total,
        -- ISSUE: Self-join for 6 months ago
        agg_m6.total_amount as prior_6m_total,
        -- ISSUE: Self-join for 12 months ago
        agg_m12.total_amount as prior_12m_total
    from aggregated agg
    left join aggregated agg_m1
        on agg.portfolio_id = agg_m1.portfolio_id
        and agg.cashflow_type = agg_m1.cashflow_type
        and agg.currency = agg_m1.currency
        and agg_m1.cashflow_month = dateadd(month, -1, agg.cashflow_month)
    left join aggregated agg_m3
        on agg.portfolio_id = agg_m3.portfolio_id
        and agg.cashflow_type = agg_m3.cashflow_type
        and agg.currency = agg_m3.currency
        and agg_m3.cashflow_month = dateadd(month, -3, agg.cashflow_month)
    left join aggregated agg_m6
        on agg.portfolio_id = agg_m6.portfolio_id
        and agg.cashflow_type = agg_m6.cashflow_type
        and agg.currency = agg_m6.currency
        and agg_m6.cashflow_month = dateadd(month, -6, agg.cashflow_month)
    left join aggregated agg_m12
        on agg.portfolio_id = agg_m12.portfolio_id
        and agg.cashflow_type = agg_m12.cashflow_type
        and agg.currency = agg_m12.currency
        and agg_m12.cashflow_month = dateadd(month, -12, agg.cashflow_month)
),

-- ISSUE: Multiple window functions with repeated partitions
with_window_calcs as (
    select
        wpm.*,
        -- ISSUE: Running totals (repeated partition)
        sum(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_total,
        sum(transaction_count) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_count,
        -- ISSUE: Moving averages (same partition repeated)
        avg(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 2 preceding and current row
        ) as rolling_3m_avg,
        avg(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 5 preceding and current row
        ) as rolling_6m_avg,
        avg(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_avg,
        -- ISSUE: More window calculations
        stddev(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_stddev,
        min(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_min,
        max(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_max
    from with_prior_months wpm
),

-- ISSUE: Correlated subqueries for fund-level context (very slow)
with_fund_context as (
    select
        wwc.*,
        -- ISSUE: Correlated subquery for fund total
        (
            select sum(total_amount)
            from aggregated agg2
            inner join portfolios p2
                on agg2.portfolio_id = p2.portfolio_id
            where p2.fund_id = wwc.fund_id
            and agg2.cashflow_month = wwc.cashflow_month
            and agg2.cashflow_type = wwc.cashflow_type
        ) as fund_total_amount,
        -- ISSUE: Another correlated subquery for portfolio count
        (
            select count(distinct agg2.portfolio_id)
            from aggregated agg2
            inner join portfolios p2
                on agg2.portfolio_id = p2.portfolio_id
            where p2.fund_id = wwc.fund_id
            and agg2.cashflow_month = wwc.cashflow_month
            and agg2.cashflow_type = wwc.cashflow_type
        ) as fund_portfolio_count
    from with_window_calcs wwc
),

-- ISSUE: Complex derived metrics with repeated CASE statements
final as (
    select
        md5(cast(coalesce(cast(wfc.portfolio_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wfc.cashflow_month as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wfc.cashflow_type as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wfc.currency as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as cashflow_summary_key,
        wfc.*,
        -- ISSUE: Portfolio share of fund (repeated division)
        case
            when wfc.fund_total_amount > 0
            then (wfc.total_amount / wfc.fund_total_amount) * 100
            else null
        end as portfolio_share_of_fund_pct,
        -- ISSUE: Month-over-month growth calculations
        case
            when wfc.prior_1m_total is not null and wfc.prior_1m_total != 0
            then ((wfc.total_amount - wfc.prior_1m_total) / abs(wfc.prior_1m_total)) * 100
            else null
        end as mom_growth_pct,
        case
            when wfc.prior_3m_total is not null and wfc.prior_3m_total != 0
            then ((wfc.total_amount - wfc.prior_3m_total) / abs(wfc.prior_3m_total)) * 100
            else null
        end as growth_3m_pct,
        case
            when wfc.prior_12m_total is not null and wfc.prior_12m_total != 0
            then ((wfc.total_amount - wfc.prior_12m_total) / abs(wfc.prior_12m_total)) * 100
            else null
        end as yoy_growth_pct,
        -- ISSUE: Trend classification (complex nested CASE)
        case
            when wfc.rolling_3m_avg > wfc.rolling_12m_avg * 1.3 then 'ACCELERATING'
            when wfc.rolling_3m_avg > wfc.rolling_12m_avg * 1.1 then 'GROWING'
            when wfc.rolling_3m_avg < wfc.rolling_12m_avg * 0.7 then 'DECLINING_FAST'
            when wfc.rolling_3m_avg < wfc.rolling_12m_avg * 0.9 then 'DECLINING'
            else 'STABLE'
        end as trend_classification,
        -- ISSUE: Volatility classification
        case
            when wfc.rolling_12m_stddev < wfc.rolling_12m_avg * 0.1 then 'LOW_VOLATILITY'
            when wfc.rolling_12m_stddev < wfc.rolling_12m_avg * 0.3 then 'MODERATE_VOLATILITY'
            when wfc.rolling_12m_stddev < wfc.rolling_12m_avg * 0.5 then 'HIGH_VOLATILITY'
            else 'VERY_HIGH_VOLATILITY'
        end as volatility_classification,
        -- ISSUE: Size classification (repeated CASE)
        case
            when abs(wfc.total_amount) >= 10000000 then 'MEGA'
            when abs(wfc.total_amount) >= 5000000 then 'LARGE'
            when abs(wfc.total_amount) >= 1000000 then 'MEDIUM'
            when abs(wfc.total_amount) >= 100000 then 'SMALL'
            else 'MICRO'
        end as transaction_size_category
    from with_fund_context wfc
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_cashflow_summary"} */;
[0m21:50:19.749377 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.stg_portfolios
[0m21:50:19.783757 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_brokers"
[0m21:50:19.784821 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_cashflows"
[0m21:50:19.786803 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_market_prices"
[0m21:50:19.789492 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_portfolios"
[0m21:50:19.790947 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_brokers"
[0m21:50:19.791359 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.stg_brokers: create or replace   view DBT_DEMO.DEV_pipeline_b.stg_brokers
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: stg_brokers
-- Description: Staging model for broker information

with source as (
    select
        broker_id,
        broker_name,
        broker_type,
        region,
        is_active,
        commission_rate,
        created_at,
        updated_at
    from DBT_DEMO.DEV.brokers
),

deduplicated as (
    select *
    from (
        select
            *,
            row_number() over (partition by broker_id order by updated_at desc) as rn
        from source
    )
    where rn = 1
)

select
    broker_id,
    trim(broker_name) as broker_name,
    upper(broker_type) as broker_type,
    upper(region) as region,
    is_active,
    commission_rate,
    created_at,
    updated_at
from deduplicated
where is_active = true
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_brokers"} */;
[0m21:50:19.792509 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_cashflows"
[0m21:50:19.794869 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_market_prices"
[0m21:50:19.795414 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.stg_cashflows: create or replace   view DBT_DEMO.DEV_pipeline_a.stg_cashflows
  
  
  
  
  as (
    -- Pipeline A: Simple Cashflow Pipeline
-- Model: stg_cashflows
-- Description: Staging model for raw cashflow data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Unnecessary DISTINCT (source already unique)
-- 2. Late filtering (should push date filter upstream)
-- 3. Non-optimal date casting

with source as (
    select distinct  -- ISSUE: Unnecessary DISTINCT, source has unique constraint
        cashflow_id,
        portfolio_id,
        cashflow_type,
        cashflow_date,
        amount,
        currency,
        created_at,
        updated_at
    from DBT_DEMO.DEV.cashflows
),

-- ISSUE: Heavy transformation before filtering
converted as (
    select
        cashflow_id,
        portfolio_id,
        upper(cashflow_type) as cashflow_type,
        cast(cashflow_date as date) as cashflow_date,
        cast(amount as decimal(18,2)) as amount,
        upper(currency) as currency,
        cast(created_at as timestamp) as created_at,
        cast(updated_at as timestamp) as updated_at
    from source
),

-- ISSUE: Filter applied after transformation, should be earlier
filtered as (
    select *
    from converted
    where cashflow_date >= '2020-01-01'
      and cashflow_date <= '2024-12-31'
)

select * from filtered
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_cashflows"} */;
[0m21:50:19.796564 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_portfolios"
[0m21:50:19.797031 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.stg_market_prices: create or replace   view DBT_DEMO.DEV_pipeline_b.stg_market_prices
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: stg_market_prices
-- Description: Staging model for daily market prices
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-join for prior day prices (inefficient)
-- 2. Late aggregation
-- 3. Multiple window functions that could be consolidated

with source as (
    select
        security_id,
        price_date,
        open_price,
        high_price,
        low_price,
        close_price,
        volume,
        created_at
    from DBT_DEMO.DEV.market_prices
    where price_date >= '2020-01-01'
),

-- ISSUE: Self-join to get prior day price (should use LAG)
with_prior_day as (
    select
        curr.security_id,
        curr.price_date,
        curr.open_price,
        curr.high_price,
        curr.low_price,
        curr.close_price,
        curr.volume,
        prev.close_price as prior_close,
        prev.volume as prior_volume
    from source curr
    left join source prev
        on curr.security_id = prev.security_id
        and curr.price_date = dateadd('day', 1, prev.price_date)  -- ISSUE: Doesn't handle weekends
),

-- ISSUE: Multiple separate window functions
with_returns as (
    select
        *,
        -- Daily return
        case
            when prior_close > 0
            then (close_price - prior_close) / prior_close
            else null
        end as daily_return,
        -- ISSUE: These could be computed together
        avg(close_price) over (
            partition by security_id
            order by price_date
            rows between 19 preceding and current row
        ) as ma_20,
        avg(close_price) over (
            partition by security_id
            order by price_date
            rows between 49 preceding and current row
        ) as ma_50,
        avg(close_price) over (
            partition by security_id
            order by price_date
            rows between 199 preceding and current row
        ) as ma_200,
        stddev(close_price) over (
            partition by security_id
            order by price_date
            rows between 19 preceding and current row
        ) as volatility_20d,
        avg(volume) over (
            partition by security_id
            order by price_date
            rows between 19 preceding and current row
        ) as avg_volume_20d
    from with_prior_day
),

-- ISSUE: Another pass for more calculations
final as (
    select
        *,
        case
            when ma_20 > ma_50 and ma_50 > ma_200 then 'BULLISH'
            when ma_20 < ma_50 and ma_50 < ma_200 then 'BEARISH'
            else 'NEUTRAL'
        end as trend_signal,
        case
            when volume > avg_volume_20d * 2 then 'HIGH'
            when volume < avg_volume_20d * 0.5 then 'LOW'
            else 'NORMAL'
        end as volume_signal
    from with_returns
)

select * from final
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_market_prices"} */;
[0m21:50:19.797791 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.stg_portfolios: create or replace   view DBT_DEMO.DEV_pipeline_a.stg_portfolios
  
  
  
  
  as (
    -- Pipeline A: Simple Cashflow Pipeline
-- Model: stg_portfolios
-- Description: Staging model for portfolio master data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Subquery for deduplication instead of QUALIFY
-- 2. Multiple passes over data

with source as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        inception_date,
        status,
        currency,
        created_at,
        updated_at,
        row_number() over (
            partition by portfolio_id
            order by updated_at desc
        ) as rn
    from DBT_DEMO.DEV.portfolios
),

-- ISSUE: Using subquery filter instead of QUALIFY
deduplicated as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        inception_date,
        status,
        currency,
        created_at,
        updated_at
    from source
    where rn = 1  -- ISSUE: Should use QUALIFY in Snowflake
),

-- ISSUE: Another pass just for active filter
active_only as (
    select *
    from deduplicated
    where status = 'ACTIVE'
)

select * from active_only
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_portfolios"} */;
[0m21:50:20.146174 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.353 seconds
[0m21:50:20.173437 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 0.375 seconds
[0m21:50:20.174229 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.377 seconds
[0m21:50:20.177789 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ccd7fef-a1e2-400f-ba9b-44748b691e9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11377ce80>]}
[0m21:50:20.178185 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ccd7fef-a1e2-400f-ba9b-44748b691e9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1132aa160>]}
[0m21:50:20.178628 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ccd7fef-a1e2-400f-ba9b-44748b691e9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11377cfd0>]}
[0m21:50:20.180325 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 0.381 seconds
[0m21:50:20.179588 [info ] [Thread-1  ]: 1 of 12 OK created sql view model DEV_pipeline_b.stg_brokers ................... [[32mSUCCESS 1[0m in 0.45s]
[0m21:50:20.180941 [info ] [Thread-3  ]: 3 of 12 OK created sql view model DEV_pipeline_b.stg_market_prices ............. [[32mSUCCESS 1[0m in 0.45s]
[0m21:50:20.181447 [info ] [Thread-2  ]: 2 of 12 OK created sql view model DEV_pipeline_a.stg_cashflows ................. [[32mSUCCESS 1[0m in 0.46s]
[0m21:50:20.183099 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ccd7fef-a1e2-400f-ba9b-44748b691e9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1132510a0>]}
[0m21:50:20.183628 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.stg_brokers
[0m21:50:20.184115 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.stg_market_prices
[0m21:50:20.184542 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.stg_cashflows
[0m21:50:20.185045 [info ] [Thread-4  ]: 4 of 12 OK created sql view model DEV_pipeline_a.stg_portfolios ................ [[32mSUCCESS 1[0m in 0.46s]
[0m21:50:20.185436 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.stg_securities
[0m21:50:20.185842 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.stg_trades
[0m21:50:20.186397 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.stg_portfolios
[0m21:50:20.186808 [info ] [Thread-1  ]: 5 of 12 START sql view model DEV_pipeline_b.stg_securities ..................... [RUN]
[0m21:50:20.187213 [info ] [Thread-3  ]: 6 of 12 START sql view model DEV_pipeline_b.stg_trades ......................... [RUN]
[0m21:50:20.187729 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_brokers, now model.bain_capital_portfolio_analytics.stg_securities)
[0m21:50:20.188048 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m21:50:20.188374 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_market_prices, now model.bain_capital_portfolio_analytics.stg_trades)
[0m21:50:20.188678 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_securities
[0m21:50:20.189053 [info ] [Thread-2  ]: 7 of 12 START sql table model DEV_pipeline_a.fact_cashflow_summary ............. [RUN]
[0m21:50:20.189373 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_trades
[0m21:50:20.193848 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_securities"
[0m21:50:20.194286 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_cashflows, now model.bain_capital_portfolio_analytics.fact_cashflow_summary)
[0m21:50:20.197675 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_trades"
[0m21:50:20.198183 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m21:50:20.205066 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.stg_securities
[0m21:50:20.207959 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_securities"
[0m21:50:20.214766 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.stg_trades
[0m21:50:20.216904 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_cashflow_summary"
[0m21:50:20.219994 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_trades"
[0m21:50:20.221894 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_securities"
[0m21:50:20.222303 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.stg_securities: create or replace   view DBT_DEMO.DEV_pipeline_b.stg_securities
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: stg_securities
-- Description: Staging model for security master data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Nested subqueries instead of QUALIFY
-- 2. Multiple deduplication passes

with source as (
    select
        security_id,
        ticker,
        security_name,
        security_type,
        asset_class,
        sector,
        industry,
        currency,
        exchange,
        is_active,
        created_at,
        updated_at
    from DBT_DEMO.DEV.securities
),

-- ISSUE: Complex deduplication using subquery
deduplicated as (
    select *
    from (
        select
            *,
            row_number() over (
                partition by security_id
                order by updated_at desc
            ) as rn
        from source
    ) sub
    where rn = 1  -- ISSUE: Should use QUALIFY
),

-- ISSUE: Separate CTE for type standardization
standardized as (
    select
        security_id,
        upper(trim(ticker)) as ticker,
        trim(security_name) as security_name,
        -- ISSUE: Repeated CASE logic found in other models
        case
            when security_type in ('STOCK', 'EQUITY', 'COMMON') then 'EQUITY'
            when security_type in ('BOND', 'NOTE', 'DEBENTURE') then 'FIXED_INCOME'
            when security_type in ('OPTION', 'FUTURE', 'SWAP') then 'DERIVATIVE'
            when security_type in ('ETF', 'MUTUAL_FUND') then 'FUND'
            else 'OTHER'
        end as security_type_standardized,
        security_type as security_type_original,
        upper(asset_class) as asset_class,
        sector,
        industry,
        upper(currency) as currency,
        exchange,
        is_active,
        created_at,
        updated_at
    from deduplicated
)

select * from standardized
where is_active = true
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_securities"} */;
[0m21:50:20.224180 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_trades"
[0m21:50:20.224778 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.stg_trades: create or replace   view DBT_DEMO.DEV_pipeline_b.stg_trades
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: stg_trades
-- Description: Staging model for trade transactions
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Complex CASE statements that repeat
-- 2. Multiple CTEs doing similar transformations
-- 3. Unnecessary string operations

with source as (
    select
        trade_id,
        portfolio_id,
        security_id,
        broker_id,
        trade_date,
        settlement_date,
        trade_type,
        quantity,
        price,
        gross_amount,
        commission,
        fees,
        net_amount,
        currency,
        created_at,
        updated_at
    from DBT_DEMO.DEV.trades
),

-- ISSUE: Repeated CASE statements for trade categorization
categorized as (
    select
        *,
        -- ISSUE: This logic is repeated in multiple models
        case
            when trade_type in ('BUY', 'COVER') then 'PURCHASE'
            when trade_type in ('SELL', 'SHORT') then 'SALE'
            when trade_type in ('DIVIDEND', 'INTEREST') then 'INCOME'
            else 'OTHER'
        end as trade_category,
        case
            when abs(net_amount) >= 10000000 then 'LARGE'
            when abs(net_amount) >= 1000000 then 'MEDIUM'
            when abs(net_amount) >= 100000 then 'SMALL'
            else 'MICRO'
        end as trade_size_bucket,
        -- ISSUE: Redundant string manipulation
        upper(trim(trade_type)) as trade_type_clean,
        upper(trim(currency)) as currency_clean
    from source
),

-- ISSUE: Another pass just for date calculations
with_dates as (
    select
        *,
        datediff('day', trade_date, settlement_date) as settlement_days,
        date_trunc('month', trade_date) as trade_month,
        date_trunc('quarter', trade_date) as trade_quarter,
        extract(year from trade_date) as trade_year,
        extract(month from trade_date) as trade_month_num,
        dayofweek(trade_date) as trade_day_of_week
    from categorized
),

-- ISSUE: Late filtering
filtered as (
    select *
    from with_dates
    where trade_date >= '2020-01-01'
)

select * from filtered
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_trades"} */;
[0m21:50:20.225186 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m21:50:20.248406 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_cashflow_summary"
[0m21:50:20.257593 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_cashflow_summary"
[0m21:50:20.258318 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.fact_cashflow_summary: create or replace transient table DBT_DEMO.DEV_pipeline_a.fact_cashflow_summary
    
    
    
    as (-- Pipeline A: Simple Cashflow Pipeline
-- Model: fact_cashflow_summary
-- Description: Fact table summarizing cashflows by portfolio and month
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-joins for prior period comparisons (should use LAG)
-- 2. Late aggregation (aggregates after full join)
-- 3. Repeated window functions with same partitions
-- 4. Redundant date calculations per row
-- 5. Correlated subqueries for fund-level totals

with cashflows as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_cashflows
),

portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

-- ISSUE: Full join before aggregation (scans all rows)
joined as (
    select
        c.cashflow_id,
        c.portfolio_id,
        p.portfolio_name,
        p.portfolio_type,
        p.fund_id,
        c.cashflow_type,
        c.cashflow_date,
        c.amount,
        c.currency,
        -- ISSUE: Redundant date calculations done per row
        date_trunc('month', c.cashflow_date) as cashflow_month,
        date_trunc('quarter', c.cashflow_date) as cashflow_quarter,
        date_trunc('year', c.cashflow_date) as cashflow_year,
        extract(year from c.cashflow_date) as year_num,
        extract(month from c.cashflow_date) as month_num,
        extract(quarter from c.cashflow_date) as quarter_num,
        extract(dayofmonth from c.cashflow_date) as day_num
    from cashflows c
    inner join portfolios p
        on c.portfolio_id = p.portfolio_id
),

-- ISSUE: Aggregation happens after full row-level join
aggregated as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        cashflow_month,
        cashflow_quarter,
        cashflow_year,
        year_num,
        month_num,
        quarter_num,
        cashflow_type,
        currency,
        count(*) as transaction_count,
        count(distinct cashflow_id) as unique_transactions,
        sum(amount) as total_amount,
        avg(amount) as avg_amount,
        min(amount) as min_amount,
        max(amount) as max_amount,
        stddev(amount) as stddev_amount,
        -- ISSUE: Percentile calculations (slow)
        percentile_cont(0.25) within group (order by amount) as p25_amount,
        percentile_cont(0.50) within group (order by amount) as median_amount,
        percentile_cont(0.75) within group (order by amount) as p75_amount
    from joined
    group by 1,2,3,4,5,6,7,8,9,10,11,12  -- ISSUE: Non-descriptive GROUP BY
),

-- ISSUE: Self-join for prior month comparisons (should use LAG)
with_prior_months as (
    select
        agg.*,
        -- ISSUE: Self-join for prior month
        agg_m1.total_amount as prior_1m_total,
        agg_m1.transaction_count as prior_1m_count,
        -- ISSUE: Self-join for 3 months ago
        agg_m3.total_amount as prior_3m_total,
        -- ISSUE: Self-join for 6 months ago
        agg_m6.total_amount as prior_6m_total,
        -- ISSUE: Self-join for 12 months ago
        agg_m12.total_amount as prior_12m_total
    from aggregated agg
    left join aggregated agg_m1
        on agg.portfolio_id = agg_m1.portfolio_id
        and agg.cashflow_type = agg_m1.cashflow_type
        and agg.currency = agg_m1.currency
        and agg_m1.cashflow_month = dateadd(month, -1, agg.cashflow_month)
    left join aggregated agg_m3
        on agg.portfolio_id = agg_m3.portfolio_id
        and agg.cashflow_type = agg_m3.cashflow_type
        and agg.currency = agg_m3.currency
        and agg_m3.cashflow_month = dateadd(month, -3, agg.cashflow_month)
    left join aggregated agg_m6
        on agg.portfolio_id = agg_m6.portfolio_id
        and agg.cashflow_type = agg_m6.cashflow_type
        and agg.currency = agg_m6.currency
        and agg_m6.cashflow_month = dateadd(month, -6, agg.cashflow_month)
    left join aggregated agg_m12
        on agg.portfolio_id = agg_m12.portfolio_id
        and agg.cashflow_type = agg_m12.cashflow_type
        and agg.currency = agg_m12.currency
        and agg_m12.cashflow_month = dateadd(month, -12, agg.cashflow_month)
),

-- ISSUE: Multiple window functions with repeated partitions
with_window_calcs as (
    select
        wpm.*,
        -- ISSUE: Running totals (repeated partition)
        sum(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_total,
        sum(transaction_count) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_count,
        -- ISSUE: Moving averages (same partition repeated)
        avg(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 2 preceding and current row
        ) as rolling_3m_avg,
        avg(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 5 preceding and current row
        ) as rolling_6m_avg,
        avg(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_avg,
        -- ISSUE: More window calculations
        stddev(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_stddev,
        min(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_min,
        max(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_max
    from with_prior_months wpm
),

-- ISSUE: Correlated subqueries for fund-level context (very slow)
with_fund_context as (
    select
        wwc.*,
        -- ISSUE: Correlated subquery for fund total
        (
            select sum(total_amount)
            from aggregated agg2
            inner join portfolios p2
                on agg2.portfolio_id = p2.portfolio_id
            where p2.fund_id = wwc.fund_id
            and agg2.cashflow_month = wwc.cashflow_month
            and agg2.cashflow_type = wwc.cashflow_type
        ) as fund_total_amount,
        -- ISSUE: Another correlated subquery for portfolio count
        (
            select count(distinct agg2.portfolio_id)
            from aggregated agg2
            inner join portfolios p2
                on agg2.portfolio_id = p2.portfolio_id
            where p2.fund_id = wwc.fund_id
            and agg2.cashflow_month = wwc.cashflow_month
            and agg2.cashflow_type = wwc.cashflow_type
        ) as fund_portfolio_count
    from with_window_calcs wwc
),

-- ISSUE: Complex derived metrics with repeated CASE statements
final as (
    select
        md5(cast(coalesce(cast(wfc.portfolio_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wfc.cashflow_month as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wfc.cashflow_type as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wfc.currency as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as cashflow_summary_key,
        wfc.*,
        -- ISSUE: Portfolio share of fund (repeated division)
        case
            when wfc.fund_total_amount > 0
            then (wfc.total_amount / wfc.fund_total_amount) * 100
            else null
        end as portfolio_share_of_fund_pct,
        -- ISSUE: Month-over-month growth calculations
        case
            when wfc.prior_1m_total is not null and wfc.prior_1m_total != 0
            then ((wfc.total_amount - wfc.prior_1m_total) / abs(wfc.prior_1m_total)) * 100
            else null
        end as mom_growth_pct,
        case
            when wfc.prior_3m_total is not null and wfc.prior_3m_total != 0
            then ((wfc.total_amount - wfc.prior_3m_total) / abs(wfc.prior_3m_total)) * 100
            else null
        end as growth_3m_pct,
        case
            when wfc.prior_12m_total is not null and wfc.prior_12m_total != 0
            then ((wfc.total_amount - wfc.prior_12m_total) / abs(wfc.prior_12m_total)) * 100
            else null
        end as yoy_growth_pct,
        -- ISSUE: Trend classification (complex nested CASE)
        case
            when wfc.rolling_3m_avg > wfc.rolling_12m_avg * 1.3 then 'ACCELERATING'
            when wfc.rolling_3m_avg > wfc.rolling_12m_avg * 1.1 then 'GROWING'
            when wfc.rolling_3m_avg < wfc.rolling_12m_avg * 0.7 then 'DECLINING_FAST'
            when wfc.rolling_3m_avg < wfc.rolling_12m_avg * 0.9 then 'DECLINING'
            else 'STABLE'
        end as trend_classification,
        -- ISSUE: Volatility classification
        case
            when wfc.rolling_12m_stddev < wfc.rolling_12m_avg * 0.1 then 'LOW_VOLATILITY'
            when wfc.rolling_12m_stddev < wfc.rolling_12m_avg * 0.3 then 'MODERATE_VOLATILITY'
            when wfc.rolling_12m_stddev < wfc.rolling_12m_avg * 0.5 then 'HIGH_VOLATILITY'
            else 'VERY_HIGH_VOLATILITY'
        end as volatility_classification,
        -- ISSUE: Size classification (repeated CASE)
        case
            when abs(wfc.total_amount) >= 10000000 then 'MEGA'
            when abs(wfc.total_amount) >= 5000000 then 'LARGE'
            when abs(wfc.total_amount) >= 1000000 then 'MEDIUM'
            when abs(wfc.total_amount) >= 100000 then 'SMALL'
            else 'MICRO'
        end as transaction_size_category
    from with_fund_context wfc
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_cashflow_summary"} */;
[0m21:50:20.682861 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.460 seconds
[0m21:50:20.687284 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ccd7fef-a1e2-400f-ba9b-44748b691e9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11332e130>]}
[0m21:50:20.688518 [info ] [Thread-1  ]: 5 of 12 OK created sql view model DEV_pipeline_b.stg_securities ................ [[32mSUCCESS 1[0m in 0.50s]
[0m21:50:20.689351 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.stg_securities
[0m21:50:20.692474 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 0.467 seconds
[0m21:50:20.695353 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ccd7fef-a1e2-400f-ba9b-44748b691e9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113508370>]}
[0m21:50:20.696132 [info ] [Thread-3  ]: 6 of 12 OK created sql view model DEV_pipeline_b.stg_trades .................... [[32mSUCCESS 1[0m in 0.51s]
[0m21:50:20.696706 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.stg_trades
[0m21:50:20.697370 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.int_trades_enriched
[0m21:50:20.697858 [info ] [Thread-4  ]: 8 of 12 START sql view model DEV_pipeline_b.int_trades_enriched ................ [RUN]
[0m21:50:20.698301 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_portfolios, now model.bain_capital_portfolio_analytics.int_trades_enriched)
[0m21:50:20.698647 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.int_trades_enriched
[0m21:50:20.704596 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_trades_enriched"
[0m21:50:20.705627 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.int_trades_enriched
[0m21:50:20.709019 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_trades_enriched"
[0m21:50:20.711774 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_trades_enriched"
[0m21:50:20.712228 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.int_trades_enriched: create or replace   view DBT_DEMO.DEV_pipeline_b.int_trades_enriched
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: int_trades_enriched
-- Description: Intermediate model enriching trades with security and price data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Multiple heavy joins done row-by-row
-- 2. Price lookup repeated for every trade
-- 3. Could pre-aggregate before joining

with trades as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_trades
),

securities as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_securities
),

market_prices as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_market_prices
),

brokers as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_brokers
),

-- ISSUE: Heavy multi-way join before any aggregation
enriched as (
    select
        t.trade_id,
        t.portfolio_id,
        t.security_id,
        t.trade_date,
        t.settlement_date,
        t.trade_type,
        t.trade_category,
        t.trade_size_bucket,
        t.quantity,
        t.price as execution_price,
        t.gross_amount,
        t.commission,
        t.fees,
        t.net_amount,
        t.currency,
        t.settlement_days,
        t.trade_month,
        t.trade_quarter,
        t.trade_year,
        -- Security attributes
        s.ticker,
        s.security_name,
        s.security_type_standardized as security_type,
        s.asset_class,
        s.sector,
        s.industry,
        -- Broker attributes
        b.broker_name,
        b.broker_type,
        b.region as broker_region,
        b.commission_rate as standard_commission_rate,
        -- Market price on trade date
        mp.close_price as market_close_price,
        mp.ma_20,
        mp.ma_50,
        mp.volatility_20d,
        mp.trend_signal,
        mp.volume_signal,
        -- ISSUE: These calculations done per row
        case
            when mp.close_price > 0
            then (t.price - mp.close_price) / mp.close_price * 100
            else null
        end as execution_vs_close_pct,
        case
            when t.price > mp.close_price then 'ABOVE_MARKET'
            when t.price < mp.close_price then 'BELOW_MARKET'
            else 'AT_MARKET'
        end as execution_quality,
        -- Cost analysis
        t.commission + t.fees as total_costs,
        case
            when t.gross_amount > 0
            then (t.commission + t.fees) / t.gross_amount * 10000
            else null
        end as cost_bps
    from trades t
    inner join securities s
        on t.security_id = s.security_id
    left join brokers b
        on t.broker_id = b.broker_id
    left join market_prices mp
        on t.security_id = mp.security_id
        and t.trade_date = mp.price_date
)

select * from enriched
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_trades_enriched"} */;
[0m21:50:21.471696 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 0.759 seconds
[0m21:50:21.476046 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ccd7fef-a1e2-400f-ba9b-44748b691e9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11352c190>]}
[0m21:50:21.477100 [info ] [Thread-4  ]: 8 of 12 OK created sql view model DEV_pipeline_b.int_trades_enriched ........... [[32mSUCCESS 1[0m in 0.78s]
[0m21:50:21.477818 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.int_trades_enriched
[0m21:50:21.478626 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.int_trade_pnl
[0m21:50:21.479239 [info ] [Thread-3  ]: 9 of 12 START sql view model DEV_pipeline_b.int_trade_pnl ...................... [RUN]
[0m21:50:21.479797 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_trades, now model.bain_capital_portfolio_analytics.int_trade_pnl)
[0m21:50:21.480182 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.int_trade_pnl
[0m21:50:21.486883 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_trade_pnl"
[0m21:50:21.488027 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.int_trade_pnl
[0m21:50:21.491683 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_trade_pnl"
[0m21:50:21.494720 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_trade_pnl"
[0m21:50:21.495209 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.int_trade_pnl: create or replace   view DBT_DEMO.DEV_pipeline_b.int_trade_pnl
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: int_trade_pnl
-- Description: Calculate P&L for each trade
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Complex position tracking logic that could be simplified
-- 2. Multiple self-joins for cost basis calculation
-- 3. Window functions recalculated multiple times

with trades as (
    select * from DBT_DEMO.DEV_pipeline_b.int_trades_enriched
),

-- ISSUE: Running position calculation done inefficiently
positions as (
    select
        trade_id,
        portfolio_id,
        security_id,
        ticker,
        security_name,
        security_type,
        asset_class,
        sector,
        industry,
        trade_date,
        trade_type,
        trade_category,
        quantity,
        execution_price,
        net_amount,
        commission,
        -- ISSUE: Multiple window functions with same partition
        sum(case
            when trade_category = 'PURCHASE' then quantity
            when trade_category = 'SALE' then -quantity
            else 0
        end) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as running_position,
        sum(case
            when trade_category = 'PURCHASE' then net_amount
            when trade_category = 'SALE' then -net_amount
            else 0
        end) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as cumulative_cost,
        -- ISSUE: Another separate window for purchase-only
        sum(case when trade_category = 'PURCHASE' then quantity else 0 end) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as cumulative_purchased_qty,
        sum(case when trade_category = 'PURCHASE' then net_amount else 0 end) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as cumulative_purchase_cost
    from trades
),

-- ISSUE: Separate CTE for cost basis
with_cost_basis as (
    select
        *,
        case
            when cumulative_purchased_qty > 0
            then cumulative_purchase_cost / cumulative_purchased_qty
            else null
        end as avg_cost_basis
    from positions
),

-- ISSUE: Another pass for realized P&L
with_pnl as (
    select
        *,
        case
            when trade_category = 'SALE' and avg_cost_basis is not null
            then (execution_price - avg_cost_basis) * quantity
            else null
        end as realized_pnl,
        case
            when trade_category = 'SALE' and avg_cost_basis is not null and avg_cost_basis > 0
            then (execution_price - avg_cost_basis) / avg_cost_basis * 100
            else null
        end as realized_pnl_pct
    from with_cost_basis
)

select * from with_pnl
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_trade_pnl"} */;
[0m21:50:21.959297 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 0.463 seconds
[0m21:50:21.963541 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ccd7fef-a1e2-400f-ba9b-44748b691e9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113370e80>]}
[0m21:50:21.964694 [info ] [Thread-3  ]: 9 of 12 OK created sql view model DEV_pipeline_b.int_trade_pnl ................. [[32mSUCCESS 1[0m in 0.48s]
[0m21:50:21.965445 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.int_trade_pnl
[0m21:50:21.966301 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.fact_trade_summary
[0m21:50:21.966979 [info ] [Thread-4  ]: 10 of 12 START sql table model DEV_pipeline_b.fact_trade_summary ............... [RUN]
[0m21:50:21.967599 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_trades_enriched, now model.bain_capital_portfolio_analytics.fact_trade_summary)
[0m21:50:21.968130 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_trade_summary
[0m21:50:21.976499 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_trade_summary"
[0m21:50:21.977689 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.fact_trade_summary
[0m21:50:21.981420 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_trade_summary"
[0m21:50:21.989693 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_trade_summary"
[0m21:50:21.990363 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.fact_trade_summary: create or replace transient table DBT_DEMO.DEV_pipeline_b.fact_trade_summary
    
    
    
    as (-- Pipeline B: Trade Analytics Pipeline
-- Model: fact_trade_summary
-- Description: Fact table for trade-level analysis
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-joins for prior trade lookups (should use LAG)
-- 2. Repeated window functions with same partitions
-- 3. Correlated subqueries for security-level aggregations
-- 4. Complex CASE statements repeated multiple times

with trade_pnl as (
    select * from DBT_DEMO.DEV_pipeline_b.int_trade_pnl
),

-- ISSUE: Getting portfolio data again (already available through joins upstream)
portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

-- ISSUE: Join that adds overhead
enriched as (
    select
        t.trade_id,
        t.portfolio_id,
        p.portfolio_name,
        p.portfolio_type,
        p.fund_id,
        t.security_id,
        t.ticker,
        t.security_name,
        t.security_type,
        t.asset_class,
        t.sector,
        t.trade_date,
        t.trade_type,
        t.trade_category,
        t.quantity,
        t.execution_price,
        t.net_amount,
        t.commission,
        t.running_position,
        t.avg_cost_basis,
        t.realized_pnl,
        t.realized_pnl_pct,
        -- ISSUE: Redundant date extractions (already done upstream)
        extract(year from t.trade_date) as trade_year,
        extract(month from t.trade_date) as trade_month,
        extract(quarter from t.trade_date) as trade_quarter,
        extract(dayofweek from t.trade_date) as trade_day_of_week,
        date_trunc('week', t.trade_date) as trade_week,
        date_trunc('month', t.trade_date) as trade_month_start
    from trade_pnl t
    left join portfolios p
        on t.portfolio_id = p.portfolio_id
),

-- ISSUE: Self-joins for prior trade comparisons (should use LAG)
-- Pre-compute trade sequence for self-join lookups
trade_sequences as (
    select
        *,
        row_number() over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
        ) as trade_seq
    from enriched
),

with_prior_trades as (
    select
        ts.*,
        -- ISSUE: Self-join for prior trade same security (should use LAG)
        ts_prior.execution_price as prior_trade_price,
        ts_prior.trade_date as prior_trade_date,
        ts_prior.quantity as prior_trade_quantity,
        -- ISSUE: Self-join for 5 trades ago (should use LAG offset)
        ts_5.execution_price as price_5_trades_ago,
        -- ISSUE: Self-join for 10 trades ago (should use LAG offset)
        ts_10.execution_price as price_10_trades_ago
    from trade_sequences ts
    left join trade_sequences ts_prior
        on ts.portfolio_id = ts_prior.portfolio_id
        and ts.security_id = ts_prior.security_id
        and ts_prior.trade_seq = ts.trade_seq - 1
    left join trade_sequences ts_5
        on ts.portfolio_id = ts_5.portfolio_id
        and ts.security_id = ts_5.security_id
        and ts_5.trade_seq = ts.trade_seq - 5
    left join trade_sequences ts_10
        on ts.portfolio_id = ts_10.portfolio_id
        and ts.security_id = ts_10.security_id
        and ts_10.trade_seq = ts.trade_seq - 10
),

-- ISSUE: Multiple window functions with repeated partitions
with_window_calcs as (
    select
        wpt.*,
        -- ISSUE: Running aggregations (repeated partition by portfolio_id, security_id)
        sum(quantity) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between unbounded preceding and current row
        ) as cumulative_quantity,
        sum(abs(net_amount)) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between unbounded preceding and current row
        ) as cumulative_trade_value,
        sum(coalesce(realized_pnl, 0)) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between unbounded preceding and current row
        ) as cumulative_realized_pnl,
        -- ISSUE: Moving averages (same partition repeated)
        avg(execution_price) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between 4 preceding and current row
        ) as rolling_5_trade_avg_price,
        avg(execution_price) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between 9 preceding and current row
        ) as rolling_10_trade_avg_price,
        avg(execution_price) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between 19 preceding and current row
        ) as rolling_20_trade_avg_price,
        -- ISSUE: More window calculations
        stddev(execution_price) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between 9 preceding and current row
        ) as rolling_10_trade_price_stddev,
        count(*) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between unbounded preceding and current row
        ) as trade_sequence_number,
        -- ISSUE: Rankings (same partition again)
        row_number() over (
            partition by wpt.portfolio_id, wpt.security_id, wpt.trade_category
            order by abs(wpt.net_amount) desc
        ) as size_rank_within_category
    from with_prior_trades wpt
),

-- ISSUE: Separate CTE for running trade stats (should be combined with window calcs above)
security_trade_aggs as (
    select
        portfolio_id,
        security_id,
        trade_date,
        trade_id,
        -- ISSUE: These window functions duplicate the partition from with_window_calcs
        count(*) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as total_portfolio_trades_this_security,
        avg(execution_price) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as avg_portfolio_price_this_security
    from enriched
),

-- ISSUE: Separate aggregation for fund-level volume (should be combined upstream)
fund_daily_volume as (
    select
        fund_id,
        security_id,
        trade_date,
        sum(abs(net_amount)) as fund_total_volume_same_security_same_day
    from enriched
    group by 1, 2, 3
),

with_security_context as (
    select
        wwc.*,
        sta.total_portfolio_trades_this_security,
        sta.avg_portfolio_price_this_security,
        fdv.fund_total_volume_same_security_same_day
    from with_window_calcs wwc
    left join security_trade_aggs sta
        on wwc.portfolio_id = sta.portfolio_id
        and wwc.security_id = sta.security_id
        and wwc.trade_id = sta.trade_id
    left join fund_daily_volume fdv
        on wwc.fund_id = fdv.fund_id
        and wwc.security_id = fdv.security_id
        and wwc.trade_date = fdv.trade_date
),

-- ISSUE: Complex derived metrics with repeated CASE statements
final as (
    select
        md5(cast(coalesce(cast(wsc.trade_id as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as trade_key,
        wsc.*,
        -- ISSUE: Price change calculations (repeated division logic)
        case
            when wsc.prior_trade_price is not null and wsc.prior_trade_price > 0
            then ((wsc.execution_price - wsc.prior_trade_price) / wsc.prior_trade_price) * 100
            else null
        end as price_change_from_prior_pct,
        case
            when wsc.price_5_trades_ago is not null and wsc.price_5_trades_ago > 0
            then ((wsc.execution_price - wsc.price_5_trades_ago) / wsc.price_5_trades_ago) * 100
            else null
        end as price_change_from_5_trades_ago_pct,
        case
            when wsc.rolling_20_trade_avg_price is not null and wsc.rolling_20_trade_avg_price > 0
            then ((wsc.execution_price - wsc.rolling_20_trade_avg_price) / wsc.rolling_20_trade_avg_price) * 100
            else null
        end as deviation_from_20_trade_avg_pct,
        -- ISSUE: Trade size classification (repeated CASE)
        case
            when abs(wsc.net_amount) >= 10000000 then 'BLOCK_TRADE'
            when abs(wsc.net_amount) >= 1000000 then 'LARGE'
            when abs(wsc.net_amount) >= 100000 then 'MEDIUM'
            when abs(wsc.net_amount) >= 10000 then 'SMALL'
            else 'MICRO'
        end as trade_size_category,
        -- ISSUE: Trade timing classification (complex nested CASE)
        case
            when wsc.execution_price > wsc.rolling_10_trade_avg_price * 1.1 then 'BOUGHT_HIGH'
            when wsc.execution_price > wsc.rolling_10_trade_avg_price * 1.03 then 'ABOVE_AVERAGE'
            when wsc.execution_price < wsc.rolling_10_trade_avg_price * 0.9 then 'BOUGHT_LOW'
            when wsc.execution_price < wsc.rolling_10_trade_avg_price * 0.97 then 'BELOW_AVERAGE'
            else 'AVERAGE'
        end as execution_quality,
        -- ISSUE: Momentum signal (repeated logic)
        case
            when wsc.rolling_5_trade_avg_price > wsc.rolling_20_trade_avg_price then 'UPTREND'
            when wsc.rolling_5_trade_avg_price < wsc.rolling_20_trade_avg_price then 'DOWNTREND'
            else 'NEUTRAL'
        end as price_momentum,
        -- ISSUE: Volatility classification
        case
            when wsc.rolling_10_trade_price_stddev < wsc.rolling_10_trade_avg_price * 0.02 then 'LOW_VOLATILITY'
            when wsc.rolling_10_trade_price_stddev < wsc.rolling_10_trade_avg_price * 0.05 then 'MODERATE_VOLATILITY'
            when wsc.rolling_10_trade_price_stddev < wsc.rolling_10_trade_avg_price * 0.10 then 'HIGH_VOLATILITY'
            else 'VERY_HIGH_VOLATILITY'
        end as price_volatility_regime,
        -- ISSUE: Trade frequency indicator
        case
            when wsc.trade_sequence_number >= 100 then 'VERY_ACTIVE'
            when wsc.trade_sequence_number >= 50 then 'ACTIVE'
            when wsc.trade_sequence_number >= 20 then 'MODERATE'
            when wsc.trade_sequence_number >= 5 then 'LIGHT'
            else 'FIRST_FEW_TRADES'
        end as trading_activity_level
    from with_security_context wsc
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_trade_summary"} */;
[0m21:50:22.870172 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 3.096 seconds
[0m21:50:22.874496 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2848b3-059d-40ff-81f1-542874500440', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113708d90>]}
[0m21:50:22.875630 [info ] [Thread-4  ]: 3 of 4 OK created sql table model DEV_pipeline_a.fact_cashflow_summary ......... [[32mSUCCESS 1[0m in 3.15s]
[0m21:50:22.876390 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m21:50:22.877638 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.report_monthly_cashflows
[0m21:50:22.878362 [info ] [Thread-2  ]: 4 of 4 START sql table model DEV_pipeline_a.report_monthly_cashflows ........... [RUN]
[0m21:50:22.878919 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_portfolios, now model.bain_capital_portfolio_analytics.report_monthly_cashflows)
[0m21:50:22.879329 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.report_monthly_cashflows
[0m21:50:22.887149 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.report_monthly_cashflows"
[0m21:50:22.889218 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.report_monthly_cashflows
[0m21:50:22.892938 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.report_monthly_cashflows"
[0m21:50:22.896436 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.report_monthly_cashflows"
[0m21:50:22.896929 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.report_monthly_cashflows: create or replace transient table DBT_DEMO.DEV_pipeline_a.report_monthly_cashflows
    
    
    
    as (-- Pipeline A: Simple Cashflow Pipeline
-- Model: report_monthly_cashflows
-- Description: LP reporting view for monthly cashflow analysis
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Re-aggregates data that's already in fact table
-- 2. Repeated window functions
-- 3. Suboptimal pivot pattern

with fact_data as (
    select * from DBT_DEMO.DEV_pipeline_a.fact_cashflow_summary
),

-- ISSUE: Re-aggregating already aggregated data
monthly_totals as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        cashflow_month,
        year_num,
        month_num,
        sum(case when cashflow_type = 'CONTRIBUTION' then total_amount else 0 end) as contributions,
        sum(case when cashflow_type = 'DISTRIBUTION' then total_amount else 0 end) as distributions,
        sum(case when cashflow_type = 'DIVIDEND' then total_amount else 0 end) as dividends,
        sum(case when cashflow_type = 'FEE' then total_amount else 0 end) as fees,
        sum(total_amount) as total_cashflow,
        sum(transaction_count) as total_transactions
    from fact_data
    group by 1,2,3,4,5,6,7
),

-- ISSUE: Window functions recalculated multiple times
with_running_totals as (
    select
        *,
        -- Running totals (repeated pattern)
        sum(contributions) over (
            partition by portfolio_id
            order by cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_contributions,
        sum(distributions) over (
            partition by portfolio_id
            order by cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_distributions,
        sum(total_cashflow) over (
            partition by portfolio_id
            order by cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_net_cashflow,
        -- Prior period comparisons (another repeated pattern)
        lag(contributions, 1) over (partition by portfolio_id order by cashflow_month) as prior_month_contributions,
        lag(distributions, 1) over (partition by portfolio_id order by cashflow_month) as prior_month_distributions,
        lag(total_cashflow, 1) over (partition by portfolio_id order by cashflow_month) as prior_month_total,
        -- YoY comparison
        lag(contributions, 12) over (partition by portfolio_id order by cashflow_month) as prior_year_contributions,
        lag(distributions, 12) over (partition by portfolio_id order by cashflow_month) as prior_year_distributions
    from monthly_totals
),

-- ISSUE: Calculated columns that could be simplified
final as (
    select
        *,
        contributions - coalesce(prior_month_contributions, 0) as mom_contribution_change,
        distributions - coalesce(prior_month_distributions, 0) as mom_distribution_change,
        case
            when prior_year_contributions > 0
            then (contributions - prior_year_contributions) / prior_year_contributions * 100
            else null
        end as yoy_contribution_pct_change,
        contributions - distributions as net_inflow
    from with_running_totals
)

select * from final
order by portfolio_id, cashflow_month
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.report_monthly_cashflows"} */;
[0m21:50:23.716931 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 3.457 seconds
[0m21:50:23.722057 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ccd7fef-a1e2-400f-ba9b-44748b691e9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113325490>]}
[0m21:50:23.723224 [info ] [Thread-2  ]: 7 of 12 OK created sql table model DEV_pipeline_a.fact_cashflow_summary ........ [[32mSUCCESS 1[0m in 3.53s]
[0m21:50:23.723988 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m21:50:23.724855 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.fact_portfolio_positions
[0m21:50:23.725529 [info ] [Thread-3  ]: 11 of 12 START sql table model DEV_pipeline_b.fact_portfolio_positions ......... [RUN]
[0m21:50:23.726129 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_trade_pnl, now model.bain_capital_portfolio_analytics.fact_portfolio_positions)
[0m21:50:23.726603 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_portfolio_positions
[0m21:50:23.735677 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_portfolio_positions"
[0m21:50:23.736725 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.fact_portfolio_positions
[0m21:50:23.742120 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_portfolio_positions"
[0m21:50:23.749410 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_portfolio_positions"
[0m21:50:23.750083 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.fact_portfolio_positions: create or replace transient table DBT_DEMO.DEV_pipeline_b.fact_portfolio_positions
    
    
    
    as (-- Pipeline B: Trade Analytics Pipeline
-- Model: fact_portfolio_positions
-- Description: Current position snapshot by portfolio and security
-- DEPENDENCY: Uses fact_cashflow_summary from Pipeline A for portfolio cash context
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Gets latest position via subquery (should use QUALIFY)
-- 2. Self-joins for historical position lookups
-- 3. Correlated subqueries for portfolio-level aggregations
-- 4. Repeated window functions

with trade_pnl as (
    select * from DBT_DEMO.DEV_pipeline_b.int_trade_pnl
),

-- DEPENDENCY ON PIPELINE A: Get cashflow context for each portfolio
cashflow_summary as (
    select * from DBT_DEMO.DEV_pipeline_a.fact_cashflow_summary
),

-- Aggregate cashflows by portfolio to get total contributions/distributions
portfolio_cashflows as (
    select
        portfolio_id,
        sum(case when cashflow_type = 'CONTRIBUTION' then cumulative_total else 0 end) as total_contributions,
        sum(case when cashflow_type = 'DISTRIBUTION' then abs(cumulative_total) else 0 end) as total_distributions,
        max(cashflow_month) as last_cashflow_date
    from cashflow_summary
    group by portfolio_id
),

latest_positions as (
    select *
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id, security_id
                order by trade_date desc, trade_id desc
            ) as rn
        from trade_pnl
    )
    where rn = 1
),

-- ISSUE: Self-join to get position 30 days ago
positions_30d_ago as (
    select
        portfolio_id,
        security_id,
        running_position as position_30d_ago,
        avg_cost_basis as cost_basis_30d_ago
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id, security_id
                order by trade_date desc, trade_id desc
            ) as rn
        from trade_pnl
        where trade_date <= dateadd(day, -30, current_date())
    )
    where rn = 1
),

-- ISSUE: Self-join to get position 90 days ago
positions_90d_ago as (
    select
        portfolio_id,
        security_id,
        running_position as position_90d_ago,
        avg_cost_basis as cost_basis_90d_ago
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id, security_id
                order by trade_date desc, trade_id desc
            ) as rn
        from trade_pnl
        where trade_date <= dateadd(day, -90, current_date())
    )
    where rn = 1
),

market_prices as (
    select
        security_id,
        close_price as current_price,
        price_date
    from (
        select
            security_id,
            close_price,
            price_date,
            row_number() over (partition by security_id order by price_date desc) as rn
        from DBT_DEMO.DEV_pipeline_b.stg_market_prices
    )
    where rn = 1  -- ISSUE: Again, should use QUALIFY
),

-- ISSUE: Get historical prices for comparison
market_prices_30d_ago as (
    select
        security_id,
        close_price as price_30d_ago
    from (
        select
            security_id,
            close_price,
            row_number() over (partition by security_id order by price_date desc) as rn
        from DBT_DEMO.DEV_pipeline_b.stg_market_prices
        where price_date <= dateadd(day, -30, current_date())
    )
    where rn = 1
),

-- ISSUE: Join all the position snapshots together
enriched_positions as (
    select
        lp.*,
        mp.current_price,
        mp.price_date as price_as_of_date,
        p30.position_30d_ago,
        p30.cost_basis_30d_ago,
        p90.position_90d_ago,
        p90.cost_basis_90d_ago,
        mp30.price_30d_ago,
        pcf.total_contributions,
        pcf.total_distributions,
        pcf.last_cashflow_date
    from latest_positions lp
    left join market_prices mp
        on lp.security_id = mp.security_id
    left join positions_30d_ago p30
        on lp.portfolio_id = p30.portfolio_id
        and lp.security_id = p30.security_id
    left join positions_90d_ago p90
        on lp.portfolio_id = p90.portfolio_id
        and lp.security_id = p90.security_id
    left join market_prices_30d_ago mp30
        on lp.security_id = mp30.security_id
    left join portfolio_cashflows pcf
        on lp.portfolio_id = pcf.portfolio_id
    where lp.running_position != 0
),

-- ISSUE: Window functions for portfolio-level context
with_portfolio_context as (
    select
        ep.*,
        -- ISSUE: Repeated partition by portfolio_id
        sum(ep.running_position * ep.current_price) over (
            partition by ep.portfolio_id
        ) as portfolio_total_market_value,
        sum(ep.running_position * ep.avg_cost_basis) over (
            partition by ep.portfolio_id
        ) as portfolio_total_cost_basis,
        count(*) over (
            partition by ep.portfolio_id
        ) as portfolio_position_count,
        -- ISSUE: Rankings
        row_number() over (
            partition by ep.portfolio_id
            order by (ep.running_position * ep.current_price) desc
        ) as position_size_rank,
        row_number() over (
            partition by ep.portfolio_id
            order by ((ep.current_price - ep.avg_cost_basis) / nullif(ep.avg_cost_basis, 0)) desc
        ) as position_return_rank
    from enriched_positions ep
),

-- ISSUE: Separate aggregation for sector context (should use window functions)
sector_aggs as (
    select
        portfolio_id,
        sector,
        sum(running_position * current_price) as sector_market_value,
        count(*) as sector_position_count
    from enriched_positions
    group by 1, 2
),

with_sector_context as (
    select
        wpc.*,
        sa.sector_market_value,
        sa.sector_position_count
    from with_portfolio_context wpc
    left join sector_aggs sa
        on wpc.portfolio_id = sa.portfolio_id
        and wpc.sector = sa.sector
),

-- ISSUE: Complex calculations in final select
final as (
    select
        md5(cast(coalesce(cast(wsc.portfolio_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wsc.security_id as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as position_key,
        wsc.portfolio_id,
        wsc.security_id,
        wsc.ticker,
        wsc.security_name,
        wsc.sector,
        wsc.asset_class,
        wsc.running_position as current_quantity,
        wsc.avg_cost_basis,
        wsc.current_price,
        wsc.price_as_of_date,
        -- Core calculations
        wsc.running_position * wsc.avg_cost_basis as cost_basis_value,
        wsc.running_position * wsc.current_price as market_value,
        (wsc.running_position * wsc.current_price) - (wsc.running_position * wsc.avg_cost_basis) as unrealized_pnl,
        -- ISSUE: Repeated division logic
        case
            when wsc.avg_cost_basis > 0
            then ((wsc.current_price - wsc.avg_cost_basis) / wsc.avg_cost_basis) * 100
            else null
        end as unrealized_pnl_pct,
        -- Portfolio context
        wsc.portfolio_total_market_value,
        wsc.portfolio_total_cost_basis,
        wsc.portfolio_position_count,
        -- ISSUE: Weight calculation (repeated division)
        case
            when wsc.portfolio_total_market_value > 0
            then ((wsc.running_position * wsc.current_price) / wsc.portfolio_total_market_value) * 100
            else null
        end as portfolio_weight_pct,
        -- Sector context
        wsc.sector_market_value,
        wsc.sector_position_count,
        case
            when wsc.sector_market_value > 0
            then ((wsc.running_position * wsc.current_price) / wsc.sector_market_value) * 100
            else null
        end as sector_weight_pct,
        -- Historical comparison
        wsc.position_30d_ago,
        wsc.position_90d_ago,
        wsc.position_30d_ago - wsc.running_position as position_change_30d,
        wsc.position_90d_ago - wsc.running_position as position_change_90d,
        -- Price momentum
        wsc.price_30d_ago,
        case
            when wsc.price_30d_ago > 0
            then ((wsc.current_price - wsc.price_30d_ago) / wsc.price_30d_ago) * 100
            else null
        end as price_change_30d_pct,
        -- Cashflow context from Pipeline A
        wsc.total_contributions,
        wsc.total_distributions,
        wsc.last_cashflow_date,
        -- Rankings
        wsc.position_size_rank,
        wsc.position_return_rank,
        -- ISSUE: Complex classification
        case
            when ((wsc.running_position * wsc.current_price) / nullif(wsc.portfolio_total_market_value, 0)) > 0.10 then 'CONCENTRATED'
            when ((wsc.running_position * wsc.current_price) / nullif(wsc.portfolio_total_market_value, 0)) > 0.05 then 'SIGNIFICANT'
            when ((wsc.running_position * wsc.current_price) / nullif(wsc.portfolio_total_market_value, 0)) > 0.02 then 'MODERATE'
            else 'SMALL'
        end as position_size_category,
        wsc.cumulative_purchase_cost as total_invested,
        current_timestamp() as snapshot_timestamp
    from with_sector_context wsc
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_portfolio_positions"} */;
[0m21:50:23.837502 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.940 seconds
[0m21:50:23.841472 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2848b3-059d-40ff-81f1-542874500440', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ce62d00>]}
[0m21:50:23.842422 [info ] [Thread-2  ]: 4 of 4 OK created sql table model DEV_pipeline_a.report_monthly_cashflows ...... [[32mSUCCESS 1[0m in 0.96s]
[0m21:50:23.843150 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.report_monthly_cashflows
[0m21:50:23.844659 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:50:23.844983 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.stg_cashflows' was left open.
[0m21:50:23.845287 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.stg_cashflows: Close
[0m21:50:24.136562 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.report_monthly_cashflows' was left open.
[0m21:50:24.137441 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.report_monthly_cashflows: Close
[0m21:50:24.499842 [debug] [MainThread]: Connection 'list_DBT_DEMO_DEV_pipeline_c' was left open.
[0m21:50:24.500794 [debug] [MainThread]: On list_DBT_DEMO_DEV_pipeline_c: Close
[0m21:50:24.951815 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.fact_cashflow_summary' was left open.
[0m21:50:24.952811 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.fact_cashflow_summary: Close
[0m21:50:25.321144 [info ] [MainThread]: 
[0m21:50:25.321575 [info ] [MainThread]: Finished running 2 table models, 2 view models in 0 hours 0 minutes and 11.26 seconds (11.26s).
[0m21:50:25.322482 [debug] [MainThread]: Command end result
[0m21:50:25.376194 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/manifest.json
[0m21:50:25.378086 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/semantic_manifest.json
[0m21:50:25.384328 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/nedazarei/Documents/turintech/dbtproject/target/run_results.json
[0m21:50:25.384626 [info ] [MainThread]: 
[0m21:50:25.384959 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:50:25.385229 [info ] [MainThread]: 
[0m21:50:25.385529 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m21:50:25.387916 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 12.619603, "process_in_blocks": "0", "process_kernel_time": 0.39864, "process_mem_max_rss": "185745408", "process_out_blocks": "0", "process_user_time": 3.215574}
[0m21:50:25.388270 [debug] [MainThread]: Command `dbt run` succeeded at 21:50:25.388198 after 12.62 seconds
[0m21:50:25.388587 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109df01c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a78cf70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c2e8400>]}
[0m21:50:25.388918 [debug] [MainThread]: Flushing usage events
[0m21:50:26.334645 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:50:33.249069 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 9.498 seconds
[0m21:50:33.253426 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ccd7fef-a1e2-400f-ba9b-44748b691e9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113370310>]}
[0m21:50:33.254449 [info ] [Thread-3  ]: 11 of 12 OK created sql table model DEV_pipeline_b.fact_portfolio_positions .... [[32mSUCCESS 1[0m in 9.53s]
[0m21:50:33.255113 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.fact_portfolio_positions
[0m21:50:59.901776 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 37.910 seconds
[0m21:50:59.906233 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ccd7fef-a1e2-400f-ba9b-44748b691e9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11352c1c0>]}
[0m21:50:59.907366 [info ] [Thread-4  ]: 10 of 12 OK created sql table model DEV_pipeline_b.fact_trade_summary .......... [[32mSUCCESS 1[0m in 37.94s]
[0m21:50:59.908102 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.fact_trade_summary
[0m21:50:59.909221 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.report_trading_performance
[0m21:50:59.909955 [info ] [Thread-2  ]: 12 of 12 START sql table model DEV_pipeline_b.report_trading_performance ....... [RUN]
[0m21:50:59.910574 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.fact_cashflow_summary, now model.bain_capital_portfolio_analytics.report_trading_performance)
[0m21:50:59.910987 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.report_trading_performance
[0m21:50:59.917805 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.report_trading_performance"
[0m21:50:59.919773 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.report_trading_performance
[0m21:50:59.923690 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.report_trading_performance"
[0m21:50:59.927382 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.report_trading_performance"
[0m21:50:59.927941 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.report_trading_performance: create or replace transient table DBT_DEMO.DEV_pipeline_b.report_trading_performance
    
    
    
    as (-- Pipeline B: Trade Analytics Pipeline
-- Model: report_trading_performance
-- Description: Trading performance report for IC dashboard
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Re-aggregates fact data that could be pre-computed
-- 2. Complex window functions repeated from other models
-- 3. Multiple CTEs that could be consolidated

with trades as (
    select * from DBT_DEMO.DEV_pipeline_b.fact_trade_summary
),

positions as (
    select * from DBT_DEMO.DEV_pipeline_b.fact_portfolio_positions
),

-- ISSUE: Re-aggregating trade data by portfolio/month
trade_metrics as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        trade_year,
        trade_month,
        count(distinct trade_id) as trade_count,
        count(distinct security_id) as securities_traded,
        sum(case when trade_category = 'PURCHASE' then 1 else 0 end) as buy_count,
        sum(case when trade_category = 'SALE' then 1 else 0 end) as sell_count,
        sum(case when trade_category = 'PURCHASE' then net_amount else 0 end) as total_purchases,
        sum(case when trade_category = 'SALE' then abs(net_amount) else 0 end) as total_sales,
        sum(coalesce(realized_pnl, 0)) as total_realized_pnl,
        avg(case when realized_pnl is not null then realized_pnl_pct else null end) as avg_realized_return_pct
    from trades
    group by 1,2,3,4,5,6
),

-- ISSUE: Aggregating positions separately
position_metrics as (
    select
        portfolio_id,
        count(distinct security_id) as position_count,
        sum(market_value) as total_market_value,
        sum(cost_basis_value) as total_cost_basis,
        sum(unrealized_pnl) as total_unrealized_pnl,
        avg(unrealized_pnl_pct) as avg_unrealized_return_pct
    from positions
    group by 1
),

-- ISSUE: Window functions for running totals (repeated pattern)
with_running_totals as (
    select
        tm.*,
        sum(total_realized_pnl) over (
            partition by tm.portfolio_id
            order by tm.trade_year, tm.trade_month
            rows between unbounded preceding and current row
        ) as cumulative_realized_pnl,
        sum(total_purchases) over (
            partition by tm.portfolio_id
            order by tm.trade_year, tm.trade_month
            rows between unbounded preceding and current row
        ) as cumulative_invested,
        -- ISSUE: Multiple LAG functions
        lag(total_realized_pnl, 1) over (
            partition by tm.portfolio_id
            order by tm.trade_year, tm.trade_month
        ) as prior_month_pnl,
        lag(trade_count, 1) over (
            partition by tm.portfolio_id
            order by tm.trade_year, tm.trade_month
        ) as prior_month_trades
    from trade_metrics tm
),

-- ISSUE: Final join adds more complexity
final as (
    select
        wrt.*,
        pm.position_count,
        pm.total_market_value,
        pm.total_cost_basis,
        pm.total_unrealized_pnl,
        pm.avg_unrealized_return_pct,
        -- Combined metrics
        wrt.total_realized_pnl + coalesce(pm.total_unrealized_pnl, 0) as total_pnl,
        case
            when pm.total_cost_basis > 0
            then ((pm.total_market_value + wrt.cumulative_realized_pnl) - pm.total_cost_basis) / pm.total_cost_basis * 100
            else null
        end as total_return_pct
    from with_running_totals wrt
    left join position_metrics pm
        on wrt.portfolio_id = pm.portfolio_id
)

select * from final
order by portfolio_id, trade_year, trade_month
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.report_trading_performance"} */;
[0m21:51:02.953692 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 3.025 seconds
[0m21:51:02.957842 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4ccd7fef-a1e2-400f-ba9b-44748b691e9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1132aa160>]}
[0m21:51:02.958984 [info ] [Thread-2  ]: 12 of 12 OK created sql table model DEV_pipeline_b.report_trading_performance .. [[32mSUCCESS 1[0m in 3.05s]
[0m21:51:02.959726 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.report_trading_performance
[0m21:51:02.961577 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:51:02.962085 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.stg_securities' was left open.
[0m21:51:02.962500 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.stg_securities: Close
[0m21:51:03.421789 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.report_trading_performance' was left open.
[0m21:51:03.422849 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.report_trading_performance: Close
[0m21:51:03.838014 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.fact_portfolio_positions' was left open.
[0m21:51:03.838999 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.fact_portfolio_positions: Close
[0m21:51:04.204250 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.fact_trade_summary' was left open.
[0m21:51:04.205216 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.fact_trade_summary: Close
[0m21:51:04.541476 [info ] [MainThread]: 
[0m21:51:04.542326 [info ] [MainThread]: Finished running 4 table models, 8 view models in 0 hours 0 minutes and 48.98 seconds (48.98s).
[0m21:51:04.545114 [debug] [MainThread]: Command end result
[0m21:51:04.608010 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/manifest.json
[0m21:51:04.609955 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/semantic_manifest.json
[0m21:51:04.616405 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/nedazarei/Documents/turintech/dbtproject/target/run_results.json
[0m21:51:04.616714 [info ] [MainThread]: 
[0m21:51:04.617055 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:51:04.617334 [info ] [MainThread]: 
[0m21:51:04.617623 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=12
[0m21:51:04.619847 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 50.308865, "process_in_blocks": "0", "process_kernel_time": 0.424918, "process_mem_max_rss": "187940864", "process_out_blocks": "0", "process_user_time": 3.387307}
[0m21:51:04.620197 [debug] [MainThread]: Command `dbt run` succeeded at 21:51:04.620130 after 50.31 seconds
[0m21:51:04.620540 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a24b280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d83ac10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d5e4a60>]}
[0m21:51:04.620865 [debug] [MainThread]: Flushing usage events
[0m21:51:05.684280 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:51:34.713749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c2ec1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d39c9a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d39c7c0>]}


============================== 21:51:34.719760 | 709d07b4-7aae-402c-97e3-9eafa372715c ==============================
[0m21:51:34.719760 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:51:34.720279 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'log_cache_events': 'False', 'invocation_command': 'dbt run --select +pipeline_a.*', 'log_format': 'default', 'empty': 'False', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'quiet': 'False', 'use_colors': 'True', 'debug': 'False', 'profiles_dir': '/Users/nedazarei/Documents/turintech/dbtproject', 'target_path': 'None', 'version_check': 'True', 'log_path': '/Users/nedazarei/Documents/turintech/dbtproject/logs', 'partial_parse': 'True', 'printer_width': '80', 'introspect': 'True', 'static_parser': 'True', 'indirect_selection': 'eager', 'write_json': 'True'}
[0m21:51:35.199020 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '709d07b4-7aae-402c-97e3-9eafa372715c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e7e3700>]}
[0m21:51:35.273793 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '709d07b4-7aae-402c-97e3-9eafa372715c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e9250a0>]}
[0m21:51:35.274681 [info ] [MainThread]: Registered adapter: snowflake=1.10.2
[0m21:51:35.406534 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m21:51:35.626891 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:51:35.627307 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:51:35.751207 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '709d07b4-7aae-402c-97e3-9eafa372715c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cbf10d0>]}
[0m21:51:35.908295 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/manifest.json
[0m21:51:35.910427 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/semantic_manifest.json
[0m21:51:35.934696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '709d07b4-7aae-402c-97e3-9eafa372715c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f7868b0>]}
[0m21:51:35.935214 [info ] [MainThread]: Found 32 models, 90 data tests, 5 seeds, 12 sources, 632 macros
[0m21:51:35.935528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '709d07b4-7aae-402c-97e3-9eafa372715c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f776670>]}
[0m21:51:35.938165 [info ] [MainThread]: 
[0m21:51:35.938506 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m21:51:35.938761 [info ] [MainThread]: 
[0m21:51:35.939177 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m21:51:35.943786 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO'
[0m21:51:36.003862 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO"
[0m21:51:36.004313 [debug] [ThreadPool]: On list_DBT_DEMO: show terse schemas in database DBT_DEMO
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO"} */
[0m21:51:36.004591 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:51:39.559094 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 3.554 seconds
[0m21:51:39.567035 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DBT_DEMO, now list_DBT_DEMO_DEV)
[0m21:51:39.567998 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO_DEV_pipeline_b'
[0m21:51:39.575057 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO_DEV_pipeline_c'
[0m21:51:39.582258 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO_DEV_pipeline_a'
[0m21:51:39.586412 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV"
[0m21:51:39.589009 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV_pipeline_b"
[0m21:51:39.591606 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV_pipeline_c"
[0m21:51:39.594650 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV_pipeline_a"
[0m21:51:39.594998 [debug] [ThreadPool]: On list_DBT_DEMO_DEV: show objects in DBT_DEMO.DEV
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV"} */;
[0m21:51:39.595298 [debug] [ThreadPool]: On list_DBT_DEMO_DEV_pipeline_b: show objects in DBT_DEMO.DEV_pipeline_b
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV_pipeline_b"} */;
[0m21:51:39.595631 [debug] [ThreadPool]: On list_DBT_DEMO_DEV_pipeline_c: show objects in DBT_DEMO.DEV_pipeline_c
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV_pipeline_c"} */;
[0m21:51:39.595914 [debug] [ThreadPool]: On list_DBT_DEMO_DEV_pipeline_a: show objects in DBT_DEMO.DEV_pipeline_a
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV_pipeline_a"} */;
[0m21:51:39.596528 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:51:39.596844 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:51:39.597156 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:51:39.861336 [debug] [ThreadPool]: SQL status: SUCCESS 17 in 0.265 seconds
[0m21:51:42.718658 [debug] [ThreadPool]: SQL status: SUCCESS 19 in 3.122 seconds
[0m21:51:43.225264 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 3.628 seconds
[0m21:51:43.269179 [debug] [ThreadPool]: SQL status: SUCCESS 9 in 3.672 seconds
[0m21:51:43.272787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '709d07b4-7aae-402c-97e3-9eafa372715c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aaba9a0>]}
[0m21:51:43.275255 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.stg_cashflows
[0m21:51:43.275673 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.stg_portfolios
[0m21:51:43.276177 [info ] [Thread-1  ]: 1 of 4 START sql view model DEV_pipeline_a.stg_cashflows ....................... [RUN]
[0m21:51:43.276678 [info ] [Thread-2  ]: 2 of 4 START sql view model DEV_pipeline_a.stg_portfolios ...................... [RUN]
[0m21:51:43.277140 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV, now model.bain_capital_portfolio_analytics.stg_cashflows)
[0m21:51:43.277499 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV_pipeline_b, now model.bain_capital_portfolio_analytics.stg_portfolios)
[0m21:51:43.277834 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_cashflows
[0m21:51:43.278142 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_portfolios
[0m21:51:43.287604 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_cashflows"
[0m21:51:43.290589 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_portfolios"
[0m21:51:43.291386 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.stg_portfolios
[0m21:51:43.291750 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.stg_cashflows
[0m21:51:43.334100 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_cashflows"
[0m21:51:43.334647 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_portfolios"
[0m21:51:43.336350 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_cashflows"
[0m21:51:43.337483 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_portfolios"
[0m21:51:43.337858 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.stg_cashflows: create or replace   view DBT_DEMO.DEV_pipeline_a.stg_cashflows
  
  
  
  
  as (
    -- Pipeline A: Simple Cashflow Pipeline
-- Model: stg_cashflows
-- Description: Staging model for raw cashflow data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Unnecessary DISTINCT (source already unique)
-- 2. Late filtering (should push date filter upstream)
-- 3. Non-optimal date casting

with source as (
    select distinct  -- ISSUE: Unnecessary DISTINCT, source has unique constraint
        cashflow_id,
        portfolio_id,
        cashflow_type,
        cashflow_date,
        amount,
        currency,
        created_at,
        updated_at
    from DBT_DEMO.DEV.cashflows
),

-- ISSUE: Heavy transformation before filtering
converted as (
    select
        cashflow_id,
        portfolio_id,
        upper(cashflow_type) as cashflow_type,
        cast(cashflow_date as date) as cashflow_date,
        cast(amount as decimal(18,2)) as amount,
        upper(currency) as currency,
        cast(created_at as timestamp) as created_at,
        cast(updated_at as timestamp) as updated_at
    from source
),

-- ISSUE: Filter applied after transformation, should be earlier
filtered as (
    select *
    from converted
    where cashflow_date >= '2020-01-01'
      and cashflow_date <= '2024-12-31'
)

select * from filtered
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_cashflows"} */;
[0m21:51:43.338263 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.stg_portfolios: create or replace   view DBT_DEMO.DEV_pipeline_a.stg_portfolios
  
  
  
  
  as (
    -- Pipeline A: Simple Cashflow Pipeline
-- Model: stg_portfolios
-- Description: Staging model for portfolio master data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Subquery for deduplication instead of QUALIFY
-- 2. Multiple passes over data

with source as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        inception_date,
        status,
        currency,
        created_at,
        updated_at,
        row_number() over (
            partition by portfolio_id
            order by updated_at desc
        ) as rn
    from DBT_DEMO.DEV.portfolios
),

-- ISSUE: Using subquery filter instead of QUALIFY
deduplicated as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        inception_date,
        status,
        currency,
        created_at,
        updated_at
    from source
    where rn = 1  -- ISSUE: Should use QUALIFY in Snowflake
),

-- ISSUE: Another pass just for active filter
active_only as (
    select *
    from deduplicated
    where status = 'ACTIVE'
)

select * from active_only
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_portfolios"} */;
[0m21:51:43.626798 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.288 seconds
[0m21:51:43.657058 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '709d07b4-7aae-402c-97e3-9eafa372715c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e7e4490>]}
[0m21:51:43.657850 [info ] [Thread-2  ]: 2 of 4 OK created sql view model DEV_pipeline_a.stg_portfolios ................. [[32mSUCCESS 1[0m in 0.38s]
[0m21:51:43.658956 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.stg_portfolios
[0m21:51:43.659934 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.321 seconds
[0m21:51:43.661712 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '709d07b4-7aae-402c-97e3-9eafa372715c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d1a8f10>]}
[0m21:51:43.662339 [info ] [Thread-1  ]: 1 of 4 OK created sql view model DEV_pipeline_a.stg_cashflows .................. [[32mSUCCESS 1[0m in 0.38s]
[0m21:51:43.662827 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.stg_cashflows
[0m21:51:43.663546 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m21:51:43.664039 [info ] [Thread-4  ]: 3 of 4 START sql table model DEV_pipeline_a.fact_cashflow_summary .............. [RUN]
[0m21:51:43.664534 [debug] [Thread-4  ]: Acquiring new snowflake connection 'model.bain_capital_portfolio_analytics.fact_cashflow_summary'
[0m21:51:43.664863 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m21:51:43.679559 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_cashflow_summary"
[0m21:51:43.680522 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m21:51:43.702857 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_cashflow_summary"
[0m21:51:43.711824 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_cashflow_summary"
[0m21:51:43.712736 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.fact_cashflow_summary: create or replace transient table DBT_DEMO.DEV_pipeline_a.fact_cashflow_summary
    
    
    
    as (-- Pipeline A: Simple Cashflow Pipeline
-- Model: fact_cashflow_summary
-- Description: Fact table summarizing cashflows by portfolio and month
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-joins for prior period comparisons (should use LAG)
-- 2. Late aggregation (aggregates after full join)
-- 3. Repeated window functions with same partitions
-- 4. Redundant date calculations per row
-- 5. Correlated subqueries for fund-level totals

with cashflows as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_cashflows
),

portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

-- ISSUE: Full join before aggregation (scans all rows)
joined as (
    select
        c.cashflow_id,
        c.portfolio_id,
        p.portfolio_name,
        p.portfolio_type,
        p.fund_id,
        c.cashflow_type,
        c.cashflow_date,
        c.amount,
        c.currency,
        -- ISSUE: Redundant date calculations done per row
        date_trunc('month', c.cashflow_date) as cashflow_month,
        date_trunc('quarter', c.cashflow_date) as cashflow_quarter,
        date_trunc('year', c.cashflow_date) as cashflow_year,
        extract(year from c.cashflow_date) as year_num,
        extract(month from c.cashflow_date) as month_num,
        extract(quarter from c.cashflow_date) as quarter_num,
        extract(dayofmonth from c.cashflow_date) as day_num
    from cashflows c
    inner join portfolios p
        on c.portfolio_id = p.portfolio_id
),

-- ISSUE: Aggregation happens after full row-level join
aggregated as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        cashflow_month,
        cashflow_quarter,
        cashflow_year,
        year_num,
        month_num,
        quarter_num,
        cashflow_type,
        currency,
        count(*) as transaction_count,
        count(distinct cashflow_id) as unique_transactions,
        sum(amount) as total_amount,
        avg(amount) as avg_amount,
        min(amount) as min_amount,
        max(amount) as max_amount,
        stddev(amount) as stddev_amount,
        -- ISSUE: Percentile calculations (slow)
        percentile_cont(0.25) within group (order by amount) as p25_amount,
        percentile_cont(0.50) within group (order by amount) as median_amount,
        percentile_cont(0.75) within group (order by amount) as p75_amount
    from joined
    group by 1,2,3,4,5,6,7,8,9,10,11,12  -- ISSUE: Non-descriptive GROUP BY
),

-- ISSUE: Self-join for prior month comparisons (should use LAG)
with_prior_months as (
    select
        agg.*,
        -- ISSUE: Self-join for prior month
        agg_m1.total_amount as prior_1m_total,
        agg_m1.transaction_count as prior_1m_count,
        -- ISSUE: Self-join for 3 months ago
        agg_m3.total_amount as prior_3m_total,
        -- ISSUE: Self-join for 6 months ago
        agg_m6.total_amount as prior_6m_total,
        -- ISSUE: Self-join for 12 months ago
        agg_m12.total_amount as prior_12m_total
    from aggregated agg
    left join aggregated agg_m1
        on agg.portfolio_id = agg_m1.portfolio_id
        and agg.cashflow_type = agg_m1.cashflow_type
        and agg.currency = agg_m1.currency
        and agg_m1.cashflow_month = dateadd(month, -1, agg.cashflow_month)
    left join aggregated agg_m3
        on agg.portfolio_id = agg_m3.portfolio_id
        and agg.cashflow_type = agg_m3.cashflow_type
        and agg.currency = agg_m3.currency
        and agg_m3.cashflow_month = dateadd(month, -3, agg.cashflow_month)
    left join aggregated agg_m6
        on agg.portfolio_id = agg_m6.portfolio_id
        and agg.cashflow_type = agg_m6.cashflow_type
        and agg.currency = agg_m6.currency
        and agg_m6.cashflow_month = dateadd(month, -6, agg.cashflow_month)
    left join aggregated agg_m12
        on agg.portfolio_id = agg_m12.portfolio_id
        and agg.cashflow_type = agg_m12.cashflow_type
        and agg.currency = agg_m12.currency
        and agg_m12.cashflow_month = dateadd(month, -12, agg.cashflow_month)
),

-- ISSUE: Multiple window functions with repeated partitions
with_window_calcs as (
    select
        wpm.*,
        -- ISSUE: Running totals (repeated partition)
        sum(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_total,
        sum(transaction_count) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_count,
        -- ISSUE: Moving averages (same partition repeated)
        avg(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 2 preceding and current row
        ) as rolling_3m_avg,
        avg(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 5 preceding and current row
        ) as rolling_6m_avg,
        avg(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_avg,
        -- ISSUE: More window calculations
        stddev(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_stddev,
        min(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_min,
        max(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_max
    from with_prior_months wpm
),

-- ISSUE: Correlated subqueries for fund-level context (very slow)
with_fund_context as (
    select
        wwc.*,
        -- ISSUE: Correlated subquery for fund total
        (
            select sum(total_amount)
            from aggregated agg2
            inner join portfolios p2
                on agg2.portfolio_id = p2.portfolio_id
            where p2.fund_id = wwc.fund_id
            and agg2.cashflow_month = wwc.cashflow_month
            and agg2.cashflow_type = wwc.cashflow_type
        ) as fund_total_amount,
        -- ISSUE: Another correlated subquery for portfolio count
        (
            select count(distinct agg2.portfolio_id)
            from aggregated agg2
            inner join portfolios p2
                on agg2.portfolio_id = p2.portfolio_id
            where p2.fund_id = wwc.fund_id
            and agg2.cashflow_month = wwc.cashflow_month
            and agg2.cashflow_type = wwc.cashflow_type
        ) as fund_portfolio_count
    from with_window_calcs wwc
),

-- ISSUE: Complex derived metrics with repeated CASE statements
final as (
    select
        md5(cast(coalesce(cast(wfc.portfolio_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wfc.cashflow_month as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wfc.cashflow_type as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wfc.currency as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as cashflow_summary_key,
        wfc.*,
        -- ISSUE: Portfolio share of fund (repeated division)
        case
            when wfc.fund_total_amount > 0
            then (wfc.total_amount / wfc.fund_total_amount) * 100
            else null
        end as portfolio_share_of_fund_pct,
        -- ISSUE: Month-over-month growth calculations
        case
            when wfc.prior_1m_total is not null and wfc.prior_1m_total != 0
            then ((wfc.total_amount - wfc.prior_1m_total) / abs(wfc.prior_1m_total)) * 100
            else null
        end as mom_growth_pct,
        case
            when wfc.prior_3m_total is not null and wfc.prior_3m_total != 0
            then ((wfc.total_amount - wfc.prior_3m_total) / abs(wfc.prior_3m_total)) * 100
            else null
        end as growth_3m_pct,
        case
            when wfc.prior_12m_total is not null and wfc.prior_12m_total != 0
            then ((wfc.total_amount - wfc.prior_12m_total) / abs(wfc.prior_12m_total)) * 100
            else null
        end as yoy_growth_pct,
        -- ISSUE: Trend classification (complex nested CASE)
        case
            when wfc.rolling_3m_avg > wfc.rolling_12m_avg * 1.3 then 'ACCELERATING'
            when wfc.rolling_3m_avg > wfc.rolling_12m_avg * 1.1 then 'GROWING'
            when wfc.rolling_3m_avg < wfc.rolling_12m_avg * 0.7 then 'DECLINING_FAST'
            when wfc.rolling_3m_avg < wfc.rolling_12m_avg * 0.9 then 'DECLINING'
            else 'STABLE'
        end as trend_classification,
        -- ISSUE: Volatility classification
        case
            when wfc.rolling_12m_stddev < wfc.rolling_12m_avg * 0.1 then 'LOW_VOLATILITY'
            when wfc.rolling_12m_stddev < wfc.rolling_12m_avg * 0.3 then 'MODERATE_VOLATILITY'
            when wfc.rolling_12m_stddev < wfc.rolling_12m_avg * 0.5 then 'HIGH_VOLATILITY'
            else 'VERY_HIGH_VOLATILITY'
        end as volatility_classification,
        -- ISSUE: Size classification (repeated CASE)
        case
            when abs(wfc.total_amount) >= 10000000 then 'MEGA'
            when abs(wfc.total_amount) >= 5000000 then 'LARGE'
            when abs(wfc.total_amount) >= 1000000 then 'MEDIUM'
            when abs(wfc.total_amount) >= 100000 then 'SMALL'
            else 'MICRO'
        end as transaction_size_category
    from with_fund_context wfc
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_cashflow_summary"} */;
[0m21:51:43.713426 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m21:51:47.198738 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 3.485 seconds
[0m21:51:47.202719 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '709d07b4-7aae-402c-97e3-9eafa372715c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115bcbf10>]}
[0m21:51:47.203950 [info ] [Thread-4  ]: 3 of 4 OK created sql table model DEV_pipeline_a.fact_cashflow_summary ......... [[32mSUCCESS 1[0m in 3.54s]
[0m21:51:47.204808 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m21:51:47.205995 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.report_monthly_cashflows
[0m21:51:47.206751 [info ] [Thread-2  ]: 4 of 4 START sql table model DEV_pipeline_a.report_monthly_cashflows ........... [RUN]
[0m21:51:47.207437 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_portfolios, now model.bain_capital_portfolio_analytics.report_monthly_cashflows)
[0m21:51:47.207904 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.report_monthly_cashflows
[0m21:51:47.214357 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.report_monthly_cashflows"
[0m21:51:47.215760 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.report_monthly_cashflows
[0m21:51:47.220033 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.report_monthly_cashflows"
[0m21:51:47.223456 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.report_monthly_cashflows"
[0m21:51:47.223905 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.report_monthly_cashflows: create or replace transient table DBT_DEMO.DEV_pipeline_a.report_monthly_cashflows
    
    
    
    as (-- Pipeline A: Simple Cashflow Pipeline
-- Model: report_monthly_cashflows
-- Description: LP reporting view for monthly cashflow analysis
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Re-aggregates data that's already in fact table
-- 2. Repeated window functions
-- 3. Suboptimal pivot pattern

with fact_data as (
    select * from DBT_DEMO.DEV_pipeline_a.fact_cashflow_summary
),

-- ISSUE: Re-aggregating already aggregated data
monthly_totals as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        cashflow_month,
        year_num,
        month_num,
        sum(case when cashflow_type = 'CONTRIBUTION' then total_amount else 0 end) as contributions,
        sum(case when cashflow_type = 'DISTRIBUTION' then total_amount else 0 end) as distributions,
        sum(case when cashflow_type = 'DIVIDEND' then total_amount else 0 end) as dividends,
        sum(case when cashflow_type = 'FEE' then total_amount else 0 end) as fees,
        sum(total_amount) as total_cashflow,
        sum(transaction_count) as total_transactions
    from fact_data
    group by 1,2,3,4,5,6,7
),

-- ISSUE: Window functions recalculated multiple times
with_running_totals as (
    select
        *,
        -- Running totals (repeated pattern)
        sum(contributions) over (
            partition by portfolio_id
            order by cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_contributions,
        sum(distributions) over (
            partition by portfolio_id
            order by cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_distributions,
        sum(total_cashflow) over (
            partition by portfolio_id
            order by cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_net_cashflow,
        -- Prior period comparisons (another repeated pattern)
        lag(contributions, 1) over (partition by portfolio_id order by cashflow_month) as prior_month_contributions,
        lag(distributions, 1) over (partition by portfolio_id order by cashflow_month) as prior_month_distributions,
        lag(total_cashflow, 1) over (partition by portfolio_id order by cashflow_month) as prior_month_total,
        -- YoY comparison
        lag(contributions, 12) over (partition by portfolio_id order by cashflow_month) as prior_year_contributions,
        lag(distributions, 12) over (partition by portfolio_id order by cashflow_month) as prior_year_distributions
    from monthly_totals
),

-- ISSUE: Calculated columns that could be simplified
final as (
    select
        *,
        contributions - coalesce(prior_month_contributions, 0) as mom_contribution_change,
        distributions - coalesce(prior_month_distributions, 0) as mom_distribution_change,
        case
            when prior_year_contributions > 0
            then (contributions - prior_year_contributions) / prior_year_contributions * 100
            else null
        end as yoy_contribution_pct_change,
        contributions - distributions as net_inflow
    from with_running_totals
)

select * from final
order by portfolio_id, cashflow_month
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.report_monthly_cashflows"} */;
[0m21:51:48.156620 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.931 seconds
[0m21:51:48.160822 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '709d07b4-7aae-402c-97e3-9eafa372715c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115c5df70>]}
[0m21:51:48.161986 [info ] [Thread-2  ]: 4 of 4 OK created sql table model DEV_pipeline_a.report_monthly_cashflows ...... [[32mSUCCESS 1[0m in 0.95s]
[0m21:51:48.163081 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.report_monthly_cashflows
[0m21:51:48.165147 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:51:48.165565 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.stg_cashflows' was left open.
[0m21:51:48.165920 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.stg_cashflows: Close
[0m21:51:48.520887 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.report_monthly_cashflows' was left open.
[0m21:51:48.521794 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.report_monthly_cashflows: Close
[0m21:51:48.877880 [debug] [MainThread]: Connection 'list_DBT_DEMO_DEV_pipeline_c' was left open.
[0m21:51:48.878339 [debug] [MainThread]: On list_DBT_DEMO_DEV_pipeline_c: Close
[0m21:51:49.196566 [debug] [MainThread]: Connection 'list_DBT_DEMO_DEV_pipeline_a' was left open.
[0m21:51:49.197514 [debug] [MainThread]: On list_DBT_DEMO_DEV_pipeline_a: Close
[0m21:51:49.566378 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.fact_cashflow_summary' was left open.
[0m21:51:49.566966 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.fact_cashflow_summary: Close
[0m21:51:49.886382 [info ] [MainThread]: 
[0m21:51:49.887358 [info ] [MainThread]: Finished running 2 table models, 2 view models in 0 hours 0 minutes and 13.95 seconds (13.95s).
[0m21:51:49.888992 [debug] [MainThread]: Command end result
[0m21:51:49.950186 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/manifest.json
[0m21:51:49.952001 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/semantic_manifest.json
[0m21:51:49.957883 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/nedazarei/Documents/turintech/dbtproject/target/run_results.json
[0m21:51:49.958172 [info ] [MainThread]: 
[0m21:51:49.958496 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:51:49.958771 [info ] [MainThread]: 
[0m21:51:49.959046 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m21:51:49.961112 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 15.329461, "process_in_blocks": "0", "process_kernel_time": 0.410572, "process_mem_max_rss": "188071936", "process_out_blocks": "0", "process_user_time": 3.321338}
[0m21:51:49.961454 [debug] [MainThread]: Command `dbt run` succeeded at 21:51:49.961387 after 15.33 seconds
[0m21:51:49.961758 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c2ec1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c5e45e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115b54c40>]}
[0m21:51:49.962080 [debug] [MainThread]: Flushing usage events
[0m21:51:50.898546 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:51:52.618947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10be4f1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cf00ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cf008e0>]}


============================== 21:51:52.624925 | eaf7b553-23fb-4e89-a0d2-23f63291b762 ==============================
[0m21:51:52.624925 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:51:52.625456 [debug] [MainThread]: running dbt with arguments {'invocation_command': 'dbt run --select +pipeline_b.*', 'static_parser': 'True', 'debug': 'False', 'profiles_dir': '/Users/nedazarei/Documents/turintech/dbtproject', 'empty': 'False', 'version_check': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'warn_error': 'None', 'quiet': 'False', 'log_path': '/Users/nedazarei/Documents/turintech/dbtproject/logs', 'cache_selected_only': 'False', 'fail_fast': 'False', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'use_colors': 'True', 'partial_parse': 'True', 'target_path': 'None', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'printer_width': '80'}
[0m21:51:53.100572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'eaf7b553-23fb-4e89-a0d2-23f63291b762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c7b3790>]}
[0m21:51:53.174209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'eaf7b553-23fb-4e89-a0d2-23f63291b762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c7d1a60>]}
[0m21:51:53.175186 [info ] [MainThread]: Registered adapter: snowflake=1.10.2
[0m21:51:53.339297 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m21:51:53.823409 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:51:53.823988 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:51:54.053612 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'eaf7b553-23fb-4e89-a0d2-23f63291b762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cea7130>]}
[0m21:51:54.480570 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/manifest.json
[0m21:51:54.501164 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/semantic_manifest.json
[0m21:51:54.574039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'eaf7b553-23fb-4e89-a0d2-23f63291b762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f2e29d0>]}
[0m21:51:54.575027 [info ] [MainThread]: Found 32 models, 90 data tests, 5 seeds, 12 sources, 632 macros
[0m21:51:54.575522 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eaf7b553-23fb-4e89-a0d2-23f63291b762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e255640>]}
[0m21:51:54.580903 [info ] [MainThread]: 
[0m21:51:54.581580 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m21:51:54.581977 [info ] [MainThread]: 
[0m21:51:54.582651 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m21:51:54.591174 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO'
[0m21:51:54.592057 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO'
[0m21:51:54.667837 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO"
[0m21:51:54.668376 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO"
[0m21:51:54.668694 [debug] [ThreadPool]: On list_DBT_DEMO: show terse schemas in database DBT_DEMO
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO"} */
[0m21:51:54.668985 [debug] [ThreadPool]: On list_DBT_DEMO: show terse schemas in database DBT_DEMO
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO"} */
[0m21:51:54.669254 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:51:54.669519 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:51:56.315123 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 1.645 seconds
[0m21:51:56.324964 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 1.656 seconds
[0m21:51:56.333103 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DBT_DEMO, now list_DBT_DEMO_DEV)
[0m21:51:56.334148 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DBT_DEMO, now list_DBT_DEMO_DEV_pipeline_c)
[0m21:51:56.342490 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO_DEV_pipeline_a'
[0m21:51:56.350323 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV"
[0m21:51:56.350934 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO_DEV_pipeline_b'
[0m21:51:56.353450 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV_pipeline_c"
[0m21:51:56.357369 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV_pipeline_a"
[0m21:51:56.357730 [debug] [ThreadPool]: On list_DBT_DEMO_DEV: show objects in DBT_DEMO.DEV
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV"} */;
[0m21:51:56.359918 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV_pipeline_b"
[0m21:51:56.360205 [debug] [ThreadPool]: On list_DBT_DEMO_DEV_pipeline_c: show objects in DBT_DEMO.DEV_pipeline_c
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV_pipeline_c"} */;
[0m21:51:56.360469 [debug] [ThreadPool]: On list_DBT_DEMO_DEV_pipeline_a: show objects in DBT_DEMO.DEV_pipeline_a
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV_pipeline_a"} */;
[0m21:51:56.360860 [debug] [ThreadPool]: On list_DBT_DEMO_DEV_pipeline_b: show objects in DBT_DEMO.DEV_pipeline_b
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV_pipeline_b"} */;
[0m21:51:56.361520 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:51:56.361820 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:51:56.564038 [debug] [ThreadPool]: SQL status: SUCCESS 17 in 0.203 seconds
[0m21:51:56.626250 [debug] [ThreadPool]: SQL status: SUCCESS 19 in 0.265 seconds
[0m21:51:58.473698 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 2.112 seconds
[0m21:51:58.544173 [debug] [ThreadPool]: SQL status: SUCCESS 9 in 2.182 seconds
[0m21:51:58.547641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eaf7b553-23fb-4e89-a0d2-23f63291b762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cedf730>]}
[0m21:51:58.550237 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.stg_brokers
[0m21:51:58.550645 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.stg_cashflows
[0m21:51:58.550973 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.stg_market_prices
[0m21:51:58.551456 [info ] [Thread-1  ]: 1 of 12 START sql view model DEV_pipeline_b.stg_brokers ........................ [RUN]
[0m21:51:58.551849 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.stg_portfolios
[0m21:51:58.552279 [info ] [Thread-2  ]: 2 of 12 START sql view model DEV_pipeline_a.stg_cashflows ...................... [RUN]
[0m21:51:58.552711 [info ] [Thread-3  ]: 3 of 12 START sql view model DEV_pipeline_b.stg_market_prices .................. [RUN]
[0m21:51:58.553138 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV, now model.bain_capital_portfolio_analytics.stg_brokers)
[0m21:51:58.553544 [info ] [Thread-4  ]: 4 of 12 START sql view model DEV_pipeline_a.stg_portfolios ..................... [RUN]
[0m21:51:58.553958 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV_pipeline_c, now model.bain_capital_portfolio_analytics.stg_cashflows)
[0m21:51:58.554292 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV_pipeline_a, now model.bain_capital_portfolio_analytics.stg_market_prices)
[0m21:51:58.554606 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_brokers
[0m21:51:58.554916 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV_pipeline_b, now model.bain_capital_portfolio_analytics.stg_portfolios)
[0m21:51:58.555216 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_cashflows
[0m21:51:58.555507 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_market_prices
[0m21:51:58.565248 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_brokers"
[0m21:51:58.565646 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_portfolios
[0m21:51:58.568987 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_cashflows"
[0m21:51:58.572169 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_market_prices"
[0m21:51:58.575194 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_portfolios"
[0m21:51:58.576126 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.stg_market_prices
[0m21:51:58.576479 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.stg_brokers
[0m21:51:58.576784 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.stg_cashflows
[0m21:51:58.582835 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.stg_portfolios
[0m21:51:58.622658 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_cashflows"
[0m21:51:58.626596 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_market_prices"
[0m21:51:58.620627 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_brokers"
[0m21:51:58.629785 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_portfolios"
[0m21:51:58.631422 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_brokers"
[0m21:51:58.631802 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.stg_brokers: create or replace   view DBT_DEMO.DEV_pipeline_b.stg_brokers
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: stg_brokers
-- Description: Staging model for broker information

with source as (
    select
        broker_id,
        broker_name,
        broker_type,
        region,
        is_active,
        commission_rate,
        created_at,
        updated_at
    from DBT_DEMO.DEV.brokers
),

deduplicated as (
    select *
    from (
        select
            *,
            row_number() over (partition by broker_id order by updated_at desc) as rn
        from source
    )
    where rn = 1
)

select
    broker_id,
    trim(broker_name) as broker_name,
    upper(broker_type) as broker_type,
    upper(region) as region,
    is_active,
    commission_rate,
    created_at,
    updated_at
from deduplicated
where is_active = true
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_brokers"} */;
[0m21:51:58.634208 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_market_prices"
[0m21:51:58.635498 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_cashflows"
[0m21:51:58.635968 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.stg_market_prices: create or replace   view DBT_DEMO.DEV_pipeline_b.stg_market_prices
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: stg_market_prices
-- Description: Staging model for daily market prices
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-join for prior day prices (inefficient)
-- 2. Late aggregation
-- 3. Multiple window functions that could be consolidated

with source as (
    select
        security_id,
        price_date,
        open_price,
        high_price,
        low_price,
        close_price,
        volume,
        created_at
    from DBT_DEMO.DEV.market_prices
    where price_date >= '2020-01-01'
),

-- ISSUE: Self-join to get prior day price (should use LAG)
with_prior_day as (
    select
        curr.security_id,
        curr.price_date,
        curr.open_price,
        curr.high_price,
        curr.low_price,
        curr.close_price,
        curr.volume,
        prev.close_price as prior_close,
        prev.volume as prior_volume
    from source curr
    left join source prev
        on curr.security_id = prev.security_id
        and curr.price_date = dateadd('day', 1, prev.price_date)  -- ISSUE: Doesn't handle weekends
),

-- ISSUE: Multiple separate window functions
with_returns as (
    select
        *,
        -- Daily return
        case
            when prior_close > 0
            then (close_price - prior_close) / prior_close
            else null
        end as daily_return,
        -- ISSUE: These could be computed together
        avg(close_price) over (
            partition by security_id
            order by price_date
            rows between 19 preceding and current row
        ) as ma_20,
        avg(close_price) over (
            partition by security_id
            order by price_date
            rows between 49 preceding and current row
        ) as ma_50,
        avg(close_price) over (
            partition by security_id
            order by price_date
            rows between 199 preceding and current row
        ) as ma_200,
        stddev(close_price) over (
            partition by security_id
            order by price_date
            rows between 19 preceding and current row
        ) as volatility_20d,
        avg(volume) over (
            partition by security_id
            order by price_date
            rows between 19 preceding and current row
        ) as avg_volume_20d
    from with_prior_day
),

-- ISSUE: Another pass for more calculations
final as (
    select
        *,
        case
            when ma_20 > ma_50 and ma_50 > ma_200 then 'BULLISH'
            when ma_20 < ma_50 and ma_50 < ma_200 then 'BEARISH'
            else 'NEUTRAL'
        end as trend_signal,
        case
            when volume > avg_volume_20d * 2 then 'HIGH'
            when volume < avg_volume_20d * 0.5 then 'LOW'
            else 'NORMAL'
        end as volume_signal
    from with_returns
)

select * from final
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_market_prices"} */;
[0m21:51:58.637183 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_portfolios"
[0m21:51:58.637575 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.stg_cashflows: create or replace   view DBT_DEMO.DEV_pipeline_a.stg_cashflows
  
  
  
  
  as (
    -- Pipeline A: Simple Cashflow Pipeline
-- Model: stg_cashflows
-- Description: Staging model for raw cashflow data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Unnecessary DISTINCT (source already unique)
-- 2. Late filtering (should push date filter upstream)
-- 3. Non-optimal date casting

with source as (
    select distinct  -- ISSUE: Unnecessary DISTINCT, source has unique constraint
        cashflow_id,
        portfolio_id,
        cashflow_type,
        cashflow_date,
        amount,
        currency,
        created_at,
        updated_at
    from DBT_DEMO.DEV.cashflows
),

-- ISSUE: Heavy transformation before filtering
converted as (
    select
        cashflow_id,
        portfolio_id,
        upper(cashflow_type) as cashflow_type,
        cast(cashflow_date as date) as cashflow_date,
        cast(amount as decimal(18,2)) as amount,
        upper(currency) as currency,
        cast(created_at as timestamp) as created_at,
        cast(updated_at as timestamp) as updated_at
    from source
),

-- ISSUE: Filter applied after transformation, should be earlier
filtered as (
    select *
    from converted
    where cashflow_date >= '2020-01-01'
      and cashflow_date <= '2024-12-31'
)

select * from filtered
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_cashflows"} */;
[0m21:51:58.638493 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.stg_portfolios: create or replace   view DBT_DEMO.DEV_pipeline_a.stg_portfolios
  
  
  
  
  as (
    -- Pipeline A: Simple Cashflow Pipeline
-- Model: stg_portfolios
-- Description: Staging model for portfolio master data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Subquery for deduplication instead of QUALIFY
-- 2. Multiple passes over data

with source as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        inception_date,
        status,
        currency,
        created_at,
        updated_at,
        row_number() over (
            partition by portfolio_id
            order by updated_at desc
        ) as rn
    from DBT_DEMO.DEV.portfolios
),

-- ISSUE: Using subquery filter instead of QUALIFY
deduplicated as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        inception_date,
        status,
        currency,
        created_at,
        updated_at
    from source
    where rn = 1  -- ISSUE: Should use QUALIFY in Snowflake
),

-- ISSUE: Another pass just for active filter
active_only as (
    select *
    from deduplicated
    where status = 'ACTIVE'
)

select * from active_only
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_portfolios"} */;
[0m21:51:58.972043 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 0.332 seconds
[0m21:51:59.003512 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eaf7b553-23fb-4e89-a0d2-23f63291b762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fea1af0>]}
[0m21:51:59.004297 [info ] [Thread-4  ]: 4 of 12 OK created sql view model DEV_pipeline_a.stg_portfolios ................ [[32mSUCCESS 1[0m in 0.45s]
[0m21:51:59.004815 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.stg_portfolios
[0m21:51:59.005179 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.stg_securities
[0m21:51:59.005693 [info ] [Thread-4  ]: 5 of 12 START sql view model DEV_pipeline_b.stg_securities ..................... [RUN]
[0m21:51:59.006144 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_portfolios, now model.bain_capital_portfolio_analytics.stg_securities)
[0m21:51:59.006473 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_securities
[0m21:51:59.009806 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_securities"
[0m21:51:59.010539 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.stg_securities
[0m21:51:59.013691 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_securities"
[0m21:51:59.015570 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_securities"
[0m21:51:59.015949 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.stg_securities: create or replace   view DBT_DEMO.DEV_pipeline_b.stg_securities
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: stg_securities
-- Description: Staging model for security master data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Nested subqueries instead of QUALIFY
-- 2. Multiple deduplication passes

with source as (
    select
        security_id,
        ticker,
        security_name,
        security_type,
        asset_class,
        sector,
        industry,
        currency,
        exchange,
        is_active,
        created_at,
        updated_at
    from DBT_DEMO.DEV.securities
),

-- ISSUE: Complex deduplication using subquery
deduplicated as (
    select *
    from (
        select
            *,
            row_number() over (
                partition by security_id
                order by updated_at desc
            ) as rn
        from source
    ) sub
    where rn = 1  -- ISSUE: Should use QUALIFY
),

-- ISSUE: Separate CTE for type standardization
standardized as (
    select
        security_id,
        upper(trim(ticker)) as ticker,
        trim(security_name) as security_name,
        -- ISSUE: Repeated CASE logic found in other models
        case
            when security_type in ('STOCK', 'EQUITY', 'COMMON') then 'EQUITY'
            when security_type in ('BOND', 'NOTE', 'DEBENTURE') then 'FIXED_INCOME'
            when security_type in ('OPTION', 'FUTURE', 'SWAP') then 'DERIVATIVE'
            when security_type in ('ETF', 'MUTUAL_FUND') then 'FUND'
            else 'OTHER'
        end as security_type_standardized,
        security_type as security_type_original,
        upper(asset_class) as asset_class,
        sector,
        industry,
        upper(currency) as currency,
        exchange,
        is_active,
        created_at,
        updated_at
    from deduplicated
)

select * from standardized
where is_active = true
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_securities"} */;
[0m21:51:59.032650 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 0.394 seconds
[0m21:51:59.034684 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eaf7b553-23fb-4e89-a0d2-23f63291b762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ff2c130>]}
[0m21:51:59.035262 [info ] [Thread-3  ]: 3 of 12 OK created sql view model DEV_pipeline_b.stg_market_prices ............. [[32mSUCCESS 1[0m in 0.48s]
[0m21:51:59.035747 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.stg_market_prices
[0m21:51:59.036088 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.stg_trades
[0m21:51:59.036563 [info ] [Thread-3  ]: 6 of 12 START sql view model DEV_pipeline_b.stg_trades ......................... [RUN]
[0m21:51:59.036956 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_market_prices, now model.bain_capital_portfolio_analytics.stg_trades)
[0m21:51:59.037265 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_trades
[0m21:51:59.040850 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_trades"
[0m21:51:59.041511 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.stg_trades
[0m21:51:59.044372 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_trades"
[0m21:51:59.046770 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_trades"
[0m21:51:59.047168 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.stg_trades: create or replace   view DBT_DEMO.DEV_pipeline_b.stg_trades
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: stg_trades
-- Description: Staging model for trade transactions
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Complex CASE statements that repeat
-- 2. Multiple CTEs doing similar transformations
-- 3. Unnecessary string operations

with source as (
    select
        trade_id,
        portfolio_id,
        security_id,
        broker_id,
        trade_date,
        settlement_date,
        trade_type,
        quantity,
        price,
        gross_amount,
        commission,
        fees,
        net_amount,
        currency,
        created_at,
        updated_at
    from DBT_DEMO.DEV.trades
),

-- ISSUE: Repeated CASE statements for trade categorization
categorized as (
    select
        *,
        -- ISSUE: This logic is repeated in multiple models
        case
            when trade_type in ('BUY', 'COVER') then 'PURCHASE'
            when trade_type in ('SELL', 'SHORT') then 'SALE'
            when trade_type in ('DIVIDEND', 'INTEREST') then 'INCOME'
            else 'OTHER'
        end as trade_category,
        case
            when abs(net_amount) >= 10000000 then 'LARGE'
            when abs(net_amount) >= 1000000 then 'MEDIUM'
            when abs(net_amount) >= 100000 then 'SMALL'
            else 'MICRO'
        end as trade_size_bucket,
        -- ISSUE: Redundant string manipulation
        upper(trim(trade_type)) as trade_type_clean,
        upper(trim(currency)) as currency_clean
    from source
),

-- ISSUE: Another pass just for date calculations
with_dates as (
    select
        *,
        datediff('day', trade_date, settlement_date) as settlement_days,
        date_trunc('month', trade_date) as trade_month,
        date_trunc('quarter', trade_date) as trade_quarter,
        extract(year from trade_date) as trade_year,
        extract(month from trade_date) as trade_month_num,
        dayofweek(trade_date) as trade_day_of_week
    from categorized
),

-- ISSUE: Late filtering
filtered as (
    select *
    from with_dates
    where trade_date >= '2020-01-01'
)

select * from filtered
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_trades"} */;
[0m21:51:59.048939 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.417 seconds
[0m21:51:59.050897 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eaf7b553-23fb-4e89-a0d2-23f63291b762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11503e760>]}
[0m21:51:59.051722 [info ] [Thread-1  ]: 1 of 12 OK created sql view model DEV_pipeline_b.stg_brokers ................... [[32mSUCCESS 1[0m in 0.50s]
[0m21:51:59.052239 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.stg_brokers
[0m21:51:59.075557 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.436 seconds
[0m21:51:59.077231 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eaf7b553-23fb-4e89-a0d2-23f63291b762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1150f7520>]}
[0m21:51:59.077756 [info ] [Thread-2  ]: 2 of 12 OK created sql view model DEV_pipeline_a.stg_cashflows ................. [[32mSUCCESS 1[0m in 0.52s]
[0m21:51:59.078243 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.stg_cashflows
[0m21:51:59.078747 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m21:51:59.079324 [info ] [Thread-1  ]: 7 of 12 START sql table model DEV_pipeline_a.fact_cashflow_summary ............. [RUN]
[0m21:51:59.079761 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_brokers, now model.bain_capital_portfolio_analytics.fact_cashflow_summary)
[0m21:51:59.080091 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m21:51:59.095323 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_cashflow_summary"
[0m21:51:59.096252 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m21:51:59.118926 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_cashflow_summary"
[0m21:51:59.127110 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_cashflow_summary"
[0m21:51:59.127783 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.fact_cashflow_summary: create or replace transient table DBT_DEMO.DEV_pipeline_a.fact_cashflow_summary
    
    
    
    as (-- Pipeline A: Simple Cashflow Pipeline
-- Model: fact_cashflow_summary
-- Description: Fact table summarizing cashflows by portfolio and month
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-joins for prior period comparisons (should use LAG)
-- 2. Late aggregation (aggregates after full join)
-- 3. Repeated window functions with same partitions
-- 4. Redundant date calculations per row
-- 5. Correlated subqueries for fund-level totals

with cashflows as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_cashflows
),

portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

-- ISSUE: Full join before aggregation (scans all rows)
joined as (
    select
        c.cashflow_id,
        c.portfolio_id,
        p.portfolio_name,
        p.portfolio_type,
        p.fund_id,
        c.cashflow_type,
        c.cashflow_date,
        c.amount,
        c.currency,
        -- ISSUE: Redundant date calculations done per row
        date_trunc('month', c.cashflow_date) as cashflow_month,
        date_trunc('quarter', c.cashflow_date) as cashflow_quarter,
        date_trunc('year', c.cashflow_date) as cashflow_year,
        extract(year from c.cashflow_date) as year_num,
        extract(month from c.cashflow_date) as month_num,
        extract(quarter from c.cashflow_date) as quarter_num,
        extract(dayofmonth from c.cashflow_date) as day_num
    from cashflows c
    inner join portfolios p
        on c.portfolio_id = p.portfolio_id
),

-- ISSUE: Aggregation happens after full row-level join
aggregated as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        cashflow_month,
        cashflow_quarter,
        cashflow_year,
        year_num,
        month_num,
        quarter_num,
        cashflow_type,
        currency,
        count(*) as transaction_count,
        count(distinct cashflow_id) as unique_transactions,
        sum(amount) as total_amount,
        avg(amount) as avg_amount,
        min(amount) as min_amount,
        max(amount) as max_amount,
        stddev(amount) as stddev_amount,
        -- ISSUE: Percentile calculations (slow)
        percentile_cont(0.25) within group (order by amount) as p25_amount,
        percentile_cont(0.50) within group (order by amount) as median_amount,
        percentile_cont(0.75) within group (order by amount) as p75_amount
    from joined
    group by 1,2,3,4,5,6,7,8,9,10,11,12  -- ISSUE: Non-descriptive GROUP BY
),

-- ISSUE: Self-join for prior month comparisons (should use LAG)
with_prior_months as (
    select
        agg.*,
        -- ISSUE: Self-join for prior month
        agg_m1.total_amount as prior_1m_total,
        agg_m1.transaction_count as prior_1m_count,
        -- ISSUE: Self-join for 3 months ago
        agg_m3.total_amount as prior_3m_total,
        -- ISSUE: Self-join for 6 months ago
        agg_m6.total_amount as prior_6m_total,
        -- ISSUE: Self-join for 12 months ago
        agg_m12.total_amount as prior_12m_total
    from aggregated agg
    left join aggregated agg_m1
        on agg.portfolio_id = agg_m1.portfolio_id
        and agg.cashflow_type = agg_m1.cashflow_type
        and agg.currency = agg_m1.currency
        and agg_m1.cashflow_month = dateadd(month, -1, agg.cashflow_month)
    left join aggregated agg_m3
        on agg.portfolio_id = agg_m3.portfolio_id
        and agg.cashflow_type = agg_m3.cashflow_type
        and agg.currency = agg_m3.currency
        and agg_m3.cashflow_month = dateadd(month, -3, agg.cashflow_month)
    left join aggregated agg_m6
        on agg.portfolio_id = agg_m6.portfolio_id
        and agg.cashflow_type = agg_m6.cashflow_type
        and agg.currency = agg_m6.currency
        and agg_m6.cashflow_month = dateadd(month, -6, agg.cashflow_month)
    left join aggregated agg_m12
        on agg.portfolio_id = agg_m12.portfolio_id
        and agg.cashflow_type = agg_m12.cashflow_type
        and agg.currency = agg_m12.currency
        and agg_m12.cashflow_month = dateadd(month, -12, agg.cashflow_month)
),

-- ISSUE: Multiple window functions with repeated partitions
with_window_calcs as (
    select
        wpm.*,
        -- ISSUE: Running totals (repeated partition)
        sum(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_total,
        sum(transaction_count) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_count,
        -- ISSUE: Moving averages (same partition repeated)
        avg(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 2 preceding and current row
        ) as rolling_3m_avg,
        avg(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 5 preceding and current row
        ) as rolling_6m_avg,
        avg(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_avg,
        -- ISSUE: More window calculations
        stddev(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_stddev,
        min(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_min,
        max(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_max
    from with_prior_months wpm
),

-- ISSUE: Correlated subqueries for fund-level context (very slow)
with_fund_context as (
    select
        wwc.*,
        -- ISSUE: Correlated subquery for fund total
        (
            select sum(total_amount)
            from aggregated agg2
            inner join portfolios p2
                on agg2.portfolio_id = p2.portfolio_id
            where p2.fund_id = wwc.fund_id
            and agg2.cashflow_month = wwc.cashflow_month
            and agg2.cashflow_type = wwc.cashflow_type
        ) as fund_total_amount,
        -- ISSUE: Another correlated subquery for portfolio count
        (
            select count(distinct agg2.portfolio_id)
            from aggregated agg2
            inner join portfolios p2
                on agg2.portfolio_id = p2.portfolio_id
            where p2.fund_id = wwc.fund_id
            and agg2.cashflow_month = wwc.cashflow_month
            and agg2.cashflow_type = wwc.cashflow_type
        ) as fund_portfolio_count
    from with_window_calcs wwc
),

-- ISSUE: Complex derived metrics with repeated CASE statements
final as (
    select
        md5(cast(coalesce(cast(wfc.portfolio_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wfc.cashflow_month as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wfc.cashflow_type as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wfc.currency as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as cashflow_summary_key,
        wfc.*,
        -- ISSUE: Portfolio share of fund (repeated division)
        case
            when wfc.fund_total_amount > 0
            then (wfc.total_amount / wfc.fund_total_amount) * 100
            else null
        end as portfolio_share_of_fund_pct,
        -- ISSUE: Month-over-month growth calculations
        case
            when wfc.prior_1m_total is not null and wfc.prior_1m_total != 0
            then ((wfc.total_amount - wfc.prior_1m_total) / abs(wfc.prior_1m_total)) * 100
            else null
        end as mom_growth_pct,
        case
            when wfc.prior_3m_total is not null and wfc.prior_3m_total != 0
            then ((wfc.total_amount - wfc.prior_3m_total) / abs(wfc.prior_3m_total)) * 100
            else null
        end as growth_3m_pct,
        case
            when wfc.prior_12m_total is not null and wfc.prior_12m_total != 0
            then ((wfc.total_amount - wfc.prior_12m_total) / abs(wfc.prior_12m_total)) * 100
            else null
        end as yoy_growth_pct,
        -- ISSUE: Trend classification (complex nested CASE)
        case
            when wfc.rolling_3m_avg > wfc.rolling_12m_avg * 1.3 then 'ACCELERATING'
            when wfc.rolling_3m_avg > wfc.rolling_12m_avg * 1.1 then 'GROWING'
            when wfc.rolling_3m_avg < wfc.rolling_12m_avg * 0.7 then 'DECLINING_FAST'
            when wfc.rolling_3m_avg < wfc.rolling_12m_avg * 0.9 then 'DECLINING'
            else 'STABLE'
        end as trend_classification,
        -- ISSUE: Volatility classification
        case
            when wfc.rolling_12m_stddev < wfc.rolling_12m_avg * 0.1 then 'LOW_VOLATILITY'
            when wfc.rolling_12m_stddev < wfc.rolling_12m_avg * 0.3 then 'MODERATE_VOLATILITY'
            when wfc.rolling_12m_stddev < wfc.rolling_12m_avg * 0.5 then 'HIGH_VOLATILITY'
            else 'VERY_HIGH_VOLATILITY'
        end as volatility_classification,
        -- ISSUE: Size classification (repeated CASE)
        case
            when abs(wfc.total_amount) >= 10000000 then 'MEGA'
            when abs(wfc.total_amount) >= 5000000 then 'LARGE'
            when abs(wfc.total_amount) >= 1000000 then 'MEDIUM'
            when abs(wfc.total_amount) >= 100000 then 'SMALL'
            else 'MICRO'
        end as transaction_size_category
    from with_fund_context wfc
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_cashflow_summary"} */;
[0m21:51:59.379345 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 0.363 seconds
[0m21:51:59.382839 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eaf7b553-23fb-4e89-a0d2-23f63291b762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b8dec10>]}
[0m21:51:59.383761 [info ] [Thread-4  ]: 5 of 12 OK created sql view model DEV_pipeline_b.stg_securities ................ [[32mSUCCESS 1[0m in 0.38s]
[0m21:51:59.384434 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.stg_securities
[0m21:51:59.428438 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 0.381 seconds
[0m21:51:59.431360 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eaf7b553-23fb-4e89-a0d2-23f63291b762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115110e20>]}
[0m21:51:59.432178 [info ] [Thread-3  ]: 6 of 12 OK created sql view model DEV_pipeline_b.stg_trades .................... [[32mSUCCESS 1[0m in 0.39s]
[0m21:51:59.432819 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.stg_trades
[0m21:51:59.433414 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.int_trades_enriched
[0m21:51:59.433868 [info ] [Thread-2  ]: 8 of 12 START sql view model DEV_pipeline_b.int_trades_enriched ................ [RUN]
[0m21:51:59.434316 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_cashflows, now model.bain_capital_portfolio_analytics.int_trades_enriched)
[0m21:51:59.434684 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.int_trades_enriched
[0m21:51:59.439604 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_trades_enriched"
[0m21:51:59.440514 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.int_trades_enriched
[0m21:51:59.443519 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_trades_enriched"
[0m21:51:59.445996 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_trades_enriched"
[0m21:51:59.446394 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.int_trades_enriched: create or replace   view DBT_DEMO.DEV_pipeline_b.int_trades_enriched
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: int_trades_enriched
-- Description: Intermediate model enriching trades with security and price data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Multiple heavy joins done row-by-row
-- 2. Price lookup repeated for every trade
-- 3. Could pre-aggregate before joining

with trades as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_trades
),

securities as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_securities
),

market_prices as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_market_prices
),

brokers as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_brokers
),

-- ISSUE: Heavy multi-way join before any aggregation
enriched as (
    select
        t.trade_id,
        t.portfolio_id,
        t.security_id,
        t.trade_date,
        t.settlement_date,
        t.trade_type,
        t.trade_category,
        t.trade_size_bucket,
        t.quantity,
        t.price as execution_price,
        t.gross_amount,
        t.commission,
        t.fees,
        t.net_amount,
        t.currency,
        t.settlement_days,
        t.trade_month,
        t.trade_quarter,
        t.trade_year,
        -- Security attributes
        s.ticker,
        s.security_name,
        s.security_type_standardized as security_type,
        s.asset_class,
        s.sector,
        s.industry,
        -- Broker attributes
        b.broker_name,
        b.broker_type,
        b.region as broker_region,
        b.commission_rate as standard_commission_rate,
        -- Market price on trade date
        mp.close_price as market_close_price,
        mp.ma_20,
        mp.ma_50,
        mp.volatility_20d,
        mp.trend_signal,
        mp.volume_signal,
        -- ISSUE: These calculations done per row
        case
            when mp.close_price > 0
            then (t.price - mp.close_price) / mp.close_price * 100
            else null
        end as execution_vs_close_pct,
        case
            when t.price > mp.close_price then 'ABOVE_MARKET'
            when t.price < mp.close_price then 'BELOW_MARKET'
            else 'AT_MARKET'
        end as execution_quality,
        -- Cost analysis
        t.commission + t.fees as total_costs,
        case
            when t.gross_amount > 0
            then (t.commission + t.fees) / t.gross_amount * 10000
            else null
        end as cost_bps
    from trades t
    inner join securities s
        on t.security_id = s.security_id
    left join brokers b
        on t.broker_id = b.broker_id
    left join market_prices mp
        on t.security_id = mp.security_id
        and t.trade_date = mp.price_date
)

select * from enriched
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_trades_enriched"} */;
[0m21:52:00.113982 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.666 seconds
[0m21:52:00.116021 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eaf7b553-23fb-4e89-a0d2-23f63291b762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd2bf40>]}
[0m21:52:00.116688 [info ] [Thread-2  ]: 8 of 12 OK created sql view model DEV_pipeline_b.int_trades_enriched ........... [[32mSUCCESS 1[0m in 0.68s]
[0m21:52:00.117265 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.int_trades_enriched
[0m21:52:00.117774 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.int_trade_pnl
[0m21:52:00.118206 [info ] [Thread-3  ]: 9 of 12 START sql view model DEV_pipeline_b.int_trade_pnl ...................... [RUN]
[0m21:52:00.118605 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_trades, now model.bain_capital_portfolio_analytics.int_trade_pnl)
[0m21:52:00.118921 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.int_trade_pnl
[0m21:52:00.122438 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_trade_pnl"
[0m21:52:00.123167 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.int_trade_pnl
[0m21:52:00.126138 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_trade_pnl"
[0m21:52:00.128990 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_trade_pnl"
[0m21:52:00.129631 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.int_trade_pnl: create or replace   view DBT_DEMO.DEV_pipeline_b.int_trade_pnl
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: int_trade_pnl
-- Description: Calculate P&L for each trade
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Complex position tracking logic that could be simplified
-- 2. Multiple self-joins for cost basis calculation
-- 3. Window functions recalculated multiple times

with trades as (
    select * from DBT_DEMO.DEV_pipeline_b.int_trades_enriched
),

-- ISSUE: Running position calculation done inefficiently
positions as (
    select
        trade_id,
        portfolio_id,
        security_id,
        ticker,
        security_name,
        security_type,
        asset_class,
        sector,
        industry,
        trade_date,
        trade_type,
        trade_category,
        quantity,
        execution_price,
        net_amount,
        commission,
        -- ISSUE: Multiple window functions with same partition
        sum(case
            when trade_category = 'PURCHASE' then quantity
            when trade_category = 'SALE' then -quantity
            else 0
        end) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as running_position,
        sum(case
            when trade_category = 'PURCHASE' then net_amount
            when trade_category = 'SALE' then -net_amount
            else 0
        end) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as cumulative_cost,
        -- ISSUE: Another separate window for purchase-only
        sum(case when trade_category = 'PURCHASE' then quantity else 0 end) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as cumulative_purchased_qty,
        sum(case when trade_category = 'PURCHASE' then net_amount else 0 end) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as cumulative_purchase_cost
    from trades
),

-- ISSUE: Separate CTE for cost basis
with_cost_basis as (
    select
        *,
        case
            when cumulative_purchased_qty > 0
            then cumulative_purchase_cost / cumulative_purchased_qty
            else null
        end as avg_cost_basis
    from positions
),

-- ISSUE: Another pass for realized P&L
with_pnl as (
    select
        *,
        case
            when trade_category = 'SALE' and avg_cost_basis is not null
            then (execution_price - avg_cost_basis) * quantity
            else null
        end as realized_pnl,
        case
            when trade_category = 'SALE' and avg_cost_basis is not null and avg_cost_basis > 0
            then (execution_price - avg_cost_basis) / avg_cost_basis * 100
            else null
        end as realized_pnl_pct
    from with_cost_basis
)

select * from with_pnl
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_trade_pnl"} */;
[0m21:52:00.619719 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 0.489 seconds
[0m21:52:00.622762 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eaf7b553-23fb-4e89-a0d2-23f63291b762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1150b5fa0>]}
[0m21:52:00.623563 [info ] [Thread-3  ]: 9 of 12 OK created sql view model DEV_pipeline_b.int_trade_pnl ................. [[32mSUCCESS 1[0m in 0.50s]
[0m21:52:00.624128 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.int_trade_pnl
[0m21:52:00.624670 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.fact_trade_summary
[0m21:52:00.625185 [info ] [Thread-2  ]: 10 of 12 START sql table model DEV_pipeline_b.fact_trade_summary ............... [RUN]
[0m21:52:00.625647 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_trades_enriched, now model.bain_capital_portfolio_analytics.fact_trade_summary)
[0m21:52:00.625986 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_trade_summary
[0m21:52:00.631682 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_trade_summary"
[0m21:52:00.632670 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.fact_trade_summary
[0m21:52:00.635630 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_trade_summary"
[0m21:52:00.644011 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_trade_summary"
[0m21:52:00.644723 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.fact_trade_summary: create or replace transient table DBT_DEMO.DEV_pipeline_b.fact_trade_summary
    
    
    
    as (-- Pipeline B: Trade Analytics Pipeline
-- Model: fact_trade_summary
-- Description: Fact table for trade-level analysis
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-joins for prior trade lookups (should use LAG)
-- 2. Repeated window functions with same partitions
-- 3. Correlated subqueries for security-level aggregations
-- 4. Complex CASE statements repeated multiple times

with trade_pnl as (
    select * from DBT_DEMO.DEV_pipeline_b.int_trade_pnl
),

-- ISSUE: Getting portfolio data again (already available through joins upstream)
portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

-- ISSUE: Join that adds overhead
enriched as (
    select
        t.trade_id,
        t.portfolio_id,
        p.portfolio_name,
        p.portfolio_type,
        p.fund_id,
        t.security_id,
        t.ticker,
        t.security_name,
        t.security_type,
        t.asset_class,
        t.sector,
        t.trade_date,
        t.trade_type,
        t.trade_category,
        t.quantity,
        t.execution_price,
        t.net_amount,
        t.commission,
        t.running_position,
        t.avg_cost_basis,
        t.realized_pnl,
        t.realized_pnl_pct,
        -- ISSUE: Redundant date extractions (already done upstream)
        extract(year from t.trade_date) as trade_year,
        extract(month from t.trade_date) as trade_month,
        extract(quarter from t.trade_date) as trade_quarter,
        extract(dayofweek from t.trade_date) as trade_day_of_week,
        date_trunc('week', t.trade_date) as trade_week,
        date_trunc('month', t.trade_date) as trade_month_start
    from trade_pnl t
    left join portfolios p
        on t.portfolio_id = p.portfolio_id
),

-- ISSUE: Self-joins for prior trade comparisons (should use LAG)
-- Pre-compute trade sequence for self-join lookups
trade_sequences as (
    select
        *,
        row_number() over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
        ) as trade_seq
    from enriched
),

with_prior_trades as (
    select
        ts.*,
        -- ISSUE: Self-join for prior trade same security (should use LAG)
        ts_prior.execution_price as prior_trade_price,
        ts_prior.trade_date as prior_trade_date,
        ts_prior.quantity as prior_trade_quantity,
        -- ISSUE: Self-join for 5 trades ago (should use LAG offset)
        ts_5.execution_price as price_5_trades_ago,
        -- ISSUE: Self-join for 10 trades ago (should use LAG offset)
        ts_10.execution_price as price_10_trades_ago
    from trade_sequences ts
    left join trade_sequences ts_prior
        on ts.portfolio_id = ts_prior.portfolio_id
        and ts.security_id = ts_prior.security_id
        and ts_prior.trade_seq = ts.trade_seq - 1
    left join trade_sequences ts_5
        on ts.portfolio_id = ts_5.portfolio_id
        and ts.security_id = ts_5.security_id
        and ts_5.trade_seq = ts.trade_seq - 5
    left join trade_sequences ts_10
        on ts.portfolio_id = ts_10.portfolio_id
        and ts.security_id = ts_10.security_id
        and ts_10.trade_seq = ts.trade_seq - 10
),

-- ISSUE: Multiple window functions with repeated partitions
with_window_calcs as (
    select
        wpt.*,
        -- ISSUE: Running aggregations (repeated partition by portfolio_id, security_id)
        sum(quantity) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between unbounded preceding and current row
        ) as cumulative_quantity,
        sum(abs(net_amount)) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between unbounded preceding and current row
        ) as cumulative_trade_value,
        sum(coalesce(realized_pnl, 0)) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between unbounded preceding and current row
        ) as cumulative_realized_pnl,
        -- ISSUE: Moving averages (same partition repeated)
        avg(execution_price) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between 4 preceding and current row
        ) as rolling_5_trade_avg_price,
        avg(execution_price) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between 9 preceding and current row
        ) as rolling_10_trade_avg_price,
        avg(execution_price) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between 19 preceding and current row
        ) as rolling_20_trade_avg_price,
        -- ISSUE: More window calculations
        stddev(execution_price) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between 9 preceding and current row
        ) as rolling_10_trade_price_stddev,
        count(*) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between unbounded preceding and current row
        ) as trade_sequence_number,
        -- ISSUE: Rankings (same partition again)
        row_number() over (
            partition by wpt.portfolio_id, wpt.security_id, wpt.trade_category
            order by abs(wpt.net_amount) desc
        ) as size_rank_within_category
    from with_prior_trades wpt
),

-- ISSUE: Separate CTE for running trade stats (should be combined with window calcs above)
security_trade_aggs as (
    select
        portfolio_id,
        security_id,
        trade_date,
        trade_id,
        -- ISSUE: These window functions duplicate the partition from with_window_calcs
        count(*) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as total_portfolio_trades_this_security,
        avg(execution_price) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as avg_portfolio_price_this_security
    from enriched
),

-- ISSUE: Separate aggregation for fund-level volume (should be combined upstream)
fund_daily_volume as (
    select
        fund_id,
        security_id,
        trade_date,
        sum(abs(net_amount)) as fund_total_volume_same_security_same_day
    from enriched
    group by 1, 2, 3
),

with_security_context as (
    select
        wwc.*,
        sta.total_portfolio_trades_this_security,
        sta.avg_portfolio_price_this_security,
        fdv.fund_total_volume_same_security_same_day
    from with_window_calcs wwc
    left join security_trade_aggs sta
        on wwc.portfolio_id = sta.portfolio_id
        and wwc.security_id = sta.security_id
        and wwc.trade_id = sta.trade_id
    left join fund_daily_volume fdv
        on wwc.fund_id = fdv.fund_id
        and wwc.security_id = fdv.security_id
        and wwc.trade_date = fdv.trade_date
),

-- ISSUE: Complex derived metrics with repeated CASE statements
final as (
    select
        md5(cast(coalesce(cast(wsc.trade_id as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as trade_key,
        wsc.*,
        -- ISSUE: Price change calculations (repeated division logic)
        case
            when wsc.prior_trade_price is not null and wsc.prior_trade_price > 0
            then ((wsc.execution_price - wsc.prior_trade_price) / wsc.prior_trade_price) * 100
            else null
        end as price_change_from_prior_pct,
        case
            when wsc.price_5_trades_ago is not null and wsc.price_5_trades_ago > 0
            then ((wsc.execution_price - wsc.price_5_trades_ago) / wsc.price_5_trades_ago) * 100
            else null
        end as price_change_from_5_trades_ago_pct,
        case
            when wsc.rolling_20_trade_avg_price is not null and wsc.rolling_20_trade_avg_price > 0
            then ((wsc.execution_price - wsc.rolling_20_trade_avg_price) / wsc.rolling_20_trade_avg_price) * 100
            else null
        end as deviation_from_20_trade_avg_pct,
        -- ISSUE: Trade size classification (repeated CASE)
        case
            when abs(wsc.net_amount) >= 10000000 then 'BLOCK_TRADE'
            when abs(wsc.net_amount) >= 1000000 then 'LARGE'
            when abs(wsc.net_amount) >= 100000 then 'MEDIUM'
            when abs(wsc.net_amount) >= 10000 then 'SMALL'
            else 'MICRO'
        end as trade_size_category,
        -- ISSUE: Trade timing classification (complex nested CASE)
        case
            when wsc.execution_price > wsc.rolling_10_trade_avg_price * 1.1 then 'BOUGHT_HIGH'
            when wsc.execution_price > wsc.rolling_10_trade_avg_price * 1.03 then 'ABOVE_AVERAGE'
            when wsc.execution_price < wsc.rolling_10_trade_avg_price * 0.9 then 'BOUGHT_LOW'
            when wsc.execution_price < wsc.rolling_10_trade_avg_price * 0.97 then 'BELOW_AVERAGE'
            else 'AVERAGE'
        end as execution_quality,
        -- ISSUE: Momentum signal (repeated logic)
        case
            when wsc.rolling_5_trade_avg_price > wsc.rolling_20_trade_avg_price then 'UPTREND'
            when wsc.rolling_5_trade_avg_price < wsc.rolling_20_trade_avg_price then 'DOWNTREND'
            else 'NEUTRAL'
        end as price_momentum,
        -- ISSUE: Volatility classification
        case
            when wsc.rolling_10_trade_price_stddev < wsc.rolling_10_trade_avg_price * 0.02 then 'LOW_VOLATILITY'
            when wsc.rolling_10_trade_price_stddev < wsc.rolling_10_trade_avg_price * 0.05 then 'MODERATE_VOLATILITY'
            when wsc.rolling_10_trade_price_stddev < wsc.rolling_10_trade_avg_price * 0.10 then 'HIGH_VOLATILITY'
            else 'VERY_HIGH_VOLATILITY'
        end as price_volatility_regime,
        -- ISSUE: Trade frequency indicator
        case
            when wsc.trade_sequence_number >= 100 then 'VERY_ACTIVE'
            when wsc.trade_sequence_number >= 50 then 'ACTIVE'
            when wsc.trade_sequence_number >= 20 then 'MODERATE'
            when wsc.trade_sequence_number >= 5 then 'LIGHT'
            else 'FIRST_FEW_TRADES'
        end as trading_activity_level
    from with_security_context wsc
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_trade_summary"} */;
[0m21:52:01.270424 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 2.142 seconds
[0m21:52:01.274697 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eaf7b553-23fb-4e89-a0d2-23f63291b762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115396ca0>]}
[0m21:52:01.275882 [info ] [Thread-1  ]: 7 of 12 OK created sql table model DEV_pipeline_a.fact_cashflow_summary ........ [[32mSUCCESS 1[0m in 2.19s]
[0m21:52:01.276623 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m21:52:01.277445 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.fact_portfolio_positions
[0m21:52:01.277983 [info ] [Thread-3  ]: 11 of 12 START sql table model DEV_pipeline_b.fact_portfolio_positions ......... [RUN]
[0m21:52:01.278519 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_trade_pnl, now model.bain_capital_portfolio_analytics.fact_portfolio_positions)
[0m21:52:01.278913 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_portfolio_positions
[0m21:52:01.288799 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_portfolio_positions"
[0m21:52:01.289949 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.fact_portfolio_positions
[0m21:52:01.293651 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_portfolio_positions"
[0m21:52:01.301271 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_portfolio_positions"
[0m21:52:01.301983 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.fact_portfolio_positions: create or replace transient table DBT_DEMO.DEV_pipeline_b.fact_portfolio_positions
    
    
    
    as (-- Pipeline B: Trade Analytics Pipeline
-- Model: fact_portfolio_positions
-- Description: Current position snapshot by portfolio and security
-- DEPENDENCY: Uses fact_cashflow_summary from Pipeline A for portfolio cash context
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Gets latest position via subquery (should use QUALIFY)
-- 2. Self-joins for historical position lookups
-- 3. Correlated subqueries for portfolio-level aggregations
-- 4. Repeated window functions

with trade_pnl as (
    select * from DBT_DEMO.DEV_pipeline_b.int_trade_pnl
),

-- DEPENDENCY ON PIPELINE A: Get cashflow context for each portfolio
cashflow_summary as (
    select * from DBT_DEMO.DEV_pipeline_a.fact_cashflow_summary
),

-- Aggregate cashflows by portfolio to get total contributions/distributions
portfolio_cashflows as (
    select
        portfolio_id,
        sum(case when cashflow_type = 'CONTRIBUTION' then cumulative_total else 0 end) as total_contributions,
        sum(case when cashflow_type = 'DISTRIBUTION' then abs(cumulative_total) else 0 end) as total_distributions,
        max(cashflow_month) as last_cashflow_date
    from cashflow_summary
    group by portfolio_id
),

latest_positions as (
    select *
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id, security_id
                order by trade_date desc, trade_id desc
            ) as rn
        from trade_pnl
    )
    where rn = 1
),

-- ISSUE: Self-join to get position 30 days ago
positions_30d_ago as (
    select
        portfolio_id,
        security_id,
        running_position as position_30d_ago,
        avg_cost_basis as cost_basis_30d_ago
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id, security_id
                order by trade_date desc, trade_id desc
            ) as rn
        from trade_pnl
        where trade_date <= dateadd(day, -30, current_date())
    )
    where rn = 1
),

-- ISSUE: Self-join to get position 90 days ago
positions_90d_ago as (
    select
        portfolio_id,
        security_id,
        running_position as position_90d_ago,
        avg_cost_basis as cost_basis_90d_ago
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id, security_id
                order by trade_date desc, trade_id desc
            ) as rn
        from trade_pnl
        where trade_date <= dateadd(day, -90, current_date())
    )
    where rn = 1
),

market_prices as (
    select
        security_id,
        close_price as current_price,
        price_date
    from (
        select
            security_id,
            close_price,
            price_date,
            row_number() over (partition by security_id order by price_date desc) as rn
        from DBT_DEMO.DEV_pipeline_b.stg_market_prices
    )
    where rn = 1  -- ISSUE: Again, should use QUALIFY
),

-- ISSUE: Get historical prices for comparison
market_prices_30d_ago as (
    select
        security_id,
        close_price as price_30d_ago
    from (
        select
            security_id,
            close_price,
            row_number() over (partition by security_id order by price_date desc) as rn
        from DBT_DEMO.DEV_pipeline_b.stg_market_prices
        where price_date <= dateadd(day, -30, current_date())
    )
    where rn = 1
),

-- ISSUE: Join all the position snapshots together
enriched_positions as (
    select
        lp.*,
        mp.current_price,
        mp.price_date as price_as_of_date,
        p30.position_30d_ago,
        p30.cost_basis_30d_ago,
        p90.position_90d_ago,
        p90.cost_basis_90d_ago,
        mp30.price_30d_ago,
        pcf.total_contributions,
        pcf.total_distributions,
        pcf.last_cashflow_date
    from latest_positions lp
    left join market_prices mp
        on lp.security_id = mp.security_id
    left join positions_30d_ago p30
        on lp.portfolio_id = p30.portfolio_id
        and lp.security_id = p30.security_id
    left join positions_90d_ago p90
        on lp.portfolio_id = p90.portfolio_id
        and lp.security_id = p90.security_id
    left join market_prices_30d_ago mp30
        on lp.security_id = mp30.security_id
    left join portfolio_cashflows pcf
        on lp.portfolio_id = pcf.portfolio_id
    where lp.running_position != 0
),

-- ISSUE: Window functions for portfolio-level context
with_portfolio_context as (
    select
        ep.*,
        -- ISSUE: Repeated partition by portfolio_id
        sum(ep.running_position * ep.current_price) over (
            partition by ep.portfolio_id
        ) as portfolio_total_market_value,
        sum(ep.running_position * ep.avg_cost_basis) over (
            partition by ep.portfolio_id
        ) as portfolio_total_cost_basis,
        count(*) over (
            partition by ep.portfolio_id
        ) as portfolio_position_count,
        -- ISSUE: Rankings
        row_number() over (
            partition by ep.portfolio_id
            order by (ep.running_position * ep.current_price) desc
        ) as position_size_rank,
        row_number() over (
            partition by ep.portfolio_id
            order by ((ep.current_price - ep.avg_cost_basis) / nullif(ep.avg_cost_basis, 0)) desc
        ) as position_return_rank
    from enriched_positions ep
),

-- ISSUE: Separate aggregation for sector context (should use window functions)
sector_aggs as (
    select
        portfolio_id,
        sector,
        sum(running_position * current_price) as sector_market_value,
        count(*) as sector_position_count
    from enriched_positions
    group by 1, 2
),

with_sector_context as (
    select
        wpc.*,
        sa.sector_market_value,
        sa.sector_position_count
    from with_portfolio_context wpc
    left join sector_aggs sa
        on wpc.portfolio_id = sa.portfolio_id
        and wpc.sector = sa.sector
),

-- ISSUE: Complex calculations in final select
final as (
    select
        md5(cast(coalesce(cast(wsc.portfolio_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wsc.security_id as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as position_key,
        wsc.portfolio_id,
        wsc.security_id,
        wsc.ticker,
        wsc.security_name,
        wsc.sector,
        wsc.asset_class,
        wsc.running_position as current_quantity,
        wsc.avg_cost_basis,
        wsc.current_price,
        wsc.price_as_of_date,
        -- Core calculations
        wsc.running_position * wsc.avg_cost_basis as cost_basis_value,
        wsc.running_position * wsc.current_price as market_value,
        (wsc.running_position * wsc.current_price) - (wsc.running_position * wsc.avg_cost_basis) as unrealized_pnl,
        -- ISSUE: Repeated division logic
        case
            when wsc.avg_cost_basis > 0
            then ((wsc.current_price - wsc.avg_cost_basis) / wsc.avg_cost_basis) * 100
            else null
        end as unrealized_pnl_pct,
        -- Portfolio context
        wsc.portfolio_total_market_value,
        wsc.portfolio_total_cost_basis,
        wsc.portfolio_position_count,
        -- ISSUE: Weight calculation (repeated division)
        case
            when wsc.portfolio_total_market_value > 0
            then ((wsc.running_position * wsc.current_price) / wsc.portfolio_total_market_value) * 100
            else null
        end as portfolio_weight_pct,
        -- Sector context
        wsc.sector_market_value,
        wsc.sector_position_count,
        case
            when wsc.sector_market_value > 0
            then ((wsc.running_position * wsc.current_price) / wsc.sector_market_value) * 100
            else null
        end as sector_weight_pct,
        -- Historical comparison
        wsc.position_30d_ago,
        wsc.position_90d_ago,
        wsc.position_30d_ago - wsc.running_position as position_change_30d,
        wsc.position_90d_ago - wsc.running_position as position_change_90d,
        -- Price momentum
        wsc.price_30d_ago,
        case
            when wsc.price_30d_ago > 0
            then ((wsc.current_price - wsc.price_30d_ago) / wsc.price_30d_ago) * 100
            else null
        end as price_change_30d_pct,
        -- Cashflow context from Pipeline A
        wsc.total_contributions,
        wsc.total_distributions,
        wsc.last_cashflow_date,
        -- Rankings
        wsc.position_size_rank,
        wsc.position_return_rank,
        -- ISSUE: Complex classification
        case
            when ((wsc.running_position * wsc.current_price) / nullif(wsc.portfolio_total_market_value, 0)) > 0.10 then 'CONCENTRATED'
            when ((wsc.running_position * wsc.current_price) / nullif(wsc.portfolio_total_market_value, 0)) > 0.05 then 'SIGNIFICANT'
            when ((wsc.running_position * wsc.current_price) / nullif(wsc.portfolio_total_market_value, 0)) > 0.02 then 'MODERATE'
            else 'SMALL'
        end as position_size_category,
        wsc.cumulative_purchase_cost as total_invested,
        current_timestamp() as snapshot_timestamp
    from with_sector_context wsc
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_portfolio_positions"} */;
[0m21:52:11.335355 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 10.032 seconds
[0m21:52:11.338964 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eaf7b553-23fb-4e89-a0d2-23f63291b762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fd5d1c0>]}
[0m21:52:11.340231 [info ] [Thread-3  ]: 11 of 12 OK created sql table model DEV_pipeline_b.fact_portfolio_positions .... [[32mSUCCESS 1[0m in 10.06s]
[0m21:52:11.341225 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.fact_portfolio_positions
[0m21:52:29.580386 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 28.934 seconds
[0m21:52:29.584403 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eaf7b553-23fb-4e89-a0d2-23f63291b762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1150901f0>]}
[0m21:52:29.585605 [info ] [Thread-2  ]: 10 of 12 OK created sql table model DEV_pipeline_b.fact_trade_summary .......... [[32mSUCCESS 1[0m in 28.96s]
[0m21:52:29.586362 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.fact_trade_summary
[0m21:52:29.587582 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.report_trading_performance
[0m21:52:29.588346 [info ] [Thread-1  ]: 12 of 12 START sql table model DEV_pipeline_b.report_trading_performance ....... [RUN]
[0m21:52:29.588998 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.fact_cashflow_summary, now model.bain_capital_portfolio_analytics.report_trading_performance)
[0m21:52:29.589551 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.report_trading_performance
[0m21:52:29.596288 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.report_trading_performance"
[0m21:52:29.597555 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.report_trading_performance
[0m21:52:29.601918 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.report_trading_performance"
[0m21:52:29.605578 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.report_trading_performance"
[0m21:52:29.606028 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.report_trading_performance: create or replace transient table DBT_DEMO.DEV_pipeline_b.report_trading_performance
    
    
    
    as (-- Pipeline B: Trade Analytics Pipeline
-- Model: report_trading_performance
-- Description: Trading performance report for IC dashboard
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Re-aggregates fact data that could be pre-computed
-- 2. Complex window functions repeated from other models
-- 3. Multiple CTEs that could be consolidated

with trades as (
    select * from DBT_DEMO.DEV_pipeline_b.fact_trade_summary
),

positions as (
    select * from DBT_DEMO.DEV_pipeline_b.fact_portfolio_positions
),

-- ISSUE: Re-aggregating trade data by portfolio/month
trade_metrics as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        trade_year,
        trade_month,
        count(distinct trade_id) as trade_count,
        count(distinct security_id) as securities_traded,
        sum(case when trade_category = 'PURCHASE' then 1 else 0 end) as buy_count,
        sum(case when trade_category = 'SALE' then 1 else 0 end) as sell_count,
        sum(case when trade_category = 'PURCHASE' then net_amount else 0 end) as total_purchases,
        sum(case when trade_category = 'SALE' then abs(net_amount) else 0 end) as total_sales,
        sum(coalesce(realized_pnl, 0)) as total_realized_pnl,
        avg(case when realized_pnl is not null then realized_pnl_pct else null end) as avg_realized_return_pct
    from trades
    group by 1,2,3,4,5,6
),

-- ISSUE: Aggregating positions separately
position_metrics as (
    select
        portfolio_id,
        count(distinct security_id) as position_count,
        sum(market_value) as total_market_value,
        sum(cost_basis_value) as total_cost_basis,
        sum(unrealized_pnl) as total_unrealized_pnl,
        avg(unrealized_pnl_pct) as avg_unrealized_return_pct
    from positions
    group by 1
),

-- ISSUE: Window functions for running totals (repeated pattern)
with_running_totals as (
    select
        tm.*,
        sum(total_realized_pnl) over (
            partition by tm.portfolio_id
            order by tm.trade_year, tm.trade_month
            rows between unbounded preceding and current row
        ) as cumulative_realized_pnl,
        sum(total_purchases) over (
            partition by tm.portfolio_id
            order by tm.trade_year, tm.trade_month
            rows between unbounded preceding and current row
        ) as cumulative_invested,
        -- ISSUE: Multiple LAG functions
        lag(total_realized_pnl, 1) over (
            partition by tm.portfolio_id
            order by tm.trade_year, tm.trade_month
        ) as prior_month_pnl,
        lag(trade_count, 1) over (
            partition by tm.portfolio_id
            order by tm.trade_year, tm.trade_month
        ) as prior_month_trades
    from trade_metrics tm
),

-- ISSUE: Final join adds more complexity
final as (
    select
        wrt.*,
        pm.position_count,
        pm.total_market_value,
        pm.total_cost_basis,
        pm.total_unrealized_pnl,
        pm.avg_unrealized_return_pct,
        -- Combined metrics
        wrt.total_realized_pnl + coalesce(pm.total_unrealized_pnl, 0) as total_pnl,
        case
            when pm.total_cost_basis > 0
            then ((pm.total_market_value + wrt.cumulative_realized_pnl) - pm.total_cost_basis) / pm.total_cost_basis * 100
            else null
        end as total_return_pct
    from with_running_totals wrt
    left join position_metrics pm
        on wrt.portfolio_id = pm.portfolio_id
)

select * from final
order by portfolio_id, trade_year, trade_month
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.report_trading_performance"} */;
[0m21:52:31.984858 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 2.378 seconds
[0m21:52:31.988730 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eaf7b553-23fb-4e89-a0d2-23f63291b762', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1150901f0>]}
[0m21:52:31.990015 [info ] [Thread-1  ]: 12 of 12 OK created sql table model DEV_pipeline_b.report_trading_performance .. [[32mSUCCESS 1[0m in 2.40s]
[0m21:52:31.990795 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.report_trading_performance
[0m21:52:31.992666 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:52:31.993027 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.report_trading_performance' was left open.
[0m21:52:31.993366 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.report_trading_performance: Close
[0m21:52:32.347536 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.fact_trade_summary' was left open.
[0m21:52:32.348478 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.fact_trade_summary: Close
[0m21:52:32.687522 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.fact_portfolio_positions' was left open.
[0m21:52:32.688522 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.fact_portfolio_positions: Close
[0m21:52:33.059148 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.stg_securities' was left open.
[0m21:52:33.060148 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.stg_securities: Close
[0m21:52:33.500219 [info ] [MainThread]: 
[0m21:52:33.501196 [info ] [MainThread]: Finished running 4 table models, 8 view models in 0 hours 0 minutes and 38.92 seconds (38.92s).
[0m21:52:33.503976 [debug] [MainThread]: Command end result
[0m21:52:33.622875 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/manifest.json
[0m21:52:33.624889 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/semantic_manifest.json
[0m21:52:33.631720 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/nedazarei/Documents/turintech/dbtproject/target/run_results.json
[0m21:52:33.632066 [info ] [MainThread]: 
[0m21:52:33.632421 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:52:33.632707 [info ] [MainThread]: 
[0m21:52:33.632997 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=12
[0m21:52:33.635320 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 41.09632, "process_in_blocks": "0", "process_kernel_time": 0.548937, "process_mem_max_rss": "189284352", "process_out_blocks": "0", "process_user_time": 3.811168}
[0m21:52:33.635675 [debug] [MainThread]: Command `dbt run` succeeded at 21:52:33.635605 after 41.10 seconds
[0m21:52:33.636000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10be4f1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e354880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11509eeb0>]}
[0m21:52:33.636328 [debug] [MainThread]: Flushing usage events
[0m21:52:34.661663 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:52:36.403590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10be07280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ceb78e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ceb7700>]}


============================== 21:52:36.409007 | ee178f1b-c4e2-4332-9309-6e2c52676b04 ==============================
[0m21:52:36.409007 [info ] [MainThread]: Running with dbt=1.11.0-b3
[0m21:52:36.409514 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'version_check': 'True', 'invocation_command': 'dbt run --select +pipeline_c.*', 'write_json': 'True', 'quiet': 'False', 'no_print': 'None', 'profiles_dir': '/Users/nedazarei/Documents/turintech/dbtproject', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'introspect': 'True', 'static_parser': 'True', 'debug': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'indirect_selection': 'eager', 'empty': 'False', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'log_path': '/Users/nedazarei/Documents/turintech/dbtproject/logs', 'send_anonymous_usage_stats': 'True', 'target_path': 'None', 'log_cache_events': 'False', 'printer_width': '80'}
[0m21:52:36.869680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e333280>]}
[0m21:52:36.943641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e333eb0>]}
[0m21:52:36.944711 [info ] [MainThread]: Registered adapter: snowflake=1.10.2
[0m21:52:37.070484 [debug] [MainThread]: checksum: e63134dccc1251cfb572caf0e4aa952f030c125ee3634f3bf2c0a2e1bb6ae349, vars: {}, profile: , target: , version: 1.11.0b3
[0m21:52:37.264962 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:52:37.265396 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:52:37.384813 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ceb7370>]}
[0m21:52:37.539756 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/manifest.json
[0m21:52:37.542064 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/semantic_manifest.json
[0m21:52:37.562916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f1a8370>]}
[0m21:52:37.563513 [info ] [MainThread]: Found 32 models, 90 data tests, 5 seeds, 12 sources, 632 macros
[0m21:52:37.563839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f292a30>]}
[0m21:52:37.567618 [info ] [MainThread]: 
[0m21:52:37.567944 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m21:52:37.568191 [info ] [MainThread]: 
[0m21:52:37.568681 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m21:52:37.574989 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO'
[0m21:52:37.575463 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO'
[0m21:52:37.583925 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO'
[0m21:52:37.632366 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO"
[0m21:52:37.632893 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO"
[0m21:52:37.633267 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO"
[0m21:52:37.633529 [debug] [ThreadPool]: On list_DBT_DEMO: show terse schemas in database DBT_DEMO
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO"} */
[0m21:52:37.633804 [debug] [ThreadPool]: On list_DBT_DEMO: show terse schemas in database DBT_DEMO
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO"} */
[0m21:52:37.634064 [debug] [ThreadPool]: On list_DBT_DEMO: show terse schemas in database DBT_DEMO
    limit 10000
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO"} */
[0m21:52:37.634309 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:52:37.634549 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:52:37.634778 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:52:41.256296 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 3.622 seconds
[0m21:52:41.436964 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 3.802 seconds
[0m21:52:41.470489 [debug] [ThreadPool]: SQL status: SUCCESS 6 in 3.835 seconds
[0m21:52:41.479648 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DBT_DEMO, now list_DBT_DEMO_DEV_pipeline_b)
[0m21:52:41.480463 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DBT_DEMO, now list_DBT_DEMO_DEV_pipeline_c)
[0m21:52:41.481095 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DBT_DEMO, now list_DBT_DEMO_DEV)
[0m21:52:41.488708 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DBT_DEMO_DEV_pipeline_a'
[0m21:52:41.498705 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV_pipeline_b"
[0m21:52:41.501261 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV_pipeline_c"
[0m21:52:41.503608 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV"
[0m21:52:41.505835 [debug] [ThreadPool]: Using snowflake connection "list_DBT_DEMO_DEV_pipeline_a"
[0m21:52:41.506158 [debug] [ThreadPool]: On list_DBT_DEMO_DEV_pipeline_b: show objects in DBT_DEMO.DEV_pipeline_b
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV_pipeline_b"} */;
[0m21:52:41.506443 [debug] [ThreadPool]: On list_DBT_DEMO_DEV_pipeline_c: show objects in DBT_DEMO.DEV_pipeline_c
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV_pipeline_c"} */;
[0m21:52:41.506719 [debug] [ThreadPool]: On list_DBT_DEMO_DEV: show objects in DBT_DEMO.DEV
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV"} */;
[0m21:52:41.506989 [debug] [ThreadPool]: On list_DBT_DEMO_DEV_pipeline_a: show objects in DBT_DEMO.DEV_pipeline_a
    limit 10000
    

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "connection_name": "list_DBT_DEMO_DEV_pipeline_a"} */;
[0m21:52:41.507968 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:52:41.789909 [debug] [ThreadPool]: SQL status: SUCCESS 17 in 0.282 seconds
[0m21:52:41.799763 [debug] [ThreadPool]: SQL status: SUCCESS 9 in 0.292 seconds
[0m21:52:41.826176 [debug] [ThreadPool]: SQL status: SUCCESS 19 in 0.319 seconds
[0m21:52:44.282445 [debug] [ThreadPool]: SQL status: SUCCESS 4 in 2.774 seconds
[0m21:52:44.287270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f1ef730>]}
[0m21:52:44.291248 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.stg_benchmark_returns
[0m21:52:44.291901 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.stg_benchmarks
[0m21:52:44.292382 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.stg_brokers
[0m21:52:44.293417 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.stg_cashflows
[0m21:52:44.293025 [info ] [Thread-1  ]: 1 of 30 START sql view model DEV_pipeline_c.stg_benchmark_returns .............. [RUN]
[0m21:52:44.294107 [info ] [Thread-2  ]: 2 of 30 START sql view model DEV_pipeline_c.stg_benchmarks ..................... [RUN]
[0m21:52:44.294647 [info ] [Thread-3  ]: 3 of 30 START sql view model DEV_pipeline_b.stg_brokers ........................ [RUN]
[0m21:52:44.295130 [info ] [Thread-4  ]: 4 of 30 START sql view model DEV_pipeline_a.stg_cashflows ...................... [RUN]
[0m21:52:44.295645 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV_pipeline_b, now model.bain_capital_portfolio_analytics.stg_benchmark_returns)
[0m21:52:44.296052 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV_pipeline_c, now model.bain_capital_portfolio_analytics.stg_benchmarks)
[0m21:52:44.296443 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV, now model.bain_capital_portfolio_analytics.stg_brokers)
[0m21:52:44.296890 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly list_DBT_DEMO_DEV_pipeline_a, now model.bain_capital_portfolio_analytics.stg_cashflows)
[0m21:52:44.297307 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_benchmark_returns
[0m21:52:44.297666 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_benchmarks
[0m21:52:44.298015 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_brokers
[0m21:52:44.298353 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_cashflows
[0m21:52:44.307538 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_benchmark_returns"
[0m21:52:44.310368 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_benchmarks"
[0m21:52:44.313063 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_brokers"
[0m21:52:44.317511 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_cashflows"
[0m21:52:44.318474 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.stg_benchmark_returns
[0m21:52:44.318819 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.stg_cashflows
[0m21:52:44.319129 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.stg_benchmarks
[0m21:52:44.319408 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.stg_brokers
[0m21:52:44.358609 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_benchmarks"
[0m21:52:44.361516 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_cashflows"
[0m21:52:44.355986 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_benchmark_returns"
[0m21:52:44.364991 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_brokers"
[0m21:52:44.366600 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_cashflows"
[0m21:52:44.367706 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_benchmarks"
[0m21:52:44.368119 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.stg_cashflows: create or replace   view DBT_DEMO.DEV_pipeline_a.stg_cashflows
  
  
  
  
  as (
    -- Pipeline A: Simple Cashflow Pipeline
-- Model: stg_cashflows
-- Description: Staging model for raw cashflow data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Unnecessary DISTINCT (source already unique)
-- 2. Late filtering (should push date filter upstream)
-- 3. Non-optimal date casting

with source as (
    select distinct  -- ISSUE: Unnecessary DISTINCT, source has unique constraint
        cashflow_id,
        portfolio_id,
        cashflow_type,
        cashflow_date,
        amount,
        currency,
        created_at,
        updated_at
    from DBT_DEMO.DEV.cashflows
),

-- ISSUE: Heavy transformation before filtering
converted as (
    select
        cashflow_id,
        portfolio_id,
        upper(cashflow_type) as cashflow_type,
        cast(cashflow_date as date) as cashflow_date,
        cast(amount as decimal(18,2)) as amount,
        upper(currency) as currency,
        cast(created_at as timestamp) as created_at,
        cast(updated_at as timestamp) as updated_at
    from source
),

-- ISSUE: Filter applied after transformation, should be earlier
filtered as (
    select *
    from converted
    where cashflow_date >= '2020-01-01'
      and cashflow_date <= '2024-12-31'
)

select * from filtered
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_cashflows"} */;
[0m21:52:44.369870 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_benchmark_returns"
[0m21:52:44.370283 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.stg_benchmarks: create or replace   view DBT_DEMO.DEV_pipeline_c.stg_benchmarks
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: stg_benchmarks
-- Description: Benchmark index data for performance comparison

with source as (
    select
        benchmark_id,
        benchmark_name,
        benchmark_ticker,
        asset_class,
        region,
        is_active,
        created_at,
        updated_at
    from DBT_DEMO.DEV.benchmarks
),

-- ISSUE: Subquery for deduplication
deduplicated as (
    select *
    from (
        select
            *,
            row_number() over (partition by benchmark_id order by updated_at desc) as rn
        from source
    )
    where rn = 1
)

select
    benchmark_id,
    trim(benchmark_name) as benchmark_name,
    upper(trim(benchmark_ticker)) as benchmark_ticker,
    upper(asset_class) as asset_class,
    upper(region) as region,
    is_active,
    created_at,
    updated_at
from deduplicated
where is_active = true
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_benchmarks"} */;
[0m21:52:44.371366 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_brokers"
[0m21:52:44.371863 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.stg_benchmark_returns: create or replace   view DBT_DEMO.DEV_pipeline_c.stg_benchmark_returns
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: stg_benchmark_returns
-- Description: Daily benchmark return data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-join for cumulative returns (inefficient)
-- 2. Multiple window functions

with source as (
    select
        benchmark_id,
        return_date,
        daily_return,
        index_level,
        created_at
    from DBT_DEMO.DEV.benchmark_returns
    where return_date >= '2020-01-01'
),

-- ISSUE: Multiple window functions that could be consolidated
with_cumulative as (
    select
        benchmark_id,
        return_date,
        daily_return,
        index_level,
        -- ISSUE: Separate window functions for each period
        exp(sum(ln(1 + daily_return)) over (
            partition by benchmark_id
            order by return_date
            rows between unbounded preceding and current row
        )) - 1 as cumulative_return,
        exp(sum(ln(1 + daily_return)) over (
            partition by benchmark_id
            order by return_date
            rows between 29 preceding and current row
        )) - 1 as return_30d,
        exp(sum(ln(1 + daily_return)) over (
            partition by benchmark_id
            order by return_date
            rows between 89 preceding and current row
        )) - 1 as return_90d,
        exp(sum(ln(1 + daily_return)) over (
            partition by benchmark_id
            order by return_date
            rows between 364 preceding and current row
        )) - 1 as return_1y,
        stddev(daily_return) over (
            partition by benchmark_id
            order by return_date
            rows between 251 preceding and current row
        ) * sqrt(252) as annualized_volatility,
        created_at
    from source
)

select * from with_cumulative
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_benchmark_returns"} */;
[0m21:52:44.372666 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.stg_brokers: create or replace   view DBT_DEMO.DEV_pipeline_b.stg_brokers
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: stg_brokers
-- Description: Staging model for broker information

with source as (
    select
        broker_id,
        broker_name,
        broker_type,
        region,
        is_active,
        commission_rate,
        created_at,
        updated_at
    from DBT_DEMO.DEV.brokers
),

deduplicated as (
    select *
    from (
        select
            *,
            row_number() over (partition by broker_id order by updated_at desc) as rn
        from source
    )
    where rn = 1
)

select
    broker_id,
    trim(broker_name) as broker_name,
    upper(broker_type) as broker_type,
    upper(region) as region,
    is_active,
    commission_rate,
    created_at,
    updated_at
from deduplicated
where is_active = true
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_brokers"} */;
[0m21:52:44.675774 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.302 seconds
[0m21:52:44.677214 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.305 seconds
[0m21:52:44.678261 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 0.306 seconds
[0m21:52:44.714699 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115127550>]}
[0m21:52:44.715114 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1151272e0>]}
[0m21:52:44.716053 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1151276a0>]}
[0m21:52:44.715789 [info ] [Thread-1  ]: 1 of 30 OK created sql view model DEV_pipeline_c.stg_benchmark_returns ......... [[32mSUCCESS 1[0m in 0.41s]
[0m21:52:44.716660 [info ] [Thread-4  ]: 4 of 30 OK created sql view model DEV_pipeline_a.stg_cashflows ................. [[32mSUCCESS 1[0m in 0.42s]
[0m21:52:44.717147 [info ] [Thread-2  ]: 2 of 30 OK created sql view model DEV_pipeline_c.stg_benchmarks ................ [[32mSUCCESS 1[0m in 0.42s]
[0m21:52:44.717642 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.stg_benchmark_returns
[0m21:52:44.718066 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.stg_cashflows
[0m21:52:44.718469 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.stg_benchmarks
[0m21:52:44.718836 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.stg_fund_hierarchy
[0m21:52:44.719287 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.stg_market_prices
[0m21:52:44.719694 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks
[0m21:52:44.720133 [info ] [Thread-1  ]: 5 of 30 START sql view model DEV_pipeline_c.stg_fund_hierarchy ................. [RUN]
[0m21:52:44.720549 [info ] [Thread-4  ]: 6 of 30 START sql view model DEV_pipeline_b.stg_market_prices .................. [RUN]
[0m21:52:44.720939 [info ] [Thread-2  ]: 7 of 30 START sql view model DEV_pipeline_c.stg_portfolio_benchmarks ........... [RUN]
[0m21:52:44.721325 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_benchmark_returns, now model.bain_capital_portfolio_analytics.stg_fund_hierarchy)
[0m21:52:44.721653 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_cashflows, now model.bain_capital_portfolio_analytics.stg_market_prices)
[0m21:52:44.721988 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_benchmarks, now model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks)
[0m21:52:44.722311 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_fund_hierarchy
[0m21:52:44.722627 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_market_prices
[0m21:52:44.722912 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks
[0m21:52:44.726247 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_fund_hierarchy"
[0m21:52:44.729415 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_market_prices"
[0m21:52:44.732327 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks"
[0m21:52:44.733863 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 0.360 seconds
[0m21:52:44.735857 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115114c70>]}
[0m21:52:44.736455 [info ] [Thread-3  ]: 3 of 30 OK created sql view model DEV_pipeline_b.stg_brokers ................... [[32mSUCCESS 1[0m in 0.44s]
[0m21:52:44.736952 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.stg_brokers
[0m21:52:44.737387 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.stg_portfolios
[0m21:52:44.737820 [info ] [Thread-3  ]: 8 of 30 START sql view model DEV_pipeline_a.stg_portfolios ..................... [RUN]
[0m21:52:44.738183 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks
[0m21:52:44.738600 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_brokers, now model.bain_capital_portfolio_analytics.stg_portfolios)
[0m21:52:44.738926 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.stg_market_prices
[0m21:52:44.739260 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.stg_fund_hierarchy
[0m21:52:44.743277 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks"
[0m21:52:44.743612 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_portfolios
[0m21:52:44.746275 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_market_prices"
[0m21:52:44.749019 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_fund_hierarchy"
[0m21:52:44.751876 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_portfolios"
[0m21:52:44.753234 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks"
[0m21:52:44.755627 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_market_prices"
[0m21:52:44.756499 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_fund_hierarchy"
[0m21:52:44.756861 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks: create or replace   view DBT_DEMO.DEV_pipeline_c.stg_portfolio_benchmarks
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: stg_portfolio_benchmarks
-- Description: Mapping of portfolios to their benchmarks

with source as (
    select
        portfolio_id,
        benchmark_id,
        is_primary,
        start_date,
        end_date,
        created_at,
        updated_at
    from DBT_DEMO.DEV.portfolio_benchmarks
)

select
    portfolio_id,
    benchmark_id,
    is_primary,
    cast(start_date as date) as start_date,
    cast(end_date as date) as end_date,
    created_at,
    updated_at
from source
where end_date is null or end_date >= current_date()
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks"} */;
[0m21:52:44.757334 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.stg_market_prices: create or replace   view DBT_DEMO.DEV_pipeline_b.stg_market_prices
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: stg_market_prices
-- Description: Staging model for daily market prices
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-join for prior day prices (inefficient)
-- 2. Late aggregation
-- 3. Multiple window functions that could be consolidated

with source as (
    select
        security_id,
        price_date,
        open_price,
        high_price,
        low_price,
        close_price,
        volume,
        created_at
    from DBT_DEMO.DEV.market_prices
    where price_date >= '2020-01-01'
),

-- ISSUE: Self-join to get prior day price (should use LAG)
with_prior_day as (
    select
        curr.security_id,
        curr.price_date,
        curr.open_price,
        curr.high_price,
        curr.low_price,
        curr.close_price,
        curr.volume,
        prev.close_price as prior_close,
        prev.volume as prior_volume
    from source curr
    left join source prev
        on curr.security_id = prev.security_id
        and curr.price_date = dateadd('day', 1, prev.price_date)  -- ISSUE: Doesn't handle weekends
),

-- ISSUE: Multiple separate window functions
with_returns as (
    select
        *,
        -- Daily return
        case
            when prior_close > 0
            then (close_price - prior_close) / prior_close
            else null
        end as daily_return,
        -- ISSUE: These could be computed together
        avg(close_price) over (
            partition by security_id
            order by price_date
            rows between 19 preceding and current row
        ) as ma_20,
        avg(close_price) over (
            partition by security_id
            order by price_date
            rows between 49 preceding and current row
        ) as ma_50,
        avg(close_price) over (
            partition by security_id
            order by price_date
            rows between 199 preceding and current row
        ) as ma_200,
        stddev(close_price) over (
            partition by security_id
            order by price_date
            rows between 19 preceding and current row
        ) as volatility_20d,
        avg(volume) over (
            partition by security_id
            order by price_date
            rows between 19 preceding and current row
        ) as avg_volume_20d
    from with_prior_day
),

-- ISSUE: Another pass for more calculations
final as (
    select
        *,
        case
            when ma_20 > ma_50 and ma_50 > ma_200 then 'BULLISH'
            when ma_20 < ma_50 and ma_50 < ma_200 then 'BEARISH'
            else 'NEUTRAL'
        end as trend_signal,
        case
            when volume > avg_volume_20d * 2 then 'HIGH'
            when volume < avg_volume_20d * 0.5 then 'LOW'
            else 'NORMAL'
        end as volume_signal
    from with_returns
)

select * from final
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_market_prices"} */;
[0m21:52:44.757775 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.stg_portfolios
[0m21:52:44.758087 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.stg_fund_hierarchy: create or replace   view DBT_DEMO.DEV_pipeline_c.stg_fund_hierarchy
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: stg_fund_hierarchy
-- Description: Fund and portfolio hierarchy for roll-up reporting

with source as (
    select
        entity_id,
        entity_name,
        entity_type,
        parent_entity_id,
        hierarchy_level,
        is_active,
        created_at,
        updated_at
    from DBT_DEMO.DEV.fund_hierarchy
)

select
    entity_id,
    trim(entity_name) as entity_name,
    upper(entity_type) as entity_type,
    parent_entity_id,
    hierarchy_level,
    is_active,
    created_at,
    updated_at
from source
where is_active = true
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_fund_hierarchy"} */;
[0m21:52:44.761183 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_portfolios"
[0m21:52:44.763592 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_portfolios"
[0m21:52:44.764482 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.stg_portfolios: create or replace   view DBT_DEMO.DEV_pipeline_a.stg_portfolios
  
  
  
  
  as (
    -- Pipeline A: Simple Cashflow Pipeline
-- Model: stg_portfolios
-- Description: Staging model for portfolio master data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Subquery for deduplication instead of QUALIFY
-- 2. Multiple passes over data

with source as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        inception_date,
        status,
        currency,
        created_at,
        updated_at,
        row_number() over (
            partition by portfolio_id
            order by updated_at desc
        ) as rn
    from DBT_DEMO.DEV.portfolios
),

-- ISSUE: Using subquery filter instead of QUALIFY
deduplicated as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        inception_date,
        status,
        currency,
        created_at,
        updated_at
    from source
    where rn = 1  -- ISSUE: Should use QUALIFY in Snowflake
),

-- ISSUE: Another pass just for active filter
active_only as (
    select *
    from deduplicated
    where status = 'ACTIVE'
)

select * from active_only
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_portfolios"} */;
[0m21:52:45.043377 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 0.284 seconds
[0m21:52:45.047897 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11523bf10>]}
[0m21:52:45.049190 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.288 seconds
[0m21:52:45.055672 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ee7fa30>]}
[0m21:52:45.050791 [info ] [Thread-4  ]: 6 of 30 OK created sql view model DEV_pipeline_b.stg_market_prices ............. [[32mSUCCESS 1[0m in 0.33s]
[0m21:52:45.056845 [info ] [Thread-1  ]: 5 of 30 OK created sql view model DEV_pipeline_c.stg_fund_hierarchy ............ [[32mSUCCESS 1[0m in 0.33s]
[0m21:52:45.057553 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.stg_market_prices
[0m21:52:45.058160 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.stg_fund_hierarchy
[0m21:52:45.058651 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.stg_positions_daily
[0m21:52:45.059167 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.stg_securities
[0m21:52:45.059695 [info ] [Thread-4  ]: 9 of 30 START sql view model DEV_pipeline_c.stg_positions_daily ................ [RUN]
[0m21:52:45.060168 [info ] [Thread-1  ]: 10 of 30 START sql view model DEV_pipeline_b.stg_securities .................... [RUN]
[0m21:52:45.060641 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_market_prices, now model.bain_capital_portfolio_analytics.stg_positions_daily)
[0m21:52:45.061030 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_fund_hierarchy, now model.bain_capital_portfolio_analytics.stg_securities)
[0m21:52:45.061400 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_positions_daily
[0m21:52:45.061742 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_securities
[0m21:52:45.066114 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_positions_daily"
[0m21:52:45.069239 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_securities"
[0m21:52:45.070211 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.312 seconds
[0m21:52:45.072094 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11523a550>]}
[0m21:52:45.072702 [info ] [Thread-2  ]: 7 of 30 OK created sql view model DEV_pipeline_c.stg_portfolio_benchmarks ...... [[32mSUCCESS 1[0m in 0.35s]
[0m21:52:45.073198 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks
[0m21:52:45.073532 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.stg_positions_daily
[0m21:52:45.073873 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.stg_trades
[0m21:52:45.074187 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.stg_securities
[0m21:52:45.077334 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_positions_daily"
[0m21:52:45.077761 [info ] [Thread-2  ]: 11 of 30 START sql view model DEV_pipeline_b.stg_trades ........................ [RUN]
[0m21:52:45.080606 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_securities"
[0m21:52:45.081102 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_portfolio_benchmarks, now model.bain_capital_portfolio_analytics.stg_trades)
[0m21:52:45.081528 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_trades
[0m21:52:45.084822 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_trades"
[0m21:52:45.086487 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_positions_daily"
[0m21:52:45.086951 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.stg_positions_daily: create or replace   view DBT_DEMO.DEV_pipeline_c.stg_positions_daily
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: stg_positions_daily
-- Description: Daily position snapshots from source system
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Heavy transformations before filtering
-- 2. Unnecessary type conversions
-- 3. Could push filters upstream

with source as (
    select
        position_id,
        portfolio_id,
        security_id,
        position_date,
        quantity,
        cost_basis_price,
        cost_basis_value,
        market_price,
        market_value,
        market_value_usd,
        unrealized_pnl,
        unrealized_pnl_pct,
        weight_pct,
        created_at,
        updated_at
    from DBT_DEMO.DEV.positions_daily
),

-- ISSUE: Transformations applied to all rows before filter
transformed as (
    select
        position_id,
        portfolio_id,
        security_id,
        cast(position_date as date) as position_date,
        cast(quantity as decimal(18,6)) as quantity,
        cast(cost_basis_price as decimal(18,4)) as cost_basis_price,
        cast(cost_basis_value as decimal(18,2)) as cost_basis_value,
        cast(market_price as decimal(18,4)) as market_price,
        cast(market_value as decimal(18,2)) as market_value,
        cast(market_value_usd as decimal(18,2)) as market_value_usd,
        cast(unrealized_pnl as decimal(18,2)) as unrealized_pnl,
        cast(unrealized_pnl_pct as decimal(10,4)) as unrealized_pnl_pct,
        cast(weight_pct as decimal(10,6)) as weight_pct,
        created_at,
        updated_at
    from source
),

-- ISSUE: Filter applied last
filtered as (
    select *
    from transformed
    where position_date >= '2020-01-01'
)

select * from filtered
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_positions_daily"} */;
[0m21:52:45.088527 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_securities"
[0m21:52:45.089225 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.stg_securities: create or replace   view DBT_DEMO.DEV_pipeline_b.stg_securities
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: stg_securities
-- Description: Staging model for security master data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Nested subqueries instead of QUALIFY
-- 2. Multiple deduplication passes

with source as (
    select
        security_id,
        ticker,
        security_name,
        security_type,
        asset_class,
        sector,
        industry,
        currency,
        exchange,
        is_active,
        created_at,
        updated_at
    from DBT_DEMO.DEV.securities
),

-- ISSUE: Complex deduplication using subquery
deduplicated as (
    select *
    from (
        select
            *,
            row_number() over (
                partition by security_id
                order by updated_at desc
            ) as rn
        from source
    ) sub
    where rn = 1  -- ISSUE: Should use QUALIFY
),

-- ISSUE: Separate CTE for type standardization
standardized as (
    select
        security_id,
        upper(trim(ticker)) as ticker,
        trim(security_name) as security_name,
        -- ISSUE: Repeated CASE logic found in other models
        case
            when security_type in ('STOCK', 'EQUITY', 'COMMON') then 'EQUITY'
            when security_type in ('BOND', 'NOTE', 'DEBENTURE') then 'FIXED_INCOME'
            when security_type in ('OPTION', 'FUTURE', 'SWAP') then 'DERIVATIVE'
            when security_type in ('ETF', 'MUTUAL_FUND') then 'FUND'
            else 'OTHER'
        end as security_type_standardized,
        security_type as security_type_original,
        upper(asset_class) as asset_class,
        sector,
        industry,
        upper(currency) as currency,
        exchange,
        is_active,
        created_at,
        updated_at
    from deduplicated
)

select * from standardized
where is_active = true
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_securities"} */;
[0m21:52:45.089642 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.stg_trades
[0m21:52:45.093676 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_trades"
[0m21:52:45.096791 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_trades"
[0m21:52:45.097525 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.stg_trades: create or replace   view DBT_DEMO.DEV_pipeline_b.stg_trades
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: stg_trades
-- Description: Staging model for trade transactions
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Complex CASE statements that repeat
-- 2. Multiple CTEs doing similar transformations
-- 3. Unnecessary string operations

with source as (
    select
        trade_id,
        portfolio_id,
        security_id,
        broker_id,
        trade_date,
        settlement_date,
        trade_type,
        quantity,
        price,
        gross_amount,
        commission,
        fees,
        net_amount,
        currency,
        created_at,
        updated_at
    from DBT_DEMO.DEV.trades
),

-- ISSUE: Repeated CASE statements for trade categorization
categorized as (
    select
        *,
        -- ISSUE: This logic is repeated in multiple models
        case
            when trade_type in ('BUY', 'COVER') then 'PURCHASE'
            when trade_type in ('SELL', 'SHORT') then 'SALE'
            when trade_type in ('DIVIDEND', 'INTEREST') then 'INCOME'
            else 'OTHER'
        end as trade_category,
        case
            when abs(net_amount) >= 10000000 then 'LARGE'
            when abs(net_amount) >= 1000000 then 'MEDIUM'
            when abs(net_amount) >= 100000 then 'SMALL'
            else 'MICRO'
        end as trade_size_bucket,
        -- ISSUE: Redundant string manipulation
        upper(trim(trade_type)) as trade_type_clean,
        upper(trim(currency)) as currency_clean
    from source
),

-- ISSUE: Another pass just for date calculations
with_dates as (
    select
        *,
        datediff('day', trade_date, settlement_date) as settlement_days,
        date_trunc('month', trade_date) as trade_month,
        date_trunc('quarter', trade_date) as trade_quarter,
        extract(year from trade_date) as trade_year,
        extract(month from trade_date) as trade_month_num,
        dayofweek(trade_date) as trade_day_of_week
    from categorized
),

-- ISSUE: Late filtering
filtered as (
    select *
    from with_dates
    where trade_date >= '2020-01-01'
)

select * from filtered
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_trades"} */;
[0m21:52:45.123321 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 0.358 seconds
[0m21:52:45.125195 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115129e80>]}
[0m21:52:45.125772 [info ] [Thread-3  ]: 8 of 30 OK created sql view model DEV_pipeline_a.stg_portfolios ................ [[32mSUCCESS 1[0m in 0.39s]
[0m21:52:45.126248 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.stg_portfolios
[0m21:52:45.126589 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.stg_valuations
[0m21:52:45.127049 [info ] [Thread-3  ]: 12 of 30 START sql view model DEV_pipeline_c.stg_valuations .................... [RUN]
[0m21:52:45.127505 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_portfolios, now model.bain_capital_portfolio_analytics.stg_valuations)
[0m21:52:45.127827 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.stg_valuations
[0m21:52:45.131228 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.stg_valuations"
[0m21:52:45.131951 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.stg_valuations
[0m21:52:45.134959 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.stg_valuations"
[0m21:52:45.136817 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.stg_valuations"
[0m21:52:45.137186 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.stg_valuations: create or replace   view DBT_DEMO.DEV_pipeline_c.stg_valuations
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: stg_valuations
-- Description: Portfolio valuation data (NAV, etc.)
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Deduplication via subquery
-- 2. Heavy calculations before filtering

with source as (
    select
        valuation_id,
        portfolio_id,
        valuation_date,
        nav,
        nav_per_share,
        shares_outstanding,
        gross_assets,
        total_liabilities,
        net_assets,
        currency,
        fx_rate_to_usd,
        nav_usd,
        created_at,
        updated_at
    from DBT_DEMO.DEV.valuations
),

-- ISSUE: Deduplication using subquery
deduplicated as (
    select *
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id, valuation_date
                order by updated_at desc
            ) as rn
        from source
    )
    where rn = 1
),

-- ISSUE: Filter applied after deduplication
filtered as (
    select
        valuation_id,
        portfolio_id,
        cast(valuation_date as date) as valuation_date,
        cast(nav as decimal(18,2)) as nav,
        cast(nav_per_share as decimal(18,6)) as nav_per_share,
        cast(shares_outstanding as decimal(18,6)) as shares_outstanding,
        cast(gross_assets as decimal(18,2)) as gross_assets,
        cast(total_liabilities as decimal(18,2)) as total_liabilities,
        cast(net_assets as decimal(18,2)) as net_assets,
        upper(currency) as currency,
        cast(fx_rate_to_usd as decimal(18,8)) as fx_rate_to_usd,
        cast(nav_usd as decimal(18,2)) as nav_usd,
        created_at,
        updated_at
    from deduplicated
    where valuation_date >= '2020-01-01'
)

select * from filtered
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.stg_valuations"} */;
[0m21:52:45.423296 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 0.285 seconds
[0m21:52:45.427593 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fc599a0>]}
[0m21:52:45.428745 [info ] [Thread-3  ]: 12 of 30 OK created sql view model DEV_pipeline_c.stg_valuations ............... [[32mSUCCESS 1[0m in 0.30s]
[0m21:52:45.429502 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.stg_valuations
[0m21:52:45.430073 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m21:52:45.431283 [info ] [Thread-3  ]: 13 of 30 START sql table model DEV_pipeline_a.fact_cashflow_summary ............ [RUN]
[0m21:52:45.432179 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_valuations, now model.bain_capital_portfolio_analytics.fact_cashflow_summary)
[0m21:52:45.433298 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m21:52:45.452679 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_cashflow_summary"
[0m21:52:45.454007 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.356 seconds
[0m21:52:45.456046 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11528f5b0>]}
[0m21:52:45.456707 [info ] [Thread-2  ]: 11 of 30 OK created sql view model DEV_pipeline_b.stg_trades ................... [[32mSUCCESS 1[0m in 0.37s]
[0m21:52:45.457430 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 0.369 seconds
[0m21:52:45.457906 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.stg_trades
[0m21:52:45.459541 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fc0fee0>]}
[0m21:52:45.460151 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.370 seconds
[0m21:52:45.460531 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.int_benchmark_aligned
[0m21:52:45.460868 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m21:52:45.461450 [info ] [Thread-4  ]: 9 of 30 OK created sql view model DEV_pipeline_c.stg_positions_daily ........... [[32mSUCCESS 1[0m in 0.40s]
[0m21:52:45.463001 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11520aeb0>]}
[0m21:52:45.463444 [info ] [Thread-2  ]: 14 of 30 START sql view model DEV_pipeline_c.int_benchmark_aligned ............. [RUN]
[0m21:52:45.470288 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.stg_positions_daily
[0m21:52:45.488000 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_cashflow_summary"
[0m21:52:45.488587 [info ] [Thread-1  ]: 10 of 30 OK created sql view model DEV_pipeline_b.stg_securities ............... [[32mSUCCESS 1[0m in 0.40s]
[0m21:52:45.489005 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_trades, now model.bain_capital_portfolio_analytics.int_benchmark_aligned)
[0m21:52:45.489380 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.int_portfolio_returns_daily
[0m21:52:45.489966 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.stg_securities
[0m21:52:45.490298 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.int_benchmark_aligned
[0m21:52:45.490701 [info ] [Thread-4  ]: 15 of 30 START sql view model DEV_pipeline_c.int_portfolio_returns_daily ....... [RUN]
[0m21:52:45.494287 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_benchmark_aligned"
[0m21:52:45.494615 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.int_position_attribution
[0m21:52:45.494969 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_positions_daily, now model.bain_capital_portfolio_analytics.int_portfolio_returns_daily)
[0m21:52:45.502742 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_cashflow_summary"
[0m21:52:45.503149 [info ] [Thread-1  ]: 16 of 30 START sql view model DEV_pipeline_c.int_position_attribution .......... [RUN]
[0m21:52:45.503527 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.int_portfolio_returns_daily
[0m21:52:45.504215 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.fact_cashflow_summary: create or replace transient table DBT_DEMO.DEV_pipeline_a.fact_cashflow_summary
    
    
    
    as (-- Pipeline A: Simple Cashflow Pipeline
-- Model: fact_cashflow_summary
-- Description: Fact table summarizing cashflows by portfolio and month
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-joins for prior period comparisons (should use LAG)
-- 2. Late aggregation (aggregates after full join)
-- 3. Repeated window functions with same partitions
-- 4. Redundant date calculations per row
-- 5. Correlated subqueries for fund-level totals

with cashflows as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_cashflows
),

portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

-- ISSUE: Full join before aggregation (scans all rows)
joined as (
    select
        c.cashflow_id,
        c.portfolio_id,
        p.portfolio_name,
        p.portfolio_type,
        p.fund_id,
        c.cashflow_type,
        c.cashflow_date,
        c.amount,
        c.currency,
        -- ISSUE: Redundant date calculations done per row
        date_trunc('month', c.cashflow_date) as cashflow_month,
        date_trunc('quarter', c.cashflow_date) as cashflow_quarter,
        date_trunc('year', c.cashflow_date) as cashflow_year,
        extract(year from c.cashflow_date) as year_num,
        extract(month from c.cashflow_date) as month_num,
        extract(quarter from c.cashflow_date) as quarter_num,
        extract(dayofmonth from c.cashflow_date) as day_num
    from cashflows c
    inner join portfolios p
        on c.portfolio_id = p.portfolio_id
),

-- ISSUE: Aggregation happens after full row-level join
aggregated as (
    select
        portfolio_id,
        portfolio_name,
        portfolio_type,
        fund_id,
        cashflow_month,
        cashflow_quarter,
        cashflow_year,
        year_num,
        month_num,
        quarter_num,
        cashflow_type,
        currency,
        count(*) as transaction_count,
        count(distinct cashflow_id) as unique_transactions,
        sum(amount) as total_amount,
        avg(amount) as avg_amount,
        min(amount) as min_amount,
        max(amount) as max_amount,
        stddev(amount) as stddev_amount,
        -- ISSUE: Percentile calculations (slow)
        percentile_cont(0.25) within group (order by amount) as p25_amount,
        percentile_cont(0.50) within group (order by amount) as median_amount,
        percentile_cont(0.75) within group (order by amount) as p75_amount
    from joined
    group by 1,2,3,4,5,6,7,8,9,10,11,12  -- ISSUE: Non-descriptive GROUP BY
),

-- ISSUE: Self-join for prior month comparisons (should use LAG)
with_prior_months as (
    select
        agg.*,
        -- ISSUE: Self-join for prior month
        agg_m1.total_amount as prior_1m_total,
        agg_m1.transaction_count as prior_1m_count,
        -- ISSUE: Self-join for 3 months ago
        agg_m3.total_amount as prior_3m_total,
        -- ISSUE: Self-join for 6 months ago
        agg_m6.total_amount as prior_6m_total,
        -- ISSUE: Self-join for 12 months ago
        agg_m12.total_amount as prior_12m_total
    from aggregated agg
    left join aggregated agg_m1
        on agg.portfolio_id = agg_m1.portfolio_id
        and agg.cashflow_type = agg_m1.cashflow_type
        and agg.currency = agg_m1.currency
        and agg_m1.cashflow_month = dateadd(month, -1, agg.cashflow_month)
    left join aggregated agg_m3
        on agg.portfolio_id = agg_m3.portfolio_id
        and agg.cashflow_type = agg_m3.cashflow_type
        and agg.currency = agg_m3.currency
        and agg_m3.cashflow_month = dateadd(month, -3, agg.cashflow_month)
    left join aggregated agg_m6
        on agg.portfolio_id = agg_m6.portfolio_id
        and agg.cashflow_type = agg_m6.cashflow_type
        and agg.currency = agg_m6.currency
        and agg_m6.cashflow_month = dateadd(month, -6, agg.cashflow_month)
    left join aggregated agg_m12
        on agg.portfolio_id = agg_m12.portfolio_id
        and agg.cashflow_type = agg_m12.cashflow_type
        and agg.currency = agg_m12.currency
        and agg_m12.cashflow_month = dateadd(month, -12, agg.cashflow_month)
),

-- ISSUE: Multiple window functions with repeated partitions
with_window_calcs as (
    select
        wpm.*,
        -- ISSUE: Running totals (repeated partition)
        sum(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_total,
        sum(transaction_count) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between unbounded preceding and current row
        ) as cumulative_count,
        -- ISSUE: Moving averages (same partition repeated)
        avg(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 2 preceding and current row
        ) as rolling_3m_avg,
        avg(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 5 preceding and current row
        ) as rolling_6m_avg,
        avg(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_avg,
        -- ISSUE: More window calculations
        stddev(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_stddev,
        min(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_min,
        max(total_amount) over (
            partition by wpm.portfolio_id, wpm.cashflow_type, wpm.currency
            order by wpm.cashflow_month
            rows between 11 preceding and current row
        ) as rolling_12m_max
    from with_prior_months wpm
),

-- ISSUE: Correlated subqueries for fund-level context (very slow)
with_fund_context as (
    select
        wwc.*,
        -- ISSUE: Correlated subquery for fund total
        (
            select sum(total_amount)
            from aggregated agg2
            inner join portfolios p2
                on agg2.portfolio_id = p2.portfolio_id
            where p2.fund_id = wwc.fund_id
            and agg2.cashflow_month = wwc.cashflow_month
            and agg2.cashflow_type = wwc.cashflow_type
        ) as fund_total_amount,
        -- ISSUE: Another correlated subquery for portfolio count
        (
            select count(distinct agg2.portfolio_id)
            from aggregated agg2
            inner join portfolios p2
                on agg2.portfolio_id = p2.portfolio_id
            where p2.fund_id = wwc.fund_id
            and agg2.cashflow_month = wwc.cashflow_month
            and agg2.cashflow_type = wwc.cashflow_type
        ) as fund_portfolio_count
    from with_window_calcs wwc
),

-- ISSUE: Complex derived metrics with repeated CASE statements
final as (
    select
        md5(cast(coalesce(cast(wfc.portfolio_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wfc.cashflow_month as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wfc.cashflow_type as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wfc.currency as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as cashflow_summary_key,
        wfc.*,
        -- ISSUE: Portfolio share of fund (repeated division)
        case
            when wfc.fund_total_amount > 0
            then (wfc.total_amount / wfc.fund_total_amount) * 100
            else null
        end as portfolio_share_of_fund_pct,
        -- ISSUE: Month-over-month growth calculations
        case
            when wfc.prior_1m_total is not null and wfc.prior_1m_total != 0
            then ((wfc.total_amount - wfc.prior_1m_total) / abs(wfc.prior_1m_total)) * 100
            else null
        end as mom_growth_pct,
        case
            when wfc.prior_3m_total is not null and wfc.prior_3m_total != 0
            then ((wfc.total_amount - wfc.prior_3m_total) / abs(wfc.prior_3m_total)) * 100
            else null
        end as growth_3m_pct,
        case
            when wfc.prior_12m_total is not null and wfc.prior_12m_total != 0
            then ((wfc.total_amount - wfc.prior_12m_total) / abs(wfc.prior_12m_total)) * 100
            else null
        end as yoy_growth_pct,
        -- ISSUE: Trend classification (complex nested CASE)
        case
            when wfc.rolling_3m_avg > wfc.rolling_12m_avg * 1.3 then 'ACCELERATING'
            when wfc.rolling_3m_avg > wfc.rolling_12m_avg * 1.1 then 'GROWING'
            when wfc.rolling_3m_avg < wfc.rolling_12m_avg * 0.7 then 'DECLINING_FAST'
            when wfc.rolling_3m_avg < wfc.rolling_12m_avg * 0.9 then 'DECLINING'
            else 'STABLE'
        end as trend_classification,
        -- ISSUE: Volatility classification
        case
            when wfc.rolling_12m_stddev < wfc.rolling_12m_avg * 0.1 then 'LOW_VOLATILITY'
            when wfc.rolling_12m_stddev < wfc.rolling_12m_avg * 0.3 then 'MODERATE_VOLATILITY'
            when wfc.rolling_12m_stddev < wfc.rolling_12m_avg * 0.5 then 'HIGH_VOLATILITY'
            else 'VERY_HIGH_VOLATILITY'
        end as volatility_classification,
        -- ISSUE: Size classification (repeated CASE)
        case
            when abs(wfc.total_amount) >= 10000000 then 'MEGA'
            when abs(wfc.total_amount) >= 5000000 then 'LARGE'
            when abs(wfc.total_amount) >= 1000000 then 'MEDIUM'
            when abs(wfc.total_amount) >= 100000 then 'SMALL'
            else 'MICRO'
        end as transaction_size_category
    from with_fund_context wfc
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_cashflow_summary"} */;
[0m21:52:45.504875 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.int_benchmark_aligned
[0m21:52:45.505187 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.stg_securities, now model.bain_capital_portfolio_analytics.int_position_attribution)
[0m21:52:45.508482 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_portfolio_returns_daily"
[0m21:52:45.511686 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_benchmark_aligned"
[0m21:52:45.512038 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.int_position_attribution
[0m21:52:45.516506 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_position_attribution"
[0m21:52:45.517650 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.int_portfolio_returns_daily
[0m21:52:45.520567 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_portfolio_returns_daily"
[0m21:52:45.522238 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_benchmark_aligned"
[0m21:52:45.522812 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.int_benchmark_aligned: create or replace   view DBT_DEMO.DEV_pipeline_c.int_benchmark_aligned
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: int_benchmark_aligned
-- Description: Align benchmark returns with portfolio dates for comparison
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Heavy join between portfolios and benchmarks
-- 2. Could pre-filter benchmarks

with portfolio_dates as (
    select distinct
        portfolio_id,
        valuation_date
    from DBT_DEMO.DEV_pipeline_c.stg_valuations
),

portfolio_benchmarks as (
    select * from DBT_DEMO.DEV_pipeline_c.stg_portfolio_benchmarks
),

benchmark_returns as (
    select * from DBT_DEMO.DEV_pipeline_c.stg_benchmark_returns
),

-- ISSUE: Cross-join like pattern (portfolio dates x benchmarks)
aligned as (
    select
        pd.portfolio_id,
        pd.valuation_date,
        pb.benchmark_id,
        1.0 as benchmark_weight,  -- Default weight since not in source
        pb.is_primary,
        br.daily_return as benchmark_daily_return,
        br.cumulative_return as benchmark_cumulative_return,
        br.return_30d as benchmark_return_30d,
        br.return_90d as benchmark_return_90d,
        br.return_1y as benchmark_return_1y,
        br.annualized_volatility as benchmark_volatility
    from portfolio_dates pd
    inner join portfolio_benchmarks pb
        on pd.portfolio_id = pb.portfolio_id
        and pd.valuation_date >= pb.start_date
        and (pb.end_date is null or pd.valuation_date <= pb.end_date)
    left join benchmark_returns br
        on pb.benchmark_id = br.benchmark_id
        and pd.valuation_date = br.return_date
)

select * from aligned
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_benchmark_aligned"} */;
[0m21:52:45.523409 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.int_position_attribution
[0m21:52:45.526282 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_position_attribution"
[0m21:52:45.530208 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_portfolio_returns_daily"
[0m21:52:45.530679 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.int_portfolio_returns_daily: create or replace   view DBT_DEMO.DEV_pipeline_c.int_portfolio_returns_daily
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: int_portfolio_returns_daily
-- Description: Calculate daily portfolio returns from NAV
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-join for prior day NAV (should use LAG)
-- 2. Multiple passes for return calculations
-- 3. Complex window functions

with valuations as (
    select * from DBT_DEMO.DEV_pipeline_c.stg_valuations
),

cashflows as (
    select
        portfolio_id,
        cashflow_date,
        sum(case when cashflow_type = 'CONTRIBUTION' then amount else 0 end) as contributions,
        sum(case when cashflow_type = 'DISTRIBUTION' then amount else 0 end) as distributions
    from DBT_DEMO.DEV_pipeline_a.stg_cashflows
    group by 1, 2
),

-- ISSUE: Self-join instead of LAG for prior NAV
with_prior_nav as (
    select
        curr.portfolio_id,
        curr.valuation_date,
        curr.nav,
        curr.nav_usd,
        prev.nav as prior_nav,
        prev.nav_usd as prior_nav_usd,
        coalesce(cf.contributions, 0) as contributions,
        coalesce(cf.distributions, 0) as distributions
    from valuations curr
    left join valuations prev
        on curr.portfolio_id = prev.portfolio_id
        and curr.valuation_date = dateadd('day', 1, prev.valuation_date)
    left join cashflows cf
        on curr.portfolio_id = cf.portfolio_id
        and curr.valuation_date = cf.cashflow_date
),

-- ISSUE: Modified Dietz calculation done inefficiently
with_daily_return as (
    select
        portfolio_id,
        valuation_date,
        nav,
        nav_usd,
        prior_nav,
        prior_nav_usd,
        contributions,
        distributions,
        -- Simple return
        case
            when prior_nav > 0
            then (nav - prior_nav - contributions + distributions) / prior_nav
            else null
        end as daily_return_simple,
        -- Modified Dietz (approximation)
        case
            when (prior_nav + contributions * 0.5) > 0
            then (nav - prior_nav - contributions + distributions) / (prior_nav + contributions * 0.5 - distributions * 0.5)
            else null
        end as daily_return_mod_dietz
    from with_prior_nav
),

-- ISSUE: Multiple window functions for different periods
with_rolling_returns as (
    select
        *,
        -- Cumulative return using log returns
        exp(sum(ln(1 + coalesce(daily_return_mod_dietz, 0))) over (
            partition by portfolio_id
            order by valuation_date
            rows between unbounded preceding and current row
        )) - 1 as cumulative_return,
        -- Rolling period returns
        exp(sum(ln(1 + coalesce(daily_return_mod_dietz, 0))) over (
            partition by portfolio_id
            order by valuation_date
            rows between 6 preceding and current row
        )) - 1 as return_1w,
        exp(sum(ln(1 + coalesce(daily_return_mod_dietz, 0))) over (
            partition by portfolio_id
            order by valuation_date
            rows between 29 preceding and current row
        )) - 1 as return_1m,
        exp(sum(ln(1 + coalesce(daily_return_mod_dietz, 0))) over (
            partition by portfolio_id
            order by valuation_date
            rows between 89 preceding and current row
        )) - 1 as return_3m,
        exp(sum(ln(1 + coalesce(daily_return_mod_dietz, 0))) over (
            partition by portfolio_id
            order by valuation_date
            rows between 179 preceding and current row
        )) - 1 as return_6m,
        exp(sum(ln(1 + coalesce(daily_return_mod_dietz, 0))) over (
            partition by portfolio_id
            order by valuation_date
            rows between 364 preceding and current row
        )) - 1 as return_1y,
        -- Rolling volatility
        stddev(daily_return_mod_dietz) over (
            partition by portfolio_id
            order by valuation_date
            rows between 29 preceding and current row
        ) * sqrt(252) as volatility_1m,
        stddev(daily_return_mod_dietz) over (
            partition by portfolio_id
            order by valuation_date
            rows between 251 preceding and current row
        ) * sqrt(252) as volatility_1y
    from with_daily_return
)

select * from with_rolling_returns
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_portfolio_returns_daily"} */;
[0m21:52:45.533609 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_position_attribution"
[0m21:52:45.534384 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.int_position_attribution: create or replace   view DBT_DEMO.DEV_pipeline_c.int_position_attribution
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: int_position_attribution
-- Description: Attribution analysis at position level
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Heavy multi-way join
-- 2. Complex attribution calculations
-- 3. Multiple window functions

with positions as (
    select * from DBT_DEMO.DEV_pipeline_c.stg_positions_daily
),

securities as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_securities
),

market_prices as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_market_prices
),

-- ISSUE: Heavy 3-way join
enriched_positions as (
    select
        p.portfolio_id,
        p.security_id,
        p.position_date,
        p.quantity,
        p.market_value_usd,
        p.weight_pct,
        s.ticker,
        s.security_type_standardized as security_type,
        s.asset_class,
        s.sector,
        s.industry,
        mp.daily_return as security_return,
        mp.close_price,
        mp.ma_20,
        mp.ma_50,
        mp.volatility_20d
    from positions p
    inner join securities s
        on p.security_id = s.security_id
    left join market_prices mp
        on p.security_id = mp.security_id
        and p.position_date = mp.price_date
),

-- ISSUE: Window functions for prior day weight
with_prior_weight as (
    select
        *,
        lag(weight_pct, 1) over (
            partition by portfolio_id, security_id
            order by position_date
        ) as prior_weight_pct,
        lag(market_value_usd, 1) over (
            partition by portfolio_id, security_id
            order by position_date
        ) as prior_market_value
    from enriched_positions
),

-- ISSUE: Attribution calculations
with_attribution as (
    select
        *,
        -- Contribution to return
        coalesce(prior_weight_pct, weight_pct) * coalesce(security_return, 0) as contribution_to_return,
        -- Allocation effect (simplified Brinson)
        (weight_pct - coalesce(prior_weight_pct, weight_pct)) * coalesce(security_return, 0) as allocation_effect,
        -- Position P&L
        market_value_usd - coalesce(prior_market_value, market_value_usd) as position_pnl
    from with_prior_weight
)

select * from with_attribution
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_position_attribution"} */;
[0m21:52:45.947316 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.424 seconds
[0m21:52:45.951472 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1153666a0>]}
[0m21:52:45.952749 [info ] [Thread-2  ]: 14 of 30 OK created sql view model DEV_pipeline_c.int_benchmark_aligned ........ [[32mSUCCESS 1[0m in 0.46s]
[0m21:52:45.953580 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.int_benchmark_aligned
[0m21:52:45.954115 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.int_trades_enriched
[0m21:52:45.954858 [info ] [Thread-2  ]: 17 of 30 START sql view model DEV_pipeline_b.int_trades_enriched ............... [RUN]
[0m21:52:45.955556 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_benchmark_aligned, now model.bain_capital_portfolio_analytics.int_trades_enriched)
[0m21:52:45.956214 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.int_trades_enriched
[0m21:52:45.964701 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_trades_enriched"
[0m21:52:45.966132 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.int_trades_enriched
[0m21:52:45.972084 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_trades_enriched"
[0m21:52:45.975417 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_trades_enriched"
[0m21:52:45.975940 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.int_trades_enriched: create or replace   view DBT_DEMO.DEV_pipeline_b.int_trades_enriched
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: int_trades_enriched
-- Description: Intermediate model enriching trades with security and price data
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Multiple heavy joins done row-by-row
-- 2. Price lookup repeated for every trade
-- 3. Could pre-aggregate before joining

with trades as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_trades
),

securities as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_securities
),

market_prices as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_market_prices
),

brokers as (
    select * from DBT_DEMO.DEV_pipeline_b.stg_brokers
),

-- ISSUE: Heavy multi-way join before any aggregation
enriched as (
    select
        t.trade_id,
        t.portfolio_id,
        t.security_id,
        t.trade_date,
        t.settlement_date,
        t.trade_type,
        t.trade_category,
        t.trade_size_bucket,
        t.quantity,
        t.price as execution_price,
        t.gross_amount,
        t.commission,
        t.fees,
        t.net_amount,
        t.currency,
        t.settlement_days,
        t.trade_month,
        t.trade_quarter,
        t.trade_year,
        -- Security attributes
        s.ticker,
        s.security_name,
        s.security_type_standardized as security_type,
        s.asset_class,
        s.sector,
        s.industry,
        -- Broker attributes
        b.broker_name,
        b.broker_type,
        b.region as broker_region,
        b.commission_rate as standard_commission_rate,
        -- Market price on trade date
        mp.close_price as market_close_price,
        mp.ma_20,
        mp.ma_50,
        mp.volatility_20d,
        mp.trend_signal,
        mp.volume_signal,
        -- ISSUE: These calculations done per row
        case
            when mp.close_price > 0
            then (t.price - mp.close_price) / mp.close_price * 100
            else null
        end as execution_vs_close_pct,
        case
            when t.price > mp.close_price then 'ABOVE_MARKET'
            when t.price < mp.close_price then 'BELOW_MARKET'
            else 'AT_MARKET'
        end as execution_quality,
        -- Cost analysis
        t.commission + t.fees as total_costs,
        case
            when t.gross_amount > 0
            then (t.commission + t.fees) / t.gross_amount * 10000
            else null
        end as cost_bps
    from trades t
    inner join securities s
        on t.security_id = s.security_id
    left join brokers b
        on t.broker_id = b.broker_id
    left join market_prices mp
        on t.security_id = mp.security_id
        and t.trade_date = mp.price_date
)

select * from enriched
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_trades_enriched"} */;
[0m21:52:46.040725 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 0.509 seconds
[0m21:52:46.044477 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fc14160>]}
[0m21:52:46.045531 [info ] [Thread-4  ]: 15 of 30 OK created sql view model DEV_pipeline_c.int_portfolio_returns_daily .. [[32mSUCCESS 1[0m in 0.55s]
[0m21:52:46.046155 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.int_portfolio_returns_daily
[0m21:52:46.046872 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark
[0m21:52:46.047634 [info ] [Thread-4  ]: 18 of 30 START sql view model DEV_pipeline_c.int_portfolio_vs_benchmark ........ [RUN]
[0m21:52:46.048309 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_portfolio_returns_daily, now model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark)
[0m21:52:46.048780 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark
[0m21:52:46.109827 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark"
[0m21:52:46.110738 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark
[0m21:52:46.113741 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark"
[0m21:52:46.116590 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark"
[0m21:52:46.117024 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark: create or replace   view DBT_DEMO.DEV_pipeline_c.int_portfolio_vs_benchmark
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: int_portfolio_vs_benchmark
-- Description: Compare portfolio returns to benchmark
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Re-joins data that could be joined once upstream
-- 2. Excess return calculation repeated
-- 3. Complex rolling calculations

with portfolio_returns as (
    select * from DBT_DEMO.DEV_pipeline_c.int_portfolio_returns_daily
),

benchmark_aligned as (
    select * from DBT_DEMO.DEV_pipeline_c.int_benchmark_aligned
    where is_primary = true
),

-- ISSUE: Another join that combines already-processed data
combined as (
    select
        pr.portfolio_id,
        pr.valuation_date,
        pr.nav,
        pr.nav_usd,
        pr.daily_return_mod_dietz as portfolio_daily_return,
        pr.cumulative_return as portfolio_cumulative_return,
        pr.return_1m as portfolio_return_1m,
        pr.return_3m as portfolio_return_3m,
        pr.return_1y as portfolio_return_1y,
        pr.volatility_1y as portfolio_volatility,
        ba.benchmark_id,
        ba.benchmark_daily_return,
        ba.benchmark_cumulative_return,
        ba.benchmark_return_30d as benchmark_return_1m,
        ba.benchmark_return_90d as benchmark_return_3m,
        ba.benchmark_return_1y,
        ba.benchmark_volatility
    from portfolio_returns pr
    left join benchmark_aligned ba
        on pr.portfolio_id = ba.portfolio_id
        and pr.valuation_date = ba.valuation_date
),

-- ISSUE: Excess return calculations
with_excess as (
    select
        *,
        portfolio_daily_return - coalesce(benchmark_daily_return, 0) as daily_excess_return,
        portfolio_cumulative_return - coalesce(benchmark_cumulative_return, 0) as cumulative_excess_return,
        portfolio_return_1m - coalesce(benchmark_return_1m, 0) as excess_return_1m,
        portfolio_return_3m - coalesce(benchmark_return_3m, 0) as excess_return_3m,
        portfolio_return_1y - coalesce(benchmark_return_1y, 0) as excess_return_1y
    from combined
),

-- ISSUE: Rolling tracking error calculation
with_tracking_error as (
    select
        *,
        stddev(daily_excess_return) over (
            partition by portfolio_id
            order by valuation_date
            rows between 251 preceding and current row
        ) * sqrt(252) as tracking_error_1y,
        avg(daily_excess_return) over (
            partition by portfolio_id
            order by valuation_date
            rows between 251 preceding and current row
        ) * 252 as annualized_alpha
    from with_excess
),

-- ISSUE: Information ratio
final as (
    select
        *,
        case
            when tracking_error_1y > 0
            then annualized_alpha / tracking_error_1y
            else null
        end as information_ratio
    from with_tracking_error
)

select * from final
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark"} */;
[0m21:52:46.141988 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.607 seconds
[0m21:52:46.143837 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115221370>]}
[0m21:52:46.144394 [info ] [Thread-1  ]: 16 of 30 OK created sql view model DEV_pipeline_c.int_position_attribution ..... [[32mSUCCESS 1[0m in 0.64s]
[0m21:52:46.144860 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.int_position_attribution
[0m21:52:46.145205 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.int_risk_metrics
[0m21:52:46.145682 [info ] [Thread-1  ]: 19 of 30 START sql view model DEV_pipeline_c.int_risk_metrics .................. [RUN]
[0m21:52:46.146163 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_position_attribution, now model.bain_capital_portfolio_analytics.int_risk_metrics)
[0m21:52:46.146528 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.int_risk_metrics
[0m21:52:46.149698 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_risk_metrics"
[0m21:52:46.150307 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.int_risk_metrics
[0m21:52:46.153059 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_risk_metrics"
[0m21:52:46.155402 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_risk_metrics"
[0m21:52:46.155821 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.int_risk_metrics: create or replace   view DBT_DEMO.DEV_pipeline_c.int_risk_metrics
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: int_risk_metrics
-- Description: Calculate risk metrics for portfolios
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Multiple window functions with same partition
-- 2. VaR calculation inefficiencies
-- 3. Could pre-compute some metrics

with portfolio_returns as (
    select * from DBT_DEMO.DEV_pipeline_c.int_portfolio_returns_daily
),

-- ISSUE: Multiple passes for different risk calculations
with_risk_metrics as (
    select
        portfolio_id,
        valuation_date,
        daily_return_mod_dietz as daily_return,
        nav_usd,
        -- ISSUE: Repeated window frame definitions
        -- Max drawdown components
        max(nav_usd) over (
            partition by portfolio_id
            order by valuation_date
            rows between unbounded preceding and current row
        ) as running_max_nav,
        -- Downside deviation
        sqrt(avg(
            case when daily_return_mod_dietz < 0 then power(daily_return_mod_dietz, 2) else 0 end
        ) over (
            partition by portfolio_id
            order by valuation_date
            rows between 251 preceding and current row
        )) * sqrt(252) as downside_deviation_1y,
        -- Sortino components
        avg(daily_return_mod_dietz) over (
            partition by portfolio_id
            order by valuation_date
            rows between 251 preceding and current row
        ) * 252 as annualized_return_1y,
        volatility_1y
    from portfolio_returns
),

-- ISSUE: Another CTE for derived metrics
with_derived as (
    select
        *,
        -- Drawdown
        (nav_usd - running_max_nav) / nullif(running_max_nav, 0) as drawdown,
        -- Sortino ratio (assuming 0% risk-free)
        case
            when downside_deviation_1y > 0
            then annualized_return_1y / downside_deviation_1y
            else null
        end as sortino_ratio,
        -- Sharpe ratio (assuming 0% risk-free)
        case
            when volatility_1y > 0
            then annualized_return_1y / volatility_1y
            else null
        end as sharpe_ratio
    from with_risk_metrics
),

-- ISSUE: Max drawdown calculation
with_max_drawdown as (
    select
        *,
        min(drawdown) over (
            partition by portfolio_id
            order by valuation_date
            rows between unbounded preceding and current row
        ) as max_drawdown
    from with_derived
),

-- ISSUE: VaR calculation (simplified parametric)
final as (
    select
        *,
        -- Parametric VaR (95%)
        nav_usd * volatility_1y / sqrt(252) * 1.645 as var_95_1d,
        -- Parametric VaR (99%)
        nav_usd * volatility_1y / sqrt(252) * 2.326 as var_99_1d
    from with_max_drawdown
)

select * from final
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_risk_metrics"} */;
[0m21:52:46.652993 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 0.496 seconds
[0m21:52:46.658352 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115221370>]}
[0m21:52:46.660487 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.684 seconds
[0m21:52:46.662572 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 0.545 seconds
[0m21:52:46.661664 [info ] [Thread-1  ]: 19 of 30 OK created sql view model DEV_pipeline_c.int_risk_metrics ............. [[32mSUCCESS 1[0m in 0.51s]
[0m21:52:46.665025 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fcc4ca0>]}
[0m21:52:46.667058 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ffb79d0>]}
[0m21:52:46.667688 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.int_risk_metrics
[0m21:52:46.668619 [info ] [Thread-2  ]: 17 of 30 OK created sql view model DEV_pipeline_b.int_trades_enriched .......... [[32mSUCCESS 1[0m in 0.71s]
[0m21:52:46.669419 [info ] [Thread-4  ]: 18 of 30 OK created sql view model DEV_pipeline_c.int_portfolio_vs_benchmark ... [[32mSUCCESS 1[0m in 0.62s]
[0m21:52:46.669984 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.fact_position_snapshot
[0m21:52:46.670757 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.int_trades_enriched
[0m21:52:46.671488 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark
[0m21:52:46.672019 [info ] [Thread-1  ]: 20 of 30 START sql table model DEV_pipeline_c.fact_position_snapshot ........... [RUN]
[0m21:52:46.672444 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.int_sector_attribution
[0m21:52:46.672888 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.int_fund_rollup
[0m21:52:46.673517 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_risk_metrics, now model.bain_capital_portfolio_analytics.fact_position_snapshot)
[0m21:52:46.674131 [info ] [Thread-2  ]: 21 of 30 START sql view model DEV_pipeline_c.int_sector_attribution ............ [RUN]
[0m21:52:46.674681 [info ] [Thread-4  ]: 22 of 30 START sql view model DEV_pipeline_c.int_fund_rollup ................... [RUN]
[0m21:52:46.675092 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_position_snapshot
[0m21:52:46.675506 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_trades_enriched, now model.bain_capital_portfolio_analytics.int_sector_attribution)
[0m21:52:46.675905 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_portfolio_vs_benchmark, now model.bain_capital_portfolio_analytics.int_fund_rollup)
[0m21:52:46.682253 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_position_snapshot"
[0m21:52:46.682638 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.int_sector_attribution
[0m21:52:46.682955 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.int_fund_rollup
[0m21:52:46.685877 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_sector_attribution"
[0m21:52:46.689054 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_fund_rollup"
[0m21:52:46.689710 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.fact_position_snapshot
[0m21:52:46.690055 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.int_sector_attribution
[0m21:52:46.693274 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_position_snapshot"
[0m21:52:46.696042 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_sector_attribution"
[0m21:52:46.696383 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.int_fund_rollup
[0m21:52:46.700096 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_fund_rollup"
[0m21:52:46.701946 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_position_snapshot"
[0m21:52:46.702357 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.fact_position_snapshot: create or replace transient table DBT_DEMO.DEV_pipeline_c.fact_position_snapshot
    
    
    
    as (-- Pipeline C: Complex Portfolio Analytics
-- Model: fact_position_snapshot
-- Description: Position-level fact table with attribution
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Heavy joins that duplicate upstream work
-- 2. Could be more selective in columns

with position_attribution as (
    select * from DBT_DEMO.DEV_pipeline_c.int_position_attribution
),

portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

-- ISSUE: Re-joining portfolio data
final as (
    select
        md5(cast(coalesce(cast(pa.portfolio_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(pa.security_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(pa.position_date as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as position_snapshot_key,
        p.portfolio_name,
        p.portfolio_type,
        p.fund_id,
        pa.portfolio_id,
        pa.security_id,
        pa.position_date,
        pa.ticker,
        pa.security_type,
        pa.asset_class,
        pa.sector,
        pa.industry,
        pa.quantity,
        pa.market_value_usd,
        pa.weight_pct,
        pa.close_price,
        pa.ma_20,
        pa.ma_50,
        pa.volatility_20d,
        pa.security_return,
        pa.contribution_to_return,
        pa.allocation_effect,
        pa.position_pnl,
        -- ISSUE: More date extractions
        extract(year from pa.position_date) as position_year,
        extract(month from pa.position_date) as position_month,
        date_trunc('month', pa.position_date) as position_month_start
    from position_attribution pa
    inner join portfolios p
        on pa.portfolio_id = p.portfolio_id
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_position_snapshot"} */;
[0m21:52:46.704612 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_fund_rollup"
[0m21:52:46.706266 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_sector_attribution"
[0m21:52:46.706640 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.int_fund_rollup: create or replace   view DBT_DEMO.DEV_pipeline_c.int_fund_rollup
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: int_fund_rollup
-- Description: Roll up portfolio metrics to fund level
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Recursive-like hierarchy traversal
-- 2. Multiple aggregation levels
-- 3. Heavy joins

with fund_hierarchy as (
    select * from DBT_DEMO.DEV_pipeline_c.stg_fund_hierarchy
),

portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

risk_metrics as (
    select * from DBT_DEMO.DEV_pipeline_c.int_risk_metrics
),

-- Get portfolio to fund mapping
portfolio_fund_map as (
    select
        p.portfolio_id,
        p.portfolio_name,
        p.fund_id,
        fh.entity_name as fund_name,
        fh.parent_entity_id,
        fh.hierarchy_level
    from portfolios p
    left join fund_hierarchy fh
        on p.fund_id = fh.entity_id
),

-- Get latest risk metrics per portfolio
latest_metrics as (
    select *
    from (
        select
            *,
            row_number() over (partition by portfolio_id order by valuation_date desc) as rn
        from risk_metrics
    )
    where rn = 1  -- ISSUE: Should use QUALIFY
),

-- ISSUE: Join and aggregate
fund_aggregated as (
    select
        pfm.fund_id,
        pfm.fund_name,
        pfm.parent_entity_id,
        pfm.hierarchy_level,
        lm.valuation_date,
        count(distinct pfm.portfolio_id) as portfolio_count,
        sum(lm.nav_usd) as total_nav_usd,
        -- Weighted average metrics
        sum(lm.nav_usd * lm.annualized_return_1y) / nullif(sum(lm.nav_usd), 0) as weighted_return_1y,
        sum(lm.nav_usd * lm.volatility_1y) / nullif(sum(lm.nav_usd), 0) as weighted_volatility_1y,
        sum(lm.nav_usd * lm.sharpe_ratio) / nullif(sum(lm.nav_usd), 0) as weighted_sharpe_ratio,
        min(lm.max_drawdown) as worst_drawdown,
        sum(lm.var_95_1d) as total_var_95
    from portfolio_fund_map pfm
    inner join latest_metrics lm
        on pfm.portfolio_id = lm.portfolio_id
    group by 1, 2, 3, 4, 5
)

select * from fund_aggregated
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_fund_rollup"} */;
[0m21:52:46.707293 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.int_sector_attribution: create or replace   view DBT_DEMO.DEV_pipeline_c.int_sector_attribution
  
  
  
  
  as (
    -- Pipeline C: Complex Portfolio Analytics
-- Model: int_sector_attribution
-- Description: Aggregate attribution to sector level
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Re-aggregation of position data
-- 2. Complex grouping logic
-- 3. Could be combined with position attribution

with position_attribution as (
    select * from DBT_DEMO.DEV_pipeline_c.int_position_attribution
),

-- ISSUE: Aggregation that could be pushed upstream
sector_daily as (
    select
        portfolio_id,
        position_date,
        sector,
        count(distinct security_id) as position_count,
        sum(market_value_usd) as sector_market_value,
        sum(weight_pct) as sector_weight,
        sum(contribution_to_return) as sector_contribution,
        sum(allocation_effect) as sector_allocation_effect,
        sum(position_pnl) as sector_pnl,
        avg(security_return) as avg_security_return
    from position_attribution
    group by 1, 2, 3
),

-- ISSUE: Window functions for rolling metrics
with_rolling as (
    select
        *,
        sum(sector_contribution) over (
            partition by portfolio_id, sector
            order by position_date
            rows between 29 preceding and current row
        ) as sector_contribution_30d,
        avg(sector_weight) over (
            partition by portfolio_id, sector
            order by position_date
            rows between 29 preceding and current row
        ) as avg_sector_weight_30d,
        lag(sector_weight, 1) over (
            partition by portfolio_id, sector
            order by position_date
        ) as prior_sector_weight
    from sector_daily
)

select * from with_rolling
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_sector_attribution"} */;
[0m21:52:47.255186 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 0.547 seconds
[0m21:52:47.259760 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ffb79d0>]}
[0m21:52:47.260928 [info ] [Thread-4  ]: 22 of 30 OK created sql view model DEV_pipeline_c.int_fund_rollup .............. [[32mSUCCESS 1[0m in 0.58s]
[0m21:52:47.261679 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.int_fund_rollup
[0m21:52:47.262212 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.int_trade_pnl
[0m21:52:47.262991 [info ] [Thread-4  ]: 23 of 30 START sql view model DEV_pipeline_b.int_trade_pnl ..................... [RUN]
[0m21:52:47.263793 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_fund_rollup, now model.bain_capital_portfolio_analytics.int_trade_pnl)
[0m21:52:47.264306 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.int_trade_pnl
[0m21:52:47.270672 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.int_trade_pnl"
[0m21:52:47.271738 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.int_trade_pnl
[0m21:52:47.276109 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.int_trade_pnl"
[0m21:52:47.279471 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.int_trade_pnl"
[0m21:52:47.280029 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.int_trade_pnl: create or replace   view DBT_DEMO.DEV_pipeline_b.int_trade_pnl
  
  
  
  
  as (
    -- Pipeline B: Trade Analytics Pipeline
-- Model: int_trade_pnl
-- Description: Calculate P&L for each trade
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Complex position tracking logic that could be simplified
-- 2. Multiple self-joins for cost basis calculation
-- 3. Window functions recalculated multiple times

with trades as (
    select * from DBT_DEMO.DEV_pipeline_b.int_trades_enriched
),

-- ISSUE: Running position calculation done inefficiently
positions as (
    select
        trade_id,
        portfolio_id,
        security_id,
        ticker,
        security_name,
        security_type,
        asset_class,
        sector,
        industry,
        trade_date,
        trade_type,
        trade_category,
        quantity,
        execution_price,
        net_amount,
        commission,
        -- ISSUE: Multiple window functions with same partition
        sum(case
            when trade_category = 'PURCHASE' then quantity
            when trade_category = 'SALE' then -quantity
            else 0
        end) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as running_position,
        sum(case
            when trade_category = 'PURCHASE' then net_amount
            when trade_category = 'SALE' then -net_amount
            else 0
        end) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as cumulative_cost,
        -- ISSUE: Another separate window for purchase-only
        sum(case when trade_category = 'PURCHASE' then quantity else 0 end) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as cumulative_purchased_qty,
        sum(case when trade_category = 'PURCHASE' then net_amount else 0 end) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as cumulative_purchase_cost
    from trades
),

-- ISSUE: Separate CTE for cost basis
with_cost_basis as (
    select
        *,
        case
            when cumulative_purchased_qty > 0
            then cumulative_purchase_cost / cumulative_purchased_qty
            else null
        end as avg_cost_basis
    from positions
),

-- ISSUE: Another pass for realized P&L
with_pnl as (
    select
        *,
        case
            when trade_category = 'SALE' and avg_cost_basis is not null
            then (execution_price - avg_cost_basis) * quantity
            else null
        end as realized_pnl,
        case
            when trade_category = 'SALE' and avg_cost_basis is not null and avg_cost_basis > 0
            then (execution_price - avg_cost_basis) / avg_cost_basis * 100
            else null
        end as realized_pnl_pct
    from with_cost_basis
)

select * from with_pnl
  )
/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.int_trade_pnl"} */;
[0m21:52:47.307458 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 0.599 seconds
[0m21:52:47.309889 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f8c3730>]}
[0m21:52:47.310565 [info ] [Thread-2  ]: 21 of 30 OK created sql view model DEV_pipeline_c.int_sector_attribution ....... [[32mSUCCESS 1[0m in 0.63s]
[0m21:52:47.311140 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.int_sector_attribution
[0m21:52:47.311543 [debug] [Thread-2  ]: Began running node model.bain_capital_portfolio_analytics.fact_fund_summary
[0m21:52:47.312084 [info ] [Thread-2  ]: 24 of 30 START sql table model DEV_pipeline_c.fact_fund_summary ................ [RUN]
[0m21:52:47.312615 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_sector_attribution, now model.bain_capital_portfolio_analytics.fact_fund_summary)
[0m21:52:47.312984 [debug] [Thread-2  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_fund_summary
[0m21:52:47.317231 [debug] [Thread-2  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_fund_summary"
[0m21:52:47.317941 [debug] [Thread-2  ]: Began executing node model.bain_capital_portfolio_analytics.fact_fund_summary
[0m21:52:47.321974 [debug] [Thread-2  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_fund_summary"
[0m21:52:47.323969 [debug] [Thread-2  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_fund_summary"
[0m21:52:47.324350 [debug] [Thread-2  ]: On model.bain_capital_portfolio_analytics.fact_fund_summary: create or replace transient table DBT_DEMO.DEV_pipeline_c.fact_fund_summary
    
    
    
    as (-- Pipeline C: Complex Portfolio Analytics
-- Model: fact_fund_summary
-- Description: Fund-level summary metrics
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Based on already-aggregated data
-- 2. Could push more logic upstream

with fund_rollup as (
    select * from DBT_DEMO.DEV_pipeline_c.int_fund_rollup
),

fund_hierarchy as (
    select * from DBT_DEMO.DEV_pipeline_c.stg_fund_hierarchy
),

-- Get parent fund info
with_parent as (
    select
        fr.*,
        parent.entity_name as parent_fund_name
    from fund_rollup fr
    left join fund_hierarchy parent
        on fr.parent_entity_id = parent.entity_id
),

final as (
    select
        md5(cast(coalesce(cast(fund_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(valuation_date as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as fund_summary_key,
        fund_id,
        fund_name,
        parent_entity_id as parent_fund_id,
        parent_fund_name,
        hierarchy_level,
        valuation_date,
        portfolio_count,
        total_nav_usd,
        weighted_return_1y,
        weighted_volatility_1y,
        weighted_sharpe_ratio,
        worst_drawdown,
        total_var_95,
        -- ISSUE: Calculated fields
        case
            when weighted_return_1y >= 0.15 then 'HIGH'
            when weighted_return_1y >= 0.08 then 'MEDIUM'
            when weighted_return_1y >= 0 then 'LOW'
            else 'NEGATIVE'
        end as return_tier,
        case
            when weighted_sharpe_ratio >= 1.5 then 'EXCELLENT'
            when weighted_sharpe_ratio >= 1.0 then 'GOOD'
            when weighted_sharpe_ratio >= 0.5 then 'FAIR'
            else 'POOR'
        end as risk_adjusted_tier
    from with_parent
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_fund_summary"} */;
[0m21:52:47.401520 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 1.892 seconds
[0m21:52:47.404874 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115366d60>]}
[0m21:52:47.405690 [info ] [Thread-3  ]: 13 of 30 OK created sql table model DEV_pipeline_a.fact_cashflow_summary ....... [[32mSUCCESS 1[0m in 1.97s]
[0m21:52:47.406381 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.fact_cashflow_summary
[0m21:52:47.406857 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.fact_sector_performance
[0m21:52:47.407494 [info ] [Thread-3  ]: 25 of 30 START sql table model DEV_pipeline_c.fact_sector_performance .......... [RUN]
[0m21:52:47.407978 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.fact_cashflow_summary, now model.bain_capital_portfolio_analytics.fact_sector_performance)
[0m21:52:47.408349 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_sector_performance
[0m21:52:47.413115 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_sector_performance"
[0m21:52:47.413856 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.fact_sector_performance
[0m21:52:47.416813 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_sector_performance"
[0m21:52:47.418662 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_sector_performance"
[0m21:52:47.419053 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.fact_sector_performance: create or replace transient table DBT_DEMO.DEV_pipeline_c.fact_sector_performance
    
    
    
    as (-- Pipeline C: Complex Portfolio Analytics
-- Model: fact_sector_performance
-- Description: Sector-level performance aggregation
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Re-aggregates data from upstream
-- 2. Complex window functions

with sector_attribution as (
    select * from DBT_DEMO.DEV_pipeline_c.int_sector_attribution
),

portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

-- ISSUE: Another portfolio join
with_portfolio_info as (
    select
        sa.*,
        p.portfolio_name,
        p.portfolio_type,
        p.fund_id
    from sector_attribution sa
    inner join portfolios p
        on sa.portfolio_id = p.portfolio_id
),

-- ISSUE: More window functions for sector ranking
with_rankings as (
    select
        *,
        rank() over (
            partition by portfolio_id, position_date
            order by sector_weight desc
        ) as sector_weight_rank,
        rank() over (
            partition by portfolio_id, position_date
            order by sector_contribution desc
        ) as sector_contribution_rank
    from with_portfolio_info
),

final as (
    select
        md5(cast(coalesce(cast(portfolio_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(sector as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(position_date as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as sector_performance_key,
        *,
        case
            when sector_weight_rank <= 3 then 'TOP_3'
            when sector_weight_rank <= 5 then 'TOP_5'
            else 'OTHER'
        end as sector_weight_tier
    from with_rankings
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_sector_performance"} */;
[0m21:52:47.962050 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 0.681 seconds
[0m21:52:47.967051 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fc2e880>]}
[0m21:52:47.968242 [info ] [Thread-4  ]: 23 of 30 OK created sql view model DEV_pipeline_b.int_trade_pnl ................ [[32mSUCCESS 1[0m in 0.70s]
[0m21:52:47.969010 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.int_trade_pnl
[0m21:52:47.969871 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.fact_portfolio_positions
[0m21:52:47.970481 [info ] [Thread-4  ]: 26 of 30 START sql table model DEV_pipeline_b.fact_portfolio_positions ......... [RUN]
[0m21:52:47.971081 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.int_trade_pnl, now model.bain_capital_portfolio_analytics.fact_portfolio_positions)
[0m21:52:47.971561 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_portfolio_positions
[0m21:52:47.980507 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_portfolio_positions"
[0m21:52:47.981504 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.fact_portfolio_positions
[0m21:52:47.985473 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_portfolio_positions"
[0m21:52:47.994464 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_portfolio_positions"
[0m21:52:47.995102 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.fact_portfolio_positions: create or replace transient table DBT_DEMO.DEV_pipeline_b.fact_portfolio_positions
    
    
    
    as (-- Pipeline B: Trade Analytics Pipeline
-- Model: fact_portfolio_positions
-- Description: Current position snapshot by portfolio and security
-- DEPENDENCY: Uses fact_cashflow_summary from Pipeline A for portfolio cash context
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Gets latest position via subquery (should use QUALIFY)
-- 2. Self-joins for historical position lookups
-- 3. Correlated subqueries for portfolio-level aggregations
-- 4. Repeated window functions

with trade_pnl as (
    select * from DBT_DEMO.DEV_pipeline_b.int_trade_pnl
),

-- DEPENDENCY ON PIPELINE A: Get cashflow context for each portfolio
cashflow_summary as (
    select * from DBT_DEMO.DEV_pipeline_a.fact_cashflow_summary
),

-- Aggregate cashflows by portfolio to get total contributions/distributions
portfolio_cashflows as (
    select
        portfolio_id,
        sum(case when cashflow_type = 'CONTRIBUTION' then cumulative_total else 0 end) as total_contributions,
        sum(case when cashflow_type = 'DISTRIBUTION' then abs(cumulative_total) else 0 end) as total_distributions,
        max(cashflow_month) as last_cashflow_date
    from cashflow_summary
    group by portfolio_id
),

latest_positions as (
    select *
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id, security_id
                order by trade_date desc, trade_id desc
            ) as rn
        from trade_pnl
    )
    where rn = 1
),

-- ISSUE: Self-join to get position 30 days ago
positions_30d_ago as (
    select
        portfolio_id,
        security_id,
        running_position as position_30d_ago,
        avg_cost_basis as cost_basis_30d_ago
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id, security_id
                order by trade_date desc, trade_id desc
            ) as rn
        from trade_pnl
        where trade_date <= dateadd(day, -30, current_date())
    )
    where rn = 1
),

-- ISSUE: Self-join to get position 90 days ago
positions_90d_ago as (
    select
        portfolio_id,
        security_id,
        running_position as position_90d_ago,
        avg_cost_basis as cost_basis_90d_ago
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id, security_id
                order by trade_date desc, trade_id desc
            ) as rn
        from trade_pnl
        where trade_date <= dateadd(day, -90, current_date())
    )
    where rn = 1
),

market_prices as (
    select
        security_id,
        close_price as current_price,
        price_date
    from (
        select
            security_id,
            close_price,
            price_date,
            row_number() over (partition by security_id order by price_date desc) as rn
        from DBT_DEMO.DEV_pipeline_b.stg_market_prices
    )
    where rn = 1  -- ISSUE: Again, should use QUALIFY
),

-- ISSUE: Get historical prices for comparison
market_prices_30d_ago as (
    select
        security_id,
        close_price as price_30d_ago
    from (
        select
            security_id,
            close_price,
            row_number() over (partition by security_id order by price_date desc) as rn
        from DBT_DEMO.DEV_pipeline_b.stg_market_prices
        where price_date <= dateadd(day, -30, current_date())
    )
    where rn = 1
),

-- ISSUE: Join all the position snapshots together
enriched_positions as (
    select
        lp.*,
        mp.current_price,
        mp.price_date as price_as_of_date,
        p30.position_30d_ago,
        p30.cost_basis_30d_ago,
        p90.position_90d_ago,
        p90.cost_basis_90d_ago,
        mp30.price_30d_ago,
        pcf.total_contributions,
        pcf.total_distributions,
        pcf.last_cashflow_date
    from latest_positions lp
    left join market_prices mp
        on lp.security_id = mp.security_id
    left join positions_30d_ago p30
        on lp.portfolio_id = p30.portfolio_id
        and lp.security_id = p30.security_id
    left join positions_90d_ago p90
        on lp.portfolio_id = p90.portfolio_id
        and lp.security_id = p90.security_id
    left join market_prices_30d_ago mp30
        on lp.security_id = mp30.security_id
    left join portfolio_cashflows pcf
        on lp.portfolio_id = pcf.portfolio_id
    where lp.running_position != 0
),

-- ISSUE: Window functions for portfolio-level context
with_portfolio_context as (
    select
        ep.*,
        -- ISSUE: Repeated partition by portfolio_id
        sum(ep.running_position * ep.current_price) over (
            partition by ep.portfolio_id
        ) as portfolio_total_market_value,
        sum(ep.running_position * ep.avg_cost_basis) over (
            partition by ep.portfolio_id
        ) as portfolio_total_cost_basis,
        count(*) over (
            partition by ep.portfolio_id
        ) as portfolio_position_count,
        -- ISSUE: Rankings
        row_number() over (
            partition by ep.portfolio_id
            order by (ep.running_position * ep.current_price) desc
        ) as position_size_rank,
        row_number() over (
            partition by ep.portfolio_id
            order by ((ep.current_price - ep.avg_cost_basis) / nullif(ep.avg_cost_basis, 0)) desc
        ) as position_return_rank
    from enriched_positions ep
),

-- ISSUE: Separate aggregation for sector context (should use window functions)
sector_aggs as (
    select
        portfolio_id,
        sector,
        sum(running_position * current_price) as sector_market_value,
        count(*) as sector_position_count
    from enriched_positions
    group by 1, 2
),

with_sector_context as (
    select
        wpc.*,
        sa.sector_market_value,
        sa.sector_position_count
    from with_portfolio_context wpc
    left join sector_aggs sa
        on wpc.portfolio_id = sa.portfolio_id
        and wpc.sector = sa.sector
),

-- ISSUE: Complex calculations in final select
final as (
    select
        md5(cast(coalesce(cast(wsc.portfolio_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wsc.security_id as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as position_key,
        wsc.portfolio_id,
        wsc.security_id,
        wsc.ticker,
        wsc.security_name,
        wsc.sector,
        wsc.asset_class,
        wsc.running_position as current_quantity,
        wsc.avg_cost_basis,
        wsc.current_price,
        wsc.price_as_of_date,
        -- Core calculations
        wsc.running_position * wsc.avg_cost_basis as cost_basis_value,
        wsc.running_position * wsc.current_price as market_value,
        (wsc.running_position * wsc.current_price) - (wsc.running_position * wsc.avg_cost_basis) as unrealized_pnl,
        -- ISSUE: Repeated division logic
        case
            when wsc.avg_cost_basis > 0
            then ((wsc.current_price - wsc.avg_cost_basis) / wsc.avg_cost_basis) * 100
            else null
        end as unrealized_pnl_pct,
        -- Portfolio context
        wsc.portfolio_total_market_value,
        wsc.portfolio_total_cost_basis,
        wsc.portfolio_position_count,
        -- ISSUE: Weight calculation (repeated division)
        case
            when wsc.portfolio_total_market_value > 0
            then ((wsc.running_position * wsc.current_price) / wsc.portfolio_total_market_value) * 100
            else null
        end as portfolio_weight_pct,
        -- Sector context
        wsc.sector_market_value,
        wsc.sector_position_count,
        case
            when wsc.sector_market_value > 0
            then ((wsc.running_position * wsc.current_price) / wsc.sector_market_value) * 100
            else null
        end as sector_weight_pct,
        -- Historical comparison
        wsc.position_30d_ago,
        wsc.position_90d_ago,
        wsc.position_30d_ago - wsc.running_position as position_change_30d,
        wsc.position_90d_ago - wsc.running_position as position_change_90d,
        -- Price momentum
        wsc.price_30d_ago,
        case
            when wsc.price_30d_ago > 0
            then ((wsc.current_price - wsc.price_30d_ago) / wsc.price_30d_ago) * 100
            else null
        end as price_change_30d_pct,
        -- Cashflow context from Pipeline A
        wsc.total_contributions,
        wsc.total_distributions,
        wsc.last_cashflow_date,
        -- Rankings
        wsc.position_size_rank,
        wsc.position_return_rank,
        -- ISSUE: Complex classification
        case
            when ((wsc.running_position * wsc.current_price) / nullif(wsc.portfolio_total_market_value, 0)) > 0.10 then 'CONCENTRATED'
            when ((wsc.running_position * wsc.current_price) / nullif(wsc.portfolio_total_market_value, 0)) > 0.05 then 'SIGNIFICANT'
            when ((wsc.running_position * wsc.current_price) / nullif(wsc.portfolio_total_market_value, 0)) > 0.02 then 'MODERATE'
            else 'SMALL'
        end as position_size_category,
        wsc.cumulative_purchase_cost as total_invested,
        current_timestamp() as snapshot_timestamp
    from with_sector_context wsc
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_portfolio_positions"} */;
[0m21:52:48.141792 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 1.439 seconds
[0m21:52:48.145922 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fc91400>]}
[0m21:52:49.119488 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 1.700 seconds
[0m21:52:49.121236 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fc55f70>]}
[0m21:52:49.263066 [debug] [Thread-1  ]: An error was encountered while trying to send an event
[0m21:52:49.264457 [info ] [Thread-3  ]: 25 of 30 OK created sql table model DEV_pipeline_c.fact_sector_performance ..... [[32mSUCCESS 1[0m in 1.71s]
[0m21:52:49.266681 [info ] [Thread-1  ]: 20 of 30 OK created sql table model DEV_pipeline_c.fact_position_snapshot ...... [[32mSUCCESS 1[0m in 1.47s]
[0m21:52:49.267611 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.fact_sector_performance
[0m21:52:49.268253 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.fact_position_snapshot
[0m21:52:49.268748 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.fact_trade_summary
[0m21:52:49.269440 [info ] [Thread-3  ]: 27 of 30 START sql table model DEV_pipeline_b.fact_trade_summary ............... [RUN]
[0m21:52:49.269926 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.fact_sector_performance, now model.bain_capital_portfolio_analytics.fact_trade_summary)
[0m21:52:49.270294 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_trade_summary
[0m21:52:49.276050 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_trade_summary"
[0m21:52:49.276771 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.fact_trade_summary
[0m21:52:49.279798 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_trade_summary"
[0m21:52:49.288158 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_trade_summary"
[0m21:52:49.288910 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.fact_trade_summary: create or replace transient table DBT_DEMO.DEV_pipeline_b.fact_trade_summary
    
    
    
    as (-- Pipeline B: Trade Analytics Pipeline
-- Model: fact_trade_summary
-- Description: Fact table for trade-level analysis
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-joins for prior trade lookups (should use LAG)
-- 2. Repeated window functions with same partitions
-- 3. Correlated subqueries for security-level aggregations
-- 4. Complex CASE statements repeated multiple times

with trade_pnl as (
    select * from DBT_DEMO.DEV_pipeline_b.int_trade_pnl
),

-- ISSUE: Getting portfolio data again (already available through joins upstream)
portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

-- ISSUE: Join that adds overhead
enriched as (
    select
        t.trade_id,
        t.portfolio_id,
        p.portfolio_name,
        p.portfolio_type,
        p.fund_id,
        t.security_id,
        t.ticker,
        t.security_name,
        t.security_type,
        t.asset_class,
        t.sector,
        t.trade_date,
        t.trade_type,
        t.trade_category,
        t.quantity,
        t.execution_price,
        t.net_amount,
        t.commission,
        t.running_position,
        t.avg_cost_basis,
        t.realized_pnl,
        t.realized_pnl_pct,
        -- ISSUE: Redundant date extractions (already done upstream)
        extract(year from t.trade_date) as trade_year,
        extract(month from t.trade_date) as trade_month,
        extract(quarter from t.trade_date) as trade_quarter,
        extract(dayofweek from t.trade_date) as trade_day_of_week,
        date_trunc('week', t.trade_date) as trade_week,
        date_trunc('month', t.trade_date) as trade_month_start
    from trade_pnl t
    left join portfolios p
        on t.portfolio_id = p.portfolio_id
),

-- ISSUE: Self-joins for prior trade comparisons (should use LAG)
-- Pre-compute trade sequence for self-join lookups
trade_sequences as (
    select
        *,
        row_number() over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
        ) as trade_seq
    from enriched
),

with_prior_trades as (
    select
        ts.*,
        -- ISSUE: Self-join for prior trade same security (should use LAG)
        ts_prior.execution_price as prior_trade_price,
        ts_prior.trade_date as prior_trade_date,
        ts_prior.quantity as prior_trade_quantity,
        -- ISSUE: Self-join for 5 trades ago (should use LAG offset)
        ts_5.execution_price as price_5_trades_ago,
        -- ISSUE: Self-join for 10 trades ago (should use LAG offset)
        ts_10.execution_price as price_10_trades_ago
    from trade_sequences ts
    left join trade_sequences ts_prior
        on ts.portfolio_id = ts_prior.portfolio_id
        and ts.security_id = ts_prior.security_id
        and ts_prior.trade_seq = ts.trade_seq - 1
    left join trade_sequences ts_5
        on ts.portfolio_id = ts_5.portfolio_id
        and ts.security_id = ts_5.security_id
        and ts_5.trade_seq = ts.trade_seq - 5
    left join trade_sequences ts_10
        on ts.portfolio_id = ts_10.portfolio_id
        and ts.security_id = ts_10.security_id
        and ts_10.trade_seq = ts.trade_seq - 10
),

-- ISSUE: Multiple window functions with repeated partitions
with_window_calcs as (
    select
        wpt.*,
        -- ISSUE: Running aggregations (repeated partition by portfolio_id, security_id)
        sum(quantity) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between unbounded preceding and current row
        ) as cumulative_quantity,
        sum(abs(net_amount)) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between unbounded preceding and current row
        ) as cumulative_trade_value,
        sum(coalesce(realized_pnl, 0)) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between unbounded preceding and current row
        ) as cumulative_realized_pnl,
        -- ISSUE: Moving averages (same partition repeated)
        avg(execution_price) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between 4 preceding and current row
        ) as rolling_5_trade_avg_price,
        avg(execution_price) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between 9 preceding and current row
        ) as rolling_10_trade_avg_price,
        avg(execution_price) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between 19 preceding and current row
        ) as rolling_20_trade_avg_price,
        -- ISSUE: More window calculations
        stddev(execution_price) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between 9 preceding and current row
        ) as rolling_10_trade_price_stddev,
        count(*) over (
            partition by wpt.portfolio_id, wpt.security_id
            order by wpt.trade_date, wpt.trade_id
            rows between unbounded preceding and current row
        ) as trade_sequence_number,
        -- ISSUE: Rankings (same partition again)
        row_number() over (
            partition by wpt.portfolio_id, wpt.security_id, wpt.trade_category
            order by abs(wpt.net_amount) desc
        ) as size_rank_within_category
    from with_prior_trades wpt
),

-- ISSUE: Separate CTE for running trade stats (should be combined with window calcs above)
security_trade_aggs as (
    select
        portfolio_id,
        security_id,
        trade_date,
        trade_id,
        -- ISSUE: These window functions duplicate the partition from with_window_calcs
        count(*) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as total_portfolio_trades_this_security,
        avg(execution_price) over (
            partition by portfolio_id, security_id
            order by trade_date, trade_id
            rows between unbounded preceding and current row
        ) as avg_portfolio_price_this_security
    from enriched
),

-- ISSUE: Separate aggregation for fund-level volume (should be combined upstream)
fund_daily_volume as (
    select
        fund_id,
        security_id,
        trade_date,
        sum(abs(net_amount)) as fund_total_volume_same_security_same_day
    from enriched
    group by 1, 2, 3
),

with_security_context as (
    select
        wwc.*,
        sta.total_portfolio_trades_this_security,
        sta.avg_portfolio_price_this_security,
        fdv.fund_total_volume_same_security_same_day
    from with_window_calcs wwc
    left join security_trade_aggs sta
        on wwc.portfolio_id = sta.portfolio_id
        and wwc.security_id = sta.security_id
        and wwc.trade_id = sta.trade_id
    left join fund_daily_volume fdv
        on wwc.fund_id = fdv.fund_id
        and wwc.security_id = fdv.security_id
        and wwc.trade_date = fdv.trade_date
),

-- ISSUE: Complex derived metrics with repeated CASE statements
final as (
    select
        md5(cast(coalesce(cast(wsc.trade_id as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as trade_key,
        wsc.*,
        -- ISSUE: Price change calculations (repeated division logic)
        case
            when wsc.prior_trade_price is not null and wsc.prior_trade_price > 0
            then ((wsc.execution_price - wsc.prior_trade_price) / wsc.prior_trade_price) * 100
            else null
        end as price_change_from_prior_pct,
        case
            when wsc.price_5_trades_ago is not null and wsc.price_5_trades_ago > 0
            then ((wsc.execution_price - wsc.price_5_trades_ago) / wsc.price_5_trades_ago) * 100
            else null
        end as price_change_from_5_trades_ago_pct,
        case
            when wsc.rolling_20_trade_avg_price is not null and wsc.rolling_20_trade_avg_price > 0
            then ((wsc.execution_price - wsc.rolling_20_trade_avg_price) / wsc.rolling_20_trade_avg_price) * 100
            else null
        end as deviation_from_20_trade_avg_pct,
        -- ISSUE: Trade size classification (repeated CASE)
        case
            when abs(wsc.net_amount) >= 10000000 then 'BLOCK_TRADE'
            when abs(wsc.net_amount) >= 1000000 then 'LARGE'
            when abs(wsc.net_amount) >= 100000 then 'MEDIUM'
            when abs(wsc.net_amount) >= 10000 then 'SMALL'
            else 'MICRO'
        end as trade_size_category,
        -- ISSUE: Trade timing classification (complex nested CASE)
        case
            when wsc.execution_price > wsc.rolling_10_trade_avg_price * 1.1 then 'BOUGHT_HIGH'
            when wsc.execution_price > wsc.rolling_10_trade_avg_price * 1.03 then 'ABOVE_AVERAGE'
            when wsc.execution_price < wsc.rolling_10_trade_avg_price * 0.9 then 'BOUGHT_LOW'
            when wsc.execution_price < wsc.rolling_10_trade_avg_price * 0.97 then 'BELOW_AVERAGE'
            else 'AVERAGE'
        end as execution_quality,
        -- ISSUE: Momentum signal (repeated logic)
        case
            when wsc.rolling_5_trade_avg_price > wsc.rolling_20_trade_avg_price then 'UPTREND'
            when wsc.rolling_5_trade_avg_price < wsc.rolling_20_trade_avg_price then 'DOWNTREND'
            else 'NEUTRAL'
        end as price_momentum,
        -- ISSUE: Volatility classification
        case
            when wsc.rolling_10_trade_price_stddev < wsc.rolling_10_trade_avg_price * 0.02 then 'LOW_VOLATILITY'
            when wsc.rolling_10_trade_price_stddev < wsc.rolling_10_trade_avg_price * 0.05 then 'MODERATE_VOLATILITY'
            when wsc.rolling_10_trade_price_stddev < wsc.rolling_10_trade_avg_price * 0.10 then 'HIGH_VOLATILITY'
            else 'VERY_HIGH_VOLATILITY'
        end as price_volatility_regime,
        -- ISSUE: Trade frequency indicator
        case
            when wsc.trade_sequence_number >= 100 then 'VERY_ACTIVE'
            when wsc.trade_sequence_number >= 50 then 'ACTIVE'
            when wsc.trade_sequence_number >= 20 then 'MODERATE'
            when wsc.trade_sequence_number >= 5 then 'LIGHT'
            else 'FIRST_FEW_TRADES'
        end as trading_activity_level
    from with_security_context wsc
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_trade_summary"} */;
[0m21:52:52.219037 [debug] [Thread-2  ]: SQL status: SUCCESS 1 in 4.894 seconds
[0m21:52:52.223770 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f8c3730>]}
[0m21:52:52.224940 [info ] [Thread-2  ]: 24 of 30 OK created sql table model DEV_pipeline_c.fact_fund_summary ........... [[32mSUCCESS 1[0m in 4.91s]
[0m21:52:52.225706 [debug] [Thread-2  ]: Finished running node model.bain_capital_portfolio_analytics.fact_fund_summary
[0m21:52:57.428487 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 9.432 seconds
[0m21:52:57.432460 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1153b4700>]}
[0m21:52:57.433538 [info ] [Thread-4  ]: 26 of 30 OK created sql table model DEV_pipeline_b.fact_portfolio_positions .... [[32mSUCCESS 1[0m in 9.46s]
[0m21:52:57.434203 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.fact_portfolio_positions
[0m21:53:18.095059 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 28.805 seconds
[0m21:53:18.100285 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f595b50>]}
[0m21:53:18.101399 [info ] [Thread-3  ]: 27 of 30 OK created sql table model DEV_pipeline_b.fact_trade_summary .......... [[32mSUCCESS 1[0m in 28.83s]
[0m21:53:18.102139 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.fact_trade_summary
[0m21:53:18.103053 [debug] [Thread-1  ]: Began running node model.bain_capital_portfolio_analytics.fact_portfolio_performance
[0m21:53:18.103651 [info ] [Thread-1  ]: 28 of 30 START sql table model DEV_pipeline_c.fact_portfolio_performance ....... [RUN]
[0m21:53:18.104183 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.fact_position_snapshot, now model.bain_capital_portfolio_analytics.fact_portfolio_performance)
[0m21:53:18.104591 [debug] [Thread-1  ]: Began compiling node model.bain_capital_portfolio_analytics.fact_portfolio_performance
[0m21:53:18.111856 [debug] [Thread-1  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.fact_portfolio_performance"
[0m21:53:18.112656 [debug] [Thread-1  ]: Began executing node model.bain_capital_portfolio_analytics.fact_portfolio_performance
[0m21:53:18.117244 [debug] [Thread-1  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.fact_portfolio_performance"
[0m21:53:18.127117 [debug] [Thread-1  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.fact_portfolio_performance"
[0m21:53:18.127922 [debug] [Thread-1  ]: On model.bain_capital_portfolio_analytics.fact_portfolio_performance: create or replace transient table DBT_DEMO.DEV_pipeline_c.fact_portfolio_performance
    
    
    
    as (-- Pipeline C: Complex Portfolio Analytics
-- Model: fact_portfolio_performance
-- Description: Main performance fact table
-- DEPENDENCIES:
--   - Pipeline A: fact_cashflow_summary (for cashflow context)
--   - Pipeline B: fact_trade_summary, fact_portfolio_positions (for trading/position context)
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Multiple self-joins for period comparisons
-- 2. Repeated window functions with same partitions
-- 3. Complex CASE statements repeated multiple times
-- 4. Correlated subqueries
-- 5. Late filtering and unnecessary full table scans

with portfolio_vs_benchmark as (
    select * from DBT_DEMO.DEV_pipeline_c.int_portfolio_vs_benchmark
),

risk_metrics as (
    select * from DBT_DEMO.DEV_pipeline_c.int_risk_metrics
),

portfolios as (
    select * from DBT_DEMO.DEV_pipeline_a.stg_portfolios
),

-- DEPENDENCY ON PIPELINE A: Cashflow summary for capital deployment context
cashflow_summary as (
    select * from DBT_DEMO.DEV_pipeline_a.fact_cashflow_summary
),

-- Aggregate cashflows to portfolio level
portfolio_cashflow_totals as (
    select
        portfolio_id,
        sum(case when cashflow_type = 'CONTRIBUTION' then cumulative_total else 0 end) as total_contributions,
        sum(case when cashflow_type = 'DISTRIBUTION' then abs(cumulative_total) else 0 end) as total_distributions,
        sum(cumulative_total) as net_cashflow
    from cashflow_summary
    group by portfolio_id
),

-- DEPENDENCY ON PIPELINE B: Trade summary for trading activity context
trade_summary as (
    select * from DBT_DEMO.DEV_pipeline_b.fact_trade_summary
),

-- Aggregate trades to portfolio/date level
portfolio_trade_activity as (
    select
        portfolio_id,
        trade_month_start as activity_month,
        count(*) as trade_count,
        sum(case when trade_category = 'PURCHASE' then abs(net_amount) else 0 end) as total_purchases,
        sum(case when trade_category = 'SALE' then abs(net_amount) else 0 end) as total_sales,
        sum(coalesce(realized_pnl, 0)) as realized_pnl
    from trade_summary
    group by portfolio_id, trade_month_start
),

-- DEPENDENCY ON PIPELINE B: Position snapshot for current holdings context
portfolio_positions as (
    select * from DBT_DEMO.DEV_pipeline_b.fact_portfolio_positions
),

-- Aggregate positions to portfolio level
portfolio_position_totals as (
    select
        portfolio_id,
        sum(market_value) as total_market_value,
        sum(cost_basis_value) as total_cost_basis,
        sum(unrealized_pnl) as total_unrealized_pnl,
        count(*) as position_count
    from portfolio_positions
    group by portfolio_id
),

-- ISSUE: Another join when data could flow from upstream
combined as (
    select
        pvb.portfolio_id,
        pvb.valuation_date,
        pvb.nav,
        pvb.nav_usd,
        -- Portfolio returns
        pvb.portfolio_daily_return,
        pvb.portfolio_cumulative_return,
        pvb.portfolio_return_1m,
        pvb.portfolio_return_3m,
        pvb.portfolio_return_1y,
        pvb.portfolio_volatility,
        -- Benchmark comparison
        pvb.benchmark_id,
        pvb.benchmark_daily_return,
        pvb.benchmark_cumulative_return,
        pvb.benchmark_return_1m,
        pvb.benchmark_return_3m,
        pvb.benchmark_return_1y,
        pvb.benchmark_volatility,
        -- Relative performance
        pvb.daily_excess_return,
        pvb.cumulative_excess_return,
        pvb.excess_return_1m,
        pvb.excess_return_3m,
        pvb.excess_return_1y,
        pvb.tracking_error_1y,
        pvb.annualized_alpha,
        pvb.information_ratio,
        -- Risk metrics
        rm.drawdown,
        rm.max_drawdown,
        rm.downside_deviation_1y,
        rm.sharpe_ratio,
        rm.sortino_ratio,
        rm.var_95_1d,
        rm.var_99_1d
    from portfolio_vs_benchmark pvb
    inner join risk_metrics rm
        on pvb.portfolio_id = rm.portfolio_id
        and pvb.valuation_date = rm.valuation_date
),

-- ISSUE: Self-join for prior period comparisons (should use LAG)
with_prior_periods as (
    select
        c.*,
        -- ISSUE: Self-join for 1 day ago
        c1d.nav_usd as nav_1d_ago,
        c1d.portfolio_cumulative_return as return_1d_ago,
        -- ISSUE: Self-join for 1 week ago
        c1w.nav_usd as nav_1w_ago,
        c1w.portfolio_cumulative_return as return_1w_ago,
        -- ISSUE: Self-join for 1 month ago
        c1m.nav_usd as nav_1m_ago,
        c1m.portfolio_cumulative_return as return_1m_ago,
        -- ISSUE: Self-join for 3 months ago
        c3m.nav_usd as nav_3m_ago,
        c3m.portfolio_cumulative_return as return_3m_ago,
        -- ISSUE: Self-join for 1 year ago
        c1y.nav_usd as nav_1y_ago,
        c1y.portfolio_cumulative_return as return_1y_ago
    from combined c
    left join combined c1d
        on c.portfolio_id = c1d.portfolio_id
        and c1d.valuation_date = dateadd(day, -1, c.valuation_date)
    left join combined c1w
        on c.portfolio_id = c1w.portfolio_id
        and c1w.valuation_date = dateadd(day, -7, c.valuation_date)
    left join combined c1m
        on c.portfolio_id = c1m.portfolio_id
        and c1m.valuation_date = dateadd(month, -1, c.valuation_date)
    left join combined c3m
        on c.portfolio_id = c3m.portfolio_id
        and c3m.valuation_date = dateadd(month, -3, c.valuation_date)
    left join combined c1y
        on c.portfolio_id = c1y.portfolio_id
        and c1y.valuation_date = dateadd(year, -1, c.valuation_date)
),

-- ISSUE: Multiple window functions with repeated partitions
with_rankings as (
    select
        wpp.*,
        -- ISSUE: Multiple ROW_NUMBER with same partition
        row_number() over (
            partition by wpp.portfolio_id
            order by wpp.portfolio_cumulative_return desc
        ) as best_performance_rank,
        row_number() over (
            partition by wpp.portfolio_id
            order by wpp.portfolio_cumulative_return asc
        ) as worst_performance_rank,
        row_number() over (
            partition by wpp.portfolio_id
            order by wpp.sharpe_ratio desc nulls last
        ) as best_sharpe_rank,
        -- ISSUE: DENSE_RANK with same partition
        dense_rank() over (
            partition by wpp.portfolio_id
            order by wpp.nav_usd desc
        ) as nav_size_rank,
        -- ISSUE: More window calculations
        avg(wpp.portfolio_daily_return) over (
            partition by wpp.portfolio_id
            order by wpp.valuation_date
            rows between 20 preceding and current row
        ) as rolling_20d_avg_return,
        avg(wpp.portfolio_daily_return) over (
            partition by wpp.portfolio_id
            order by wpp.valuation_date
            rows between 60 preceding and current row
        ) as rolling_60d_avg_return,
        stddev(wpp.portfolio_daily_return) over (
            partition by wpp.portfolio_id
            order by wpp.valuation_date
            rows between 20 preceding and current row
        ) as rolling_20d_volatility,
        stddev(wpp.portfolio_daily_return) over (
            partition by wpp.portfolio_id
            order by wpp.valuation_date
            rows between 60 preceding and current row
        ) as rolling_60d_volatility
    from with_prior_periods wpp
),

-- ISSUE: Complex derived metrics with repeated CASE statements
with_derived_metrics as (
    select
        wr.*,
        -- ISSUE: Repeated complex CASE for performance classification
        case
            when wr.portfolio_cumulative_return >= 0.50 then 'EXCEPTIONAL'
            when wr.portfolio_cumulative_return >= 0.30 then 'EXCELLENT'
            when wr.portfolio_cumulative_return >= 0.15 then 'VERY_GOOD'
            when wr.portfolio_cumulative_return >= 0.05 then 'GOOD'
            when wr.portfolio_cumulative_return >= 0.00 then 'NEUTRAL'
            when wr.portfolio_cumulative_return >= -0.05 then 'POOR'
            when wr.portfolio_cumulative_return >= -0.15 then 'VERY_POOR'
            else 'UNACCEPTABLE'
        end as performance_rating,
        -- ISSUE: Same CASE for risk classification
        case
            when wr.sharpe_ratio >= 3.0 then 'EXCEPTIONAL'
            when wr.sharpe_ratio >= 2.0 then 'EXCELLENT'
            when wr.sharpe_ratio >= 1.5 then 'VERY_GOOD'
            when wr.sharpe_ratio >= 1.0 then 'GOOD'
            when wr.sharpe_ratio >= 0.5 then 'NEUTRAL'
            when wr.sharpe_ratio >= 0.0 then 'POOR'
            else 'VERY_POOR'
        end as risk_adjusted_rating,
        -- ISSUE: Complex calculation repeated
        case
            when wr.nav_1m_ago is not null and wr.nav_1m_ago > 0
            then (wr.nav_usd - wr.nav_1m_ago) / wr.nav_1m_ago
            else null
        end as nav_change_1m_pct,
        case
            when wr.nav_3m_ago is not null and wr.nav_3m_ago > 0
            then (wr.nav_usd - wr.nav_3m_ago) / wr.nav_3m_ago
            else null
        end as nav_change_3m_pct,
        case
            when wr.nav_1y_ago is not null and wr.nav_1y_ago > 0
            then (wr.nav_usd - wr.nav_1y_ago) / wr.nav_1y_ago
            else null
        end as nav_change_1y_pct,
        -- ISSUE: Momentum indicators with repeated logic
        case
            when wr.rolling_20d_avg_return > wr.rolling_60d_avg_return then 'POSITIVE_MOMENTUM'
            when wr.rolling_20d_avg_return < wr.rolling_60d_avg_return then 'NEGATIVE_MOMENTUM'
            else 'NEUTRAL_MOMENTUM'
        end as momentum_signal,
        -- ISSUE: Volatility regime classification
        case
            when wr.rolling_20d_volatility > wr.rolling_60d_volatility * 1.5 then 'HIGH_VOLATILITY'
            when wr.rolling_20d_volatility > wr.rolling_60d_volatility * 1.2 then 'ELEVATED_VOLATILITY'
            when wr.rolling_20d_volatility < wr.rolling_60d_volatility * 0.8 then 'LOW_VOLATILITY'
            else 'NORMAL_VOLATILITY'
        end as volatility_regime
    from with_rankings wr
),

-- ISSUE: Separate peer aggregation CTEs (should use window functions with partition by portfolio_type)
peer_return_aggs as (
    select
        p2.portfolio_type,
        pvb2.valuation_date,
        avg(pvb2.portfolio_cumulative_return) as peer_avg_return
    from portfolio_vs_benchmark pvb2
    inner join portfolios p2
        on pvb2.portfolio_id = p2.portfolio_id
    group by 1, 2
),

peer_sharpe_aggs as (
    select
        p2.portfolio_type,
        rm2.valuation_date,
        percentile_cont(0.5) within group (order by rm2.sharpe_ratio) as peer_median_sharpe
    from risk_metrics rm2
    inner join portfolios p2
        on rm2.portfolio_id = p2.portfolio_id
    group by 1, 2
),

with_peer_comparison as (
    select
        wdm.*,
        pra.peer_avg_return,
        psa.peer_median_sharpe
    from with_derived_metrics wdm
    inner join portfolios p_self
        on wdm.portfolio_id = p_self.portfolio_id
    left join peer_return_aggs pra
        on p_self.portfolio_type = pra.portfolio_type
        and wdm.valuation_date = pra.valuation_date
    left join peer_sharpe_aggs psa
        on p_self.portfolio_type = psa.portfolio_type
        and wdm.valuation_date = psa.valuation_date
),

-- ISSUE: Portfolio attributes added last
final as (
    select
        md5(cast(coalesce(cast(wpc.portfolio_id as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(wpc.valuation_date as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as performance_key,
        p.portfolio_name,
        p.portfolio_type,
        p.fund_id,
        wpc.*,
        -- ISSUE: Date dimensions recalculated (should be in dim table)
        extract(year from wpc.valuation_date) as valuation_year,
        extract(month from wpc.valuation_date) as valuation_month,
        extract(quarter from wpc.valuation_date) as valuation_quarter,
        extract(dayofweek from wpc.valuation_date) as valuation_day_of_week,
        extract(dayofyear from wpc.valuation_date) as valuation_day_of_year,
        date_trunc('month', wpc.valuation_date) as valuation_month_start,
        date_trunc('quarter', wpc.valuation_date) as valuation_quarter_start,
        date_trunc('year', wpc.valuation_date) as valuation_year_start,
        -- ISSUE: String concatenations (slow)
        concat(p.portfolio_name, ' - ', wpc.valuation_date::varchar) as display_name,
        concat('Q', extract(quarter from wpc.valuation_date), ' ', extract(year from wpc.valuation_date)) as quarter_label,
        -- ISSUE: Complex derived field
        case
            when wpc.portfolio_cumulative_return > wpc.peer_avg_return then 'OUTPERFORMING'
            when wpc.portfolio_cumulative_return < wpc.peer_avg_return then 'UNDERPERFORMING'
            else 'AT_PEER'
        end as peer_relative_performance
    from with_peer_comparison wpc
    inner join portfolios p
        on wpc.portfolio_id = p.portfolio_id
)

select * from final
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.fact_portfolio_performance"} */;
[0m21:53:31.222953 [debug] [Thread-1  ]: SQL status: SUCCESS 1 in 13.094 seconds
[0m21:53:31.228144 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fca5ee0>]}
[0m21:53:31.229299 [info ] [Thread-1  ]: 28 of 30 OK created sql table model DEV_pipeline_c.fact_portfolio_performance .. [[32mSUCCESS 1[0m in 13.12s]
[0m21:53:31.230072 [debug] [Thread-1  ]: Finished running node model.bain_capital_portfolio_analytics.fact_portfolio_performance
[0m21:53:31.231263 [debug] [Thread-4  ]: Began running node model.bain_capital_portfolio_analytics.report_ic_dashboard
[0m21:53:31.231833 [debug] [Thread-3  ]: Began running node model.bain_capital_portfolio_analytics.report_lp_quarterly
[0m21:53:31.232513 [info ] [Thread-4  ]: 29 of 30 START sql table model DEV_pipeline_c.report_ic_dashboard .............. [RUN]
[0m21:53:31.233042 [info ] [Thread-3  ]: 30 of 30 START sql table model DEV_pipeline_c.report_lp_quarterly .............. [RUN]
[0m21:53:31.233568 [debug] [Thread-4  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.fact_portfolio_positions, now model.bain_capital_portfolio_analytics.report_ic_dashboard)
[0m21:53:31.233966 [debug] [Thread-3  ]: Re-using an available connection from the pool (formerly model.bain_capital_portfolio_analytics.fact_trade_summary, now model.bain_capital_portfolio_analytics.report_lp_quarterly)
[0m21:53:31.234361 [debug] [Thread-4  ]: Began compiling node model.bain_capital_portfolio_analytics.report_ic_dashboard
[0m21:53:31.234724 [debug] [Thread-3  ]: Began compiling node model.bain_capital_portfolio_analytics.report_lp_quarterly
[0m21:53:31.241807 [debug] [Thread-4  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.report_ic_dashboard"
[0m21:53:31.247141 [debug] [Thread-3  ]: Writing injected SQL for node "model.bain_capital_portfolio_analytics.report_lp_quarterly"
[0m21:53:31.248326 [debug] [Thread-4  ]: Began executing node model.bain_capital_portfolio_analytics.report_ic_dashboard
[0m21:53:31.248738 [debug] [Thread-3  ]: Began executing node model.bain_capital_portfolio_analytics.report_lp_quarterly
[0m21:53:31.252163 [debug] [Thread-4  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.report_ic_dashboard"
[0m21:53:31.255023 [debug] [Thread-3  ]: Writing runtime sql for node "model.bain_capital_portfolio_analytics.report_lp_quarterly"
[0m21:53:31.259274 [debug] [Thread-4  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.report_ic_dashboard"
[0m21:53:31.259844 [debug] [Thread-4  ]: On model.bain_capital_portfolio_analytics.report_ic_dashboard: create or replace transient table DBT_DEMO.DEV_pipeline_c.report_ic_dashboard
    
    
    
    as (-- Pipeline C: Complex Portfolio Analytics
-- Model: report_ic_dashboard
-- Description: Investment Committee dashboard report
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Combines multiple fact tables
-- 2. Re-aggregates aggregated data
-- 3. Complex pivoting logic
-- 4. Multiple CTEs that could be simplified

with portfolio_performance as (
    select * from DBT_DEMO.DEV_pipeline_c.fact_portfolio_performance
),

fund_summary as (
    select * from DBT_DEMO.DEV_pipeline_c.fact_fund_summary
),

position_snapshot as (
    select * from DBT_DEMO.DEV_pipeline_c.fact_position_snapshot
),

-- ISSUE: Get latest performance per portfolio
latest_portfolio_perf as (
    select *
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id
                order by valuation_date desc
            ) as rn
        from portfolio_performance
    )
    where rn = 1  -- ISSUE: Should use QUALIFY
),

-- ISSUE: Get latest positions per portfolio
latest_positions as (
    select *
    from (
        select
            *,
            row_number() over (
                partition by portfolio_id, security_id
                order by position_date desc
            ) as rn
        from position_snapshot
    )
    where rn = 1
),

-- ISSUE: Re-aggregate positions for portfolio summary
position_summary as (
    select
        portfolio_id,
        count(distinct security_id) as total_positions,
        count(distinct sector) as sector_count,
        sum(market_value_usd) as total_market_value,
        max(weight_pct) as max_position_weight,
        -- Concentration metrics
        sum(case when weight_pct >= 0.05 then 1 else 0 end) as positions_over_5pct
    from latest_positions
    group by 1
),

-- ISSUE: Sector concentration
sector_concentration as (
    select
        portfolio_id,
        listagg(sector, ', ') within group (order by sector_weight desc) as top_sectors
    from (
        select
            portfolio_id,
            sector,
            sum(weight_pct) as sector_weight,
            row_number() over (partition by portfolio_id order by sum(weight_pct) desc) as sector_rank
        from latest_positions
        group by 1, 2
    )
    where sector_rank <= 3
    group by 1
),

-- ISSUE: Combine all metrics
dashboard_data as (
    select
        lpp.portfolio_id,
        lpp.portfolio_name,
        lpp.portfolio_type,
        lpp.fund_id,
        fs.fund_name,
        lpp.valuation_date as as_of_date,
        lpp.nav_usd,
        -- Performance
        lpp.portfolio_return_1m,
        lpp.portfolio_return_3m,
        lpp.portfolio_return_1y,
        lpp.portfolio_cumulative_return as inception_return,
        -- Benchmark comparison
        lpp.benchmark_id,
        lpp.excess_return_1m,
        lpp.excess_return_1y,
        lpp.information_ratio,
        -- Risk
        lpp.portfolio_volatility,
        lpp.sharpe_ratio,
        lpp.sortino_ratio,
        lpp.max_drawdown,
        lpp.var_95_1d,
        -- Positions
        ps.total_positions,
        ps.sector_count,
        ps.max_position_weight,
        ps.positions_over_5pct,
        sc.top_sectors,
        -- Fund level
        fs.total_nav_usd as fund_total_nav,
        fs.portfolio_count as fund_portfolio_count,
        fs.weighted_sharpe_ratio as fund_sharpe,
        -- Portfolio share of fund
        case
            when fs.total_nav_usd > 0
            then lpp.nav_usd / fs.total_nav_usd * 100
            else null
        end as pct_of_fund
    from latest_portfolio_perf lpp
    left join fund_summary fs
        on lpp.fund_id = fs.fund_id
        and lpp.valuation_date = fs.valuation_date
    left join position_summary ps
        on lpp.portfolio_id = ps.portfolio_id
    left join sector_concentration sc
        on lpp.portfolio_id = sc.portfolio_id
),

-- ISSUE: Final scoring/ranking
final as (
    select
        *,
        -- Performance score (simplified)
        (coalesce(portfolio_return_1y, 0) * 0.4 +
         coalesce(sharpe_ratio, 0) * 0.3 +
         coalesce(information_ratio, 0) * 0.3) as composite_score,
        rank() over (order by portfolio_return_1y desc nulls last) as return_rank,
        rank() over (order by sharpe_ratio desc nulls last) as sharpe_rank
    from dashboard_data
)

select * from final
order by composite_score desc
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.report_ic_dashboard"} */;
[0m21:53:31.271936 [debug] [Thread-3  ]: Using snowflake connection "model.bain_capital_portfolio_analytics.report_lp_quarterly"
[0m21:53:31.272791 [debug] [Thread-3  ]: On model.bain_capital_portfolio_analytics.report_lp_quarterly: create or replace transient table DBT_DEMO.DEV_pipeline_c.report_lp_quarterly
    
    
    
    as (-- Pipeline C: Complex Portfolio Analytics
-- Model: report_lp_quarterly
-- Description: Quarterly LP reporting with period comparisons
--
-- ISSUES FOR ARTEMIS TO OPTIMIZE:
-- 1. Self-joins for multi-period comparisons (should use LAG)
-- 2. Repeated window functions with same partitions
-- 3. Correlated subqueries for fund-level aggregations
-- 4. Complex CASE statements repeated multiple times
-- 5. Late filtering after heavy computation

with portfolio_performance as (
    select * from DBT_DEMO.DEV_pipeline_c.fact_portfolio_performance
),

cashflow_summary as (
    select * from DBT_DEMO.DEV_pipeline_a.fact_cashflow_summary
),

-- ISSUE: Filter to quarter-end dates only (filtering late)
quarter_end_perf as (
    select *
    from portfolio_performance
    where valuation_date = last_day(valuation_date, 'quarter')
),

-- ISSUE: Aggregate cashflows to quarterly
quarterly_cashflows as (
    select
        portfolio_id,
        date_trunc('quarter', cashflow_month) as quarter_start,
        sum(case when cashflow_type = 'CONTRIBUTION' then total_amount else 0 end) as quarterly_contributions,
        sum(case when cashflow_type = 'DISTRIBUTION' then total_amount else 0 end) as quarterly_distributions,
        sum(case when cashflow_type = 'DIVIDEND' then total_amount else 0 end) as quarterly_dividends,
        sum(case when cashflow_type = 'FEE' then abs(total_amount) else 0 end) as quarterly_fees,
        sum(total_amount) as quarterly_net_cashflow,
        count(distinct transaction_count) as total_transactions
    from cashflow_summary
    group by 1, 2
),

-- ISSUE: Join performance and cashflows
combined as (
    select
        qep.portfolio_id,
        qep.portfolio_name,
        qep.portfolio_type,
        qep.fund_id,
        qep.valuation_date as quarter_end,
        qep.valuation_quarter_start as quarter_start,
        qep.valuation_year,
        qep.valuation_quarter,
        qep.nav_usd,
        qep.portfolio_return_3m as quarterly_return,
        qep.portfolio_return_1y,
        qep.portfolio_cumulative_return,
        qep.benchmark_return_3m as benchmark_quarterly_return,
        qep.excess_return_3m as quarterly_excess_return,
        qep.sharpe_ratio,
        qep.sortino_ratio,
        qep.max_drawdown,
        qep.performance_rating,
        qep.risk_adjusted_rating,
        qcf.quarterly_contributions,
        qcf.quarterly_distributions,
        qcf.quarterly_dividends,
        qcf.quarterly_fees,
        qcf.quarterly_net_cashflow,
        qcf.total_transactions
    from quarter_end_perf qep
    left join quarterly_cashflows qcf
        on qep.portfolio_id = qcf.portfolio_id
        and qep.valuation_quarter_start = qcf.quarter_start
),

-- ISSUE: Multiple self-joins for historical comparisons (should use LAG)
with_self_joins as (
    select
        c.*,
        -- ISSUE: Self-join for prior quarter
        c_q1.nav_usd as prior_1q_nav,
        c_q1.quarterly_return as prior_1q_return,
        c_q1.sharpe_ratio as prior_1q_sharpe,
        -- ISSUE: Self-join for 2 quarters ago
        c_q2.nav_usd as prior_2q_nav,
        c_q2.quarterly_return as prior_2q_return,
        -- ISSUE: Self-join for 3 quarters ago
        c_q3.nav_usd as prior_3q_nav,
        c_q3.quarterly_return as prior_3q_return,
        -- ISSUE: Self-join for 4 quarters ago (1 year)
        c_q4.nav_usd as prior_4q_nav,
        c_q4.quarterly_return as prior_4q_return,
        c_q4.sharpe_ratio as prior_4q_sharpe,
        -- ISSUE: Self-join for 8 quarters ago (2 years)
        c_q8.nav_usd as prior_8q_nav,
        c_q8.quarterly_return as prior_8q_return
    from combined c
    left join combined c_q1
        on c.portfolio_id = c_q1.portfolio_id
        and c_q1.quarter_end = dateadd(quarter, -1, c.quarter_end)
    left join combined c_q2
        on c.portfolio_id = c_q2.portfolio_id
        and c_q2.quarter_end = dateadd(quarter, -2, c.quarter_end)
    left join combined c_q3
        on c.portfolio_id = c_q3.portfolio_id
        and c_q3.quarter_end = dateadd(quarter, -3, c.quarter_end)
    left join combined c_q4
        on c.portfolio_id = c_q4.portfolio_id
        and c_q4.quarter_end = dateadd(quarter, -4, c.quarter_end)
    left join combined c_q8
        on c.portfolio_id = c_q8.portfolio_id
        and c_q8.quarter_end = dateadd(quarter, -8, c.quarter_end)
),

-- ISSUE: Multiple window functions with repeated partitions
with_window_calcs as (
    select
        wsj.*,
        -- ISSUE: Running totals (repeated partition)
        sum(quarterly_contributions) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between unbounded preceding and current row
        ) as cumulative_contributions,
        sum(quarterly_distributions) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between unbounded preceding and current row
        ) as cumulative_distributions,
        sum(quarterly_dividends) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between unbounded preceding and current row
        ) as cumulative_dividends,
        sum(quarterly_fees) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between unbounded preceding and current row
        ) as cumulative_fees,
        -- ISSUE: Moving averages (same partition repeated)
        avg(quarterly_return) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between 3 preceding and current row
        ) as rolling_4q_avg_return,
        avg(quarterly_return) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between 7 preceding and current row
        ) as rolling_8q_avg_return,
        -- ISSUE: More window calculations
        stddev(quarterly_return) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between 3 preceding and current row
        ) as rolling_4q_volatility,
        min(quarterly_return) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between 3 preceding and current row
        ) as rolling_4q_min_return,
        max(quarterly_return) over (
            partition by wsj.portfolio_id
            order by wsj.quarter_end
            rows between 3 preceding and current row
        ) as rolling_4q_max_return,
        -- ISSUE: Ranking (same partition again)
        row_number() over (
            partition by wsj.portfolio_id
            order by wsj.quarterly_return desc
        ) as best_quarter_rank,
        row_number() over (
            partition by wsj.portfolio_id
            order by wsj.quarterly_return asc
        ) as worst_quarter_rank
    from with_self_joins wsj
),

-- ISSUE: Separate fund-level aggregation (should use window functions)
fund_quarterly_aggs as (
    select
        fund_id,
        valuation_date as quarter_end,
        sum(nav_usd) as fund_total_nav,
        avg(portfolio_return_3m) as fund_avg_quarterly_return,
        count(distinct portfolio_id) as fund_portfolio_count
    from quarter_end_perf
    group by 1, 2
),

with_fund_context as (
    select
        wwc.*,
        fqa.fund_total_nav,
        fqa.fund_avg_quarterly_return,
        fqa.fund_portfolio_count
    from with_window_calcs wwc
    left join fund_quarterly_aggs fqa
        on wwc.fund_id = fqa.fund_id
        and wwc.quarter_end = fqa.quarter_end
),

-- ISSUE: Complex derived metrics with repeated CASE statements
with_derived_metrics as (
    select
        wfc.*,
        -- ISSUE: Portfolio weight in fund
        case
            when wfc.fund_total_nav > 0
            then (wfc.nav_usd / wfc.fund_total_nav) * 100
            else null
        end as portfolio_weight_in_fund,
        -- ISSUE: Complex QoQ calculations (repeated division logic)
        case
            when wfc.prior_1q_nav is not null and wfc.prior_1q_nav > 0
            then ((wfc.nav_usd - wfc.prior_1q_nav) / wfc.prior_1q_nav) * 100
            else null
        end as qoq_nav_growth_pct,
        case
            when wfc.prior_1q_return is not null
            then (wfc.quarterly_return - wfc.prior_1q_return)
            else null
        end as qoq_return_change,
        -- ISSUE: Complex YoY calculations
        case
            when wfc.prior_4q_nav is not null and wfc.prior_4q_nav > 0
            then ((wfc.nav_usd - wfc.prior_4q_nav) / wfc.prior_4q_nav) * 100
            else null
        end as yoy_nav_growth_pct,
        case
            when wfc.prior_4q_return is not null
            then (wfc.quarterly_return - wfc.prior_4q_return)
            else null
        end as yoy_return_change,
        -- ISSUE: 2-year growth
        case
            when wfc.prior_8q_nav is not null and wfc.prior_8q_nav > 0
            then ((wfc.nav_usd - wfc.prior_8q_nav) / wfc.prior_8q_nav) * 100
            else null
        end as two_year_nav_growth_pct,
        -- ISSUE: TVPI and DPI calculations (repeated division)
        case
            when wfc.cumulative_contributions > 0
            then (wfc.nav_usd + wfc.cumulative_distributions) / wfc.cumulative_contributions
            else null
        end as tvpi,
        case
            when wfc.cumulative_contributions > 0
            then wfc.cumulative_distributions / wfc.cumulative_contributions
            else null
        end as dpi,
        case
            when wfc.cumulative_contributions > 0
            then wfc.nav_usd / wfc.cumulative_contributions
            else null
        end as rvpi,
        -- ISSUE: Performance trend classification (complex nested CASE)
        case
            when wfc.rolling_4q_avg_return > wfc.rolling_8q_avg_return * 1.2 then 'ACCELERATING'
            when wfc.rolling_4q_avg_return > wfc.rolling_8q_avg_return * 1.05 then 'IMPROVING'
            when wfc.rolling_4q_avg_return < wfc.rolling_8q_avg_return * 0.8 then 'DECELERATING'
            when wfc.rolling_4q_avg_return < wfc.rolling_8q_avg_return * 0.95 then 'DECLINING'
            else 'STABLE'
        end as performance_trend,
        -- ISSUE: Consistency rating (complex nested CASE)
        case
            when wfc.rolling_4q_volatility < 0.02 then 'VERY_CONSISTENT'
            when wfc.rolling_4q_volatility < 0.05 then 'CONSISTENT'
            when wfc.rolling_4q_volatility < 0.10 then 'MODERATE'
            when wfc.rolling_4q_volatility < 0.15 then 'VARIABLE'
            else 'HIGHLY_VARIABLE'
        end as consistency_rating,
        -- ISSUE: Relative to fund performance (nested CASE)
        case
            when wfc.quarterly_return > wfc.fund_avg_quarterly_return + 0.05 then 'SIGNIFICANT_OUTPERFORM'
            when wfc.quarterly_return > wfc.fund_avg_quarterly_return + 0.02 then 'OUTPERFORM'
            when wfc.quarterly_return < wfc.fund_avg_quarterly_return - 0.05 then 'SIGNIFICANT_UNDERPERFORM'
            when wfc.quarterly_return < wfc.fund_avg_quarterly_return - 0.02 then 'UNDERPERFORM'
            else 'IN_LINE'
        end as relative_to_fund
    from with_fund_context wfc
),

-- ISSUE: More complex calculations and string operations
final as (
    select
        wdm.portfolio_id,
        wdm.portfolio_name,
        wdm.portfolio_type,
        wdm.fund_id,
        wdm.quarter_end,
        wdm.quarter_start,
        wdm.valuation_year,
        wdm.valuation_quarter,
        -- ISSUE: String concatenations (slow)
        concat('Q', wdm.valuation_quarter, ' ', wdm.valuation_year) as quarter_label,
        concat(wdm.valuation_year, '-Q', wdm.valuation_quarter) as quarter_code,
        concat(wdm.portfolio_name, ' (', wdm.portfolio_type, ')') as portfolio_display,
        -- Core metrics
        wdm.nav_usd,
        wdm.quarterly_return,
        wdm.benchmark_quarterly_return,
        wdm.quarterly_excess_return,
        wdm.portfolio_return_1y as trailing_1y_return,
        wdm.portfolio_cumulative_return as since_inception_return,
        wdm.sharpe_ratio,
        wdm.sortino_ratio,
        wdm.max_drawdown,
        wdm.performance_rating,
        wdm.risk_adjusted_rating,
        -- Cashflow metrics
        wdm.quarterly_contributions,
        wdm.quarterly_distributions,
        wdm.quarterly_dividends,
        wdm.quarterly_fees,
        wdm.quarterly_net_cashflow,
        wdm.cumulative_contributions,
        wdm.cumulative_distributions,
        wdm.total_transactions,
        -- Period comparisons
        wdm.qoq_nav_growth_pct,
        wdm.qoq_return_change,
        wdm.yoy_nav_growth_pct,
        wdm.yoy_return_change,
        wdm.two_year_nav_growth_pct,
        -- Performance ratios
        wdm.tvpi,
        wdm.dpi,
        wdm.rvpi,
        -- Rolling metrics
        wdm.rolling_4q_avg_return,
        wdm.rolling_8q_avg_return,
        wdm.rolling_4q_volatility,
        wdm.rolling_4q_min_return,
        wdm.rolling_4q_max_return,
        -- Fund context
        wdm.fund_total_nav,
        wdm.fund_avg_quarterly_return,
        wdm.fund_portfolio_count,
        wdm.portfolio_weight_in_fund,
        -- Classifications
        wdm.performance_trend,
        wdm.consistency_rating,
        wdm.relative_to_fund,
        wdm.best_quarter_rank,
        wdm.worst_quarter_rank,
        -- ISSUE: Additional derived fields (repeated calculations)
        case
            when wdm.cumulative_contributions > 0
            then wdm.cumulative_distributions / wdm.cumulative_contributions * 100
            else null
        end as distribution_yield_pct,
        case
            when wdm.cumulative_contributions > 0
            then wdm.cumulative_fees / wdm.cumulative_contributions * 100
            else null
        end as fee_burden_pct
    from with_derived_metrics wdm
)

select * from final
order by portfolio_id, quarter_end
    )

/* {"app": "dbt", "dbt_version": "1.11.0b3", "profile_name": "bain_capital", "target_name": "dev", "node_id": "model.bain_capital_portfolio_analytics.report_lp_quarterly"} */;
[0m21:53:33.130042 [debug] [Thread-4  ]: SQL status: SUCCESS 1 in 1.863 seconds
[0m21:53:33.134271 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f5ca520>]}
[0m21:53:33.135430 [info ] [Thread-4  ]: 29 of 30 OK created sql table model DEV_pipeline_c.report_ic_dashboard ......... [[32mSUCCESS 1[0m in 1.90s]
[0m21:53:33.136181 [debug] [Thread-4  ]: Finished running node model.bain_capital_portfolio_analytics.report_ic_dashboard
[0m21:53:33.238847 [debug] [Thread-3  ]: SQL status: SUCCESS 1 in 1.965 seconds
[0m21:53:33.243033 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee178f1b-c4e2-4332-9309-6e2c52676b04', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fc73f10>]}
[0m21:53:33.244159 [info ] [Thread-3  ]: 30 of 30 OK created sql table model DEV_pipeline_c.report_lp_quarterly ......... [[32mSUCCESS 1[0m in 2.01s]
[0m21:53:33.244904 [debug] [Thread-3  ]: Finished running node model.bain_capital_portfolio_analytics.report_lp_quarterly
[0m21:53:33.246894 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:53:33.247352 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.fact_portfolio_performance' was left open.
[0m21:53:33.247768 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.fact_portfolio_performance: Close
[0m21:53:33.582333 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.fact_fund_summary' was left open.
[0m21:53:33.583320 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.fact_fund_summary: Close
[0m21:53:33.907737 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.report_lp_quarterly' was left open.
[0m21:53:33.908726 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.report_lp_quarterly: Close
[0m21:53:34.279746 [debug] [MainThread]: Connection 'model.bain_capital_portfolio_analytics.report_ic_dashboard' was left open.
[0m21:53:34.280736 [debug] [MainThread]: On model.bain_capital_portfolio_analytics.report_ic_dashboard: Close
[0m21:53:34.613508 [info ] [MainThread]: 
[0m21:53:34.614464 [info ] [MainThread]: Finished running 9 table models, 21 view models in 0 hours 0 minutes and 57.04 seconds (57.04s).
[0m21:53:34.619785 [debug] [MainThread]: Command end result
[0m21:53:34.676861 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/manifest.json
[0m21:53:34.679330 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/nedazarei/Documents/turintech/dbtproject/target/semantic_manifest.json
[0m21:53:34.686925 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/nedazarei/Documents/turintech/dbtproject/target/run_results.json
[0m21:53:34.687255 [info ] [MainThread]: 
[0m21:53:34.687610 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:53:34.687905 [info ] [MainThread]: 
[0m21:53:34.688196 [info ] [MainThread]: Done. PASS=30 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=30
[0m21:53:34.690777 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 58.365604, "process_in_blocks": "0", "process_kernel_time": 0.576165, "process_mem_max_rss": "192380928", "process_out_blocks": "0", "process_user_time": 3.996578}
[0m21:53:34.691131 [debug] [MainThread]: Command `dbt run` succeeded at 21:53:34.691063 after 58.37 seconds
[0m21:53:34.691460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10be07280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f1a8be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c0fe730>]}
[0m21:53:34.691787 [debug] [MainThread]: Flushing usage events
[0m21:53:35.674450 [debug] [MainThread]: An error was encountered while trying to flush usage events
